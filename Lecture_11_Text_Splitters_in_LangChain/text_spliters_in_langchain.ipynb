{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a053c4c8",
   "metadata": {},
   "source": [
    "# **Length Based Text Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10656c84",
   "metadata": {},
   "source": [
    "**Length Based Text Splitting** is a technique in LangChain (and other NLP frameworks) where large texts are divided into smaller chunks based on a fixed number of characters or tokens. This approach is commonly used to ensure that each chunk fits within the context window of language models, making it easier to process, embed, or retrieve relevant information.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "Simple and fast to implement, requiring minimal configuration.\n",
    "Ensures all chunks are of manageable and predictable size, which is ideal for models with strict input limits.\n",
    "Works well for generic, unstructured text where semantic boundaries are less important.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "May split sentences or paragraphs in unnatural places, potentially breaking context and meaning.\n",
    "Can lead to loss of coherence if important information is divided between chunks.\n",
    "Does not consider the semantic structure of the text, which may reduce the quality of downstream tasks like summarization or question answering.\n",
    "\n",
    "**Limitation:**\n",
    "\n",
    "Length based splitting is not suitable for documents where maintaining semantic or logical boundaries is critical, such as legal contracts or structured reports, as it may disrupt the flow and integrity of the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc73bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CharacterTextSplitter for splitting text into chunks based on character count.\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c176aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a long multi-paragraph text about data science to demonstrate text splitting.\n",
    "\n",
    "text = \"\"\"\n",
    "Data science is an interdisciplinary field[10] focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business.[11]\n",
    "\n",
    "Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action.[12] Andrew Gelman of Columbia University has described statistics as a non-essential part of data science.[13] Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.[14]\n",
    "\n",
    "Etymology\n",
    "Early usage\n",
    "In 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science.[14] In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics.[15] Later, attendees at a 1992 statistics symposium at the University of Montpellier  II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.[16][17]\n",
    "\n",
    "The term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science.[6] In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic.[6] However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data.[18] In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.[17]\n",
    "\n",
    "Modern usage\n",
    "In 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\",[19] a catchphrase that was picked up even by major-city newspapers like the New York Times[20] and the Boston Globe.[21] A decade later, they reaffirmed it, stating that \"the job is more in demand than ever with employers\".[22]\n",
    "\n",
    "The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland.[23] In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.[24]\n",
    "\n",
    "The professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008.[25] Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role in managing a digital data collection.[26]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f161a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CharacterTextSplitter instance to split text into chunks of 100 characters with no overlap.\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 0,\n",
    "    separator=''\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f2f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data science is an interdisciplinary field[10] focused on extracting knowledge from typically large', 'data sets and applying the knowledge from that data to solve problems in other application domains.', 'The field encompasses preparing data for analysis, formulating data science problems, analyzing dat', 'a, and summarizing these findings. As such, it incorporates skills from computer science, mathematic', 's, data visualization, graphic design, communication, and business.[11]\\n\\nVasant Dhar writes that sta', 'tistics emphasizes quantitative data and description. In contrast, data science deals with quantitat', 'ive and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.', ') and emphasizes prediction and action.[12] Andrew Gelman of Columbia University has described stati', 'stics as a non-essential part of data science.[13] Stanford professor David Donoho writes that data', 'science is not distinguished from statistics by the size of datasets or use of computing and that ma', 'ny graduate programs misleadingly advertise their analytics and statistics training as the essence o', 'f a data-science program. He describes data science as an applied field growing out of traditional s', 'tatistics.[14]\\n\\nEtymology\\nEarly usage\\nIn 1962, John Tukey described a field he called \"data analysis', '\", which resembles modern data science.[14] In 1985, in a lecture given to the Chinese Academy of Sc', 'iences in Beijing, C. F. Jeff Wu used the term \"data science\" for the first time as an alternative n', 'ame for statistics.[15] Later, attendees at a 1992 statistics symposium at the University of Montpel', 'lier  II acknowledged the emergence of a new discipline focused on data of various origins and forms', ', combining established concepts and principles of statistics and data analysis with computing.[16][', '17]\\n\\nThe term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alterna', 'tive name to computer science.[6] In 1996, the International Federation of Classification Societies', 'became the first conference to specifically feature data science as a topic.[6] However, the definit', 'ion was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997', 'C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a ne', 'w name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting o', 'r limited to describing data.[18] In 1998, Hayashi Chikio argued for data science as a new, interdis', 'ciplinary concept, with three aspects: data design, collection, and analysis.[17]\\n\\nModern usage\\nIn 2', '012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the', '21st Century\",[19] a catchphrase that was picked up even by major-city newspapers like the New York', 'Times[20] and the Boston Globe.[21] A decade later, they reaffirmed it, stating that \"the job is mo', 're in demand than ever with employers\".[22]\\n\\nThe modern conception of data science as an independent', 'discipline is sometimes attributed to William S. Cleveland.[23] In 2014, the American Statistical A', \"ssociation's Section on Statistical Learning and Data Mining changed its name to the Section on Stat\", 'istical Learning and Data Science, reflecting the ascendant popularity of data science.[24]\\n\\nThe pro', 'fessional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008.[2', '5] Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Co', 'llections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role', 'in managing a digital data collection.[26]']\n"
     ]
    }
   ],
   "source": [
    "# Split the sample text into chunks using the splitter and print the resulting list of chunks.\n",
    "\n",
    "result = splitter.split_text(text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16e13e",
   "metadata": {},
   "source": [
    "# **Using CharacterTextSplitter with PyPDFDirectoryLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6edeac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CharacterTextSplitter and PyPDFLoader for splitting PDF documents into text chunks.\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a58703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a PDF file ('dl-curriculum.pdf') and convert its pages into LangChain document objects.\n",
    "\n",
    "loader = PyPDFLoader('dl-curriculum.pdf')\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7846eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CharacterTextSplitter to split PDF document content into chunks of 200 characters with no overlap.\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 0,\n",
    "    separator=''\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de9b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ificialneurons\n",
      "2.HistoryofNeuralNetworks\n",
      "● Earlymodels(Perceptron)● BackpropagationandMLPs● The\"AIWinter\"andresurgenceofneuralnetworks● Emergenceofdeeplearning\n",
      "3.PerceptronandMultilayerPerceptrons(MLP\n"
     ]
    }
   ],
   "source": [
    "# Split the loaded PDF documents into smaller chunks and print the content of the second chunk.\n",
    "\n",
    "result = splitter.split_documents(docs)\n",
    "\n",
    "print(result[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf30b86d",
   "metadata": {},
   "source": [
    "# **Text_Structure_Based**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba11088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RecursiveCharacterTextSplitter for splitting text based on structure (e.g., paragraphs, sentences).\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e91bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a multi-paragraph text about the mindset of a data scientist for demonstrating structure-based splitting.\n",
    "\n",
    "text = \"\"\"\n",
    "Becoming a data scientist demands a mindset rooted in curiosity and lifelong learning. You must be endlessly inquisitive about how things work, why patterns emerge in data, and how to turn numbers into meaningful insights. It’s important to love asking questions, digging deeper into problems, and being willing to learn new tools, algorithms, and concepts as the field rapidly evolves. A good data scientist embraces uncertainty and sees each dataset as an opportunity to discover something valuable, rather than as a mere technical task to complete.\n",
    "\n",
    "Equally crucial is resilience and a growth mindset. Much of data science involves trial and error: models fail, hypotheses don’t hold, and data can be messy or incomplete. Instead of getting frustrated, you need to see setbacks as lessons and persist until you find a solution. Communication skills and empathy are also vital — you must translate technical findings into clear, actionable insights for non-technical audiences and collaborate effectively with diverse teams. Ultimately, the mindset of a successful data scientist blends analytical rigor with creativity, adaptability, and the courage to tackle complex, ambiguous problems.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RecursiveCharacterTextSplitter to split the text into chunks of 300 characters, preserving structure where possible.\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 0,\n",
    "    # separator=''\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8279d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['Becoming a data scientist demands a mindset rooted in curiosity and lifelong learning. You must be endlessly inquisitive about how things work, why patterns emerge in data, and how to turn numbers into meaningful insights. It’s important to love asking questions, digging deeper into problems, and', 'being willing to learn new tools, algorithms, and concepts as the field rapidly evolves. A good data scientist embraces uncertainty and sees each dataset as an opportunity to discover something valuable, rather than as a mere technical task to complete.', 'Equally crucial is resilience and a growth mindset. Much of data science involves trial and error: models fail, hypotheses don’t hold, and data can be messy or incomplete. Instead of getting frustrated, you need to see setbacks as lessons and persist until you find a solution. Communication skills', 'and empathy are also vital — you must translate technical findings into clear, actionable insights for non-technical audiences and collaborate effectively with diverse teams. Ultimately, the mindset of a successful data scientist blends analytical rigor with creativity, adaptability, and the', 'courage to tackle complex, ambiguous problems.']\n"
     ]
    }
   ],
   "source": [
    "# Split the text into structured chunks and print the number of chunks and their content.\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "print(len(chunks))\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd93a93c",
   "metadata": {},
   "source": [
    "# **Python Code Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RecursiveCharacterTextSplitter and Language for code-aware splitting.\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69cb123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample Python code as a string for code splitting demonstration.\n",
    "text = \"\"\"\n",
    "text = \"\"\"\n",
    "class Student:\n",
    "    def __init__(self, name, age, grade):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.grade = grade  # Grade is a float (like 8.5 or 9.2)\n",
    "\n",
    "    def get_details(self):\n",
    "        return self.name\"\n",
    "\n",
    "    def is_passing(self):\n",
    "        return self.grade >= 6.0\n",
    "\n",
    "\n",
    "# Example usage\n",
    "student1 = Student(\"Aarav\", 20, 8.2)\n",
    "print(student1.get_details())\n",
    "\n",
    "if student1.is_passing():\n",
    "    print(\"The student is passing.\")\n",
    "else:\n",
    "    print(\"The student is not passing.\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab18f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a RecursiveCharacterTextSplitter for Python code, specifying language and chunk size.\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b680aa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "class Student:\n",
      "    def __init__(self, name, age, grade):\n",
      "        self.name = name\n",
      "        self.age = age\n",
      "        self.grade = grade  # Grade is a float (like 8.5 or 9.2)\n",
      "\n",
      "    def get_details(self):\n",
      "        return self.name\"\n",
      "\n",
      "    def is_passing(self):\n",
      "        return self.grade >= 6.0\n"
     ]
    }
   ],
   "source": [
    "# Split the Python code into logical code chunks and print the number of chunks and the first chunk.\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "print(len(chunks))\n",
    "\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271b64b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Example usage\n",
      "student1 = Student(\"Aarav\", 20, 8.2)\n",
      "print(student1.get_details())\n",
      "\n",
      "if student1.is_passing():\n",
      "    print(\"The student is passing.\")\n",
      "else:\n",
      "    print(\"The student is not passing.\")\n"
     ]
    }
   ],
   "source": [
    "# Print the second chunk of the split Python code.\n",
    "print(chunks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d128c594",
   "metadata": {},
   "source": [
    "# **Markdown Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf164b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RecursiveCharacterTextSplitter and Language for Markdown-aware splitting.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02819334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample Markdown document as a string for splitting demonstration.\n",
    "text=\"\"\"\n",
    "# Project Name: Smart Student Tracker\n",
    "\n",
    "A simple Python-based project to manage and track student data, including their grades, age, and academic status.\n",
    "\n",
    "\n",
    "## Features\n",
    "\n",
    "- Add new students with relevant info\n",
    "- View student details\n",
    "- Check if a student is passing\n",
    "- Easily extendable class-based design\n",
    "\n",
    "\n",
    "## 🛠 Tech Stack\n",
    "\n",
    "- Python 3.10+\n",
    "- No external dependencies\n",
    "\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. Clone the repo  \n",
    "   ```bash\n",
    "   git clone https://github.com/your-username/student-tracker.git\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f082559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a RecursiveCharacterTextSplitter for Markdown, specifying language and chunk size.\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN,\n",
    "    chunk_size =200,\n",
    "    chunk_overlap =0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Markdown text into logical chunks.\n",
    "chunks = splitter.split_text(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d37f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "# Project Name: Smart Student Tracker\n",
      "\n",
      "A simple Python-based project to manage and track student data, including their grades, age, and academic status.\n"
     ]
    }
   ],
   "source": [
    "# Print the number of Markdown chunks and the first chunk.\n",
    "\n",
    "print(len(chunks))\n",
    "\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5283e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Features\n",
      "\n",
      "- Add new students with relevant info\n",
      "- View student details\n",
      "- Check if a student is passing\n",
      "- Easily extendable class-based design\n"
     ]
    }
   ],
   "source": [
    "# Print the second chunk of the split Markdown document.\n",
    "print(chunks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c5b9a",
   "metadata": {},
   "source": [
    "# **Semantic Meaning Based**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19692410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import SemanticChunker for semantic-based splitting and HuggingFaceEmbeddings for embeddings.\n",
    "# Also import dotenv to load environment variables.\n",
    "# Uncomment the pip install line if you haven't installed langchain-experimental.\n",
    "# %pip install langchain-experimental\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cb94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HuggingFaceEmbeddings with a specific model for semantic chunking.\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    # huggingfacehub_api_token=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b3570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SemanticChunker using the embeddings and specify the breakpoint threshold.\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings, \n",
    "    breakpoint_threshold_type='standard_deviation',\n",
    "    breakpoint_threshold_amount=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944d0fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample text with multiple topics for semantic chunking demonstration.\n",
    "\n",
    "sample = \"\"\"\n",
    "Farmers were working hard in the fields, preparing the soil and planting seeds for the next season. The sun was bright, and the air smelled of earth and fresh grass. The Indian Premier League (IPL) is the biggest cricket league in the world. People all over the world watch the matches and cheer for their favourite teams.\n",
    "\n",
    "\n",
    "Terrorism is a big danger to peace and safety. It causes harm to people and creates fear in cities and villages. When such attacks happen, they leave behind pain and sadness. To fight terrorism, we need strong laws, alert security forces, and support from people who care about peace and safety.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eefecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the SemanticChunker to split the sample text into semantically meaningful chunks and print the results.\n",
    "\n",
    "docs = text_splitter.create_documents([sample])\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "print(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
