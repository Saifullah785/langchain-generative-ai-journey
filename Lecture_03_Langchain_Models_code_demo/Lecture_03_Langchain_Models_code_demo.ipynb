{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkPLed48PnLqMeExJ4kZDo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saifullah785/langchain-generative-ai-journey/blob/main/Lecture_03_Langchain_Models_code_demo/Lecture_03_Langchain_Models_code_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain Models | Code Demo**"
      ],
      "metadata": {
        "id": "W28ZWuZ1gOCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Install Libraries:**\n",
        "\n",
        "This code first installs the necessary libraries:\n",
        "\n",
        "langchain-huggingface, huggingface_hub, transformers, accelerate, bitsandbytes, and langchain.\n",
        "\n",
        " Version numbers are added for clarity and reproducibility."
      ],
      "metadata": {
        "id": "oGkXyCCiUJSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Libraries Required\n",
        "# !pip install langchain-huggingface\n",
        "# ## For API Calls\n",
        "# !pip install huggingface_hub\n",
        "# !pip install transformers\n",
        "# !pip install accelerate\n",
        "# !pip install  bitsandbytes\n",
        "# !pip install langchain"
      ],
      "metadata": {
        "id": "dFn4BGlkXka6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Get API Key:**\n",
        "\n",
        "It retrieves your Hugging Face API token from Google Colab's user data secrets.\n",
        "\n",
        "It's important to store your API key securely using Colab secrets."
      ],
      "metadata": {
        "id": "VvVj7EVUUWkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment secret keys\n",
        "\n",
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "6LlUngcwQhuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your Hugging Face API token from Colab secrets\n",
        "\n",
        "sec_key = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "print(\"API Key retrieved (first few characters):\", sec_key[:5] + \"...\") # Print partially for confirmation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slX6MkZJQotp",
        "outputId": "979bdccf-b380-4b0f-9bf1-f62b67ab4341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key retrieved (first few characters): hf_pq...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Environment Variable:**\n",
        "\n",
        "The API token is set as an environment variable HUGGINGFACEHUB_API_TOKEN.\n",
        "\n",
        "This is how the HuggingFaceEndpoint class will authenticate with the Hugging Face API."
      ],
      "metadata": {
        "id": "IRTPo0ifUu1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the environment variable for the API token\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = sec_key"
      ],
      "metadata": {
        "id": "IMmrOoW-QqYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import HuggingFaceEndpoint:**\n",
        "\n",
        " The HuggingFaceEndpoint class is imported from langchain_huggingface."
      ],
      "metadata": {
        "id": "gUcNIz12U105"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the HuggingFaceEndpoint class\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "TnJMhOTlQsZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Define Model:**\n",
        "\n",
        "The repo_id variable is set to the specific model you want to use, which is \"mistralai/Mistral-7-B-Instruct-v0.3\"."
      ],
      "metadata": {
        "id": "mI8LWqmLU9Mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the repository ID for the model\n",
        "\n",
        "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T7goFBOGQwUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Initialize Model:**\n",
        "\n",
        "An instance of HuggingFaceEndpoint is created with the specified repo_id, max_new_tokens, and temperature.\n",
        "\n",
        "max_new_tokens controls the maximum length of the generated response, and temperature affects the randomness of the output."
      ],
      "metadata": {
        "id": "fmbi64igVNGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the HuggingFaceEndpoint model\n",
        "\n",
        "llm = HuggingFaceEndpoint(repo_id=repo_id, max_new_tokens=128, temperature=0.7)"
      ],
      "metadata": {
        "id": "uhe25R2NVMWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Invoke Model:**\n",
        "\n",
        "The llm.invoke() method is used to send a prompt (\"what is machine learning.\") to the model.\n",
        "\n",
        "#**Print Response:**\n",
        "\n",
        "The generated response from the model is printed."
      ],
      "metadata": {
        "id": "vkBCHEJlVZ-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Use the model to invoke a prompt\n",
        "\n",
        "# response = llm.invoke(\"who is data scientist.\")\n",
        "\n",
        "# # Print the response from the model\n",
        "\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "UsFZFLEQQWnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "model_id = \"gpt2\" # You can change this to your desired model if it fits within your Colab resources\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\"text-generation\" ,model=model, tokenizer=tokenizer , max_new_tokens=100)\n",
        "\n",
        "hf = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Use the pipeline to invoke your prompt\n",
        "response = hf.invoke('what is machine learning.')\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3P_4YG-bUMr",
        "outputId": "b0a71504-3689-4fd9-adaf-9de0d5eb1d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is machine learning. What is machine learning? I think machine learning is about learning something about values and values. What is machine learning? I think machine learning is about learning something about values and values. I think machine learning is about learning something about values and values. I think machine learning is about learning something about values and values. I think machine learning is about learning something about values and values. I think machine learning is about learning something about values and values. I think machine learning is about learning something about values and values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFaceEndpoint\n",
        "## How to Access HuggingFace Models with API\n",
        "There are also two ways to use this class. You can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code.\n"
      ],
      "metadata": {
        "id": "hbgZgL-hEBm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "question = 'what is machine learning?'\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "M2fBPMcP_Cua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "# print(llm_chain.invoke(question))"
      ],
      "metadata": {
        "id": "xvpsq07J_cNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HuggingFacePipeline**\n",
        "\n",
        "Among transformers, the Pipeline is the most versatile tool in the Hugging Face toolbox.\n",
        "\n",
        "LangChain being designed primarily to address RAG and Agent use cases, the scope of the pipeline here is reduced to the following text-centric tasks:\n",
        "\n",
        "“text-generation\", “text2text-generation\", “summarization”, “translation”. Models can be loaded directly with the from_model_id method"
      ],
      "metadata": {
        "id": "gk7SI3MhJ9a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "cSKndr-eKImd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_id = \"gpt2\"\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "W2UrumNNL_E0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pipe = pipeline(\"text-generation\" ,model=model, tokenizer=tokenizer , max_new_tokens=100)\n",
        "\n",
        "hf = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtxx2JTYMC8a",
        "outputId": "bca534a2-190b-4226-ccef-afa60889b706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p1Jo2goM2zZ",
        "outputId": "ad041bc1-68bc-4e17-b3ac-04ac677e8375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f9a8a001990>, model_id='gpt2')"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf.invoke('what is machine learning')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "7IEhiJ2JM8vY",
        "outputId": "09f066ca-21f7-4f57-f26c-5aa969001777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"what is machine learning and what's going on with machine learning?\\n\\nIf you're a programmer, you'll probably think of machine learning as the process of learning. But it's much more than that. The computer is a powerful tool to learn, to be able to take a piece of data and make a decision, and then the human is able to do it.\\n\\nMachine learning is very scalable, and it's the same way that we're using GPUs to build a game. It uses the same architecture\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use HuggingfacePipelines With Gpu\n",
        "\n",
        "gpu_llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"gpt2\",\n",
        "    task=\"text-generation\",\n",
        "    device=-1,  # replace with device_map=\"auto\" to use the accelerate library.\n",
        "    pipeline_kwargs={\"max_new_tokens\": 100},\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPgTiOHHO76a",
        "outputId": "abf327d9-00a1-477d-830b-cbc0302e2435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "z0J5F5SmPdmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain=prompt|gpu_llm"
      ],
      "metadata": {
        "id": "WraATj9kRysO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"What is artificial intelligence?\"\n",
        "chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "XFetOPTrR06F",
        "outputId": "57080211-3161-4b66-c4b3-5197790e89fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Question: What is artificial intelligence?\\n\\nAnswer: Let\\'s think step by step.\\n\\n1. We see that a \"smart\" device is the equivalent of a \"smart\" person when it is actually a robot.\\n\\n2. We see that humans are not \"smart\" at all.\\n\\n3. We see that \"smart\" people are not \"smart\" at all!\\n\\n4. We see that humans are \"smart\" at all.\\n\\n5. We see that human intelligence is not \"smart\" at all.\\n\\n6. We'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    }
  ]
}