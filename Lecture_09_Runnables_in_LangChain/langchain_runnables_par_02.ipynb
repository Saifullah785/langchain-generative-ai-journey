{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d229a0",
   "metadata": {},
   "source": [
    "# **1.Runnable Sequence**\n",
    "\n",
    "\n",
    "RunnableSequence is a Sequential chain of runnables in langchain that executes each step one after another, passing the output of one step as the input to the next.\n",
    "\n",
    "it is useful when you need to compose multiple runnables together in a structured workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd012f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules for HuggingFace LLM, prompt templates, output parsing, environment variables, and runnable chains.\n",
    "from langchain_huggingface import HuggingFaceEndpoint,ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HuggingFaceEndpoint with the Meta-Llama model for text generation.\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id ='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    task = 'text-generation'\n",
    ")\n",
    "\n",
    "# Wrap the endpoint in a ChatHuggingFace object for chat-style interaction.\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90278a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string output parser to extract plain text from the model's response.\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef39e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template to generate a joke about a given topic.\n",
    "prompt1 = PromptTemplate(\n",
    "    template='write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f07f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template to explain a given joke.\n",
    "prompt2 = PromptTemplate(\n",
    "    template = \"Explain the following joke - {text}\",\n",
    "    input_variables= ['text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c32ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a runnable sequence chain that generates a joke, then explains it.\n",
    "chain = RunnableSequence(prompt1, model, parser, prompt2, model, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9385eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This joke is a play on words, combining a common phrase related to weight loss with a term from computer science.\n",
      "\n",
      "In the context of the joke, \"lose some bytes\" is a pun. In computer science, a \"byte\" is a unit of digital information, equivalent to 8 bits. It's a fundamental concept in programming and data storage.\n",
      "\n",
      "The phrase \"lose some weight\" is a common idiom used in the context of dieting, implying that someone wants to reduce their body weight.\n",
      "\n",
      "In the joke, the AI program is said to \"lose some bytes,\" which is a clever play on words, as it connects the concept of digital information (bytes) with the idea of weight loss (losing weight). The punchline is a clever and humorous connection between the two concepts.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain with the topic \"AI\" and print the explanation of the generated joke.\n",
    "print(chain.invoke({'topic':'AI'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c26d1",
   "metadata": {},
   "source": [
    "# **2.RunnableParallel**\n",
    "\n",
    "RunnableParallel is a runnable primitive that allow multiple runnables to execute in parallel.\n",
    "\n",
    "Each runnable receives the same input and processes it independently, producing a dictionary of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf0779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules for parallel chains, prompt templates, output parsing, and environment variables.\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint,ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableParallel\n",
    "\n",
    "# Load environment variables (such as API keys) from a .env file.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e0ae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HuggingFaceEndpoint and wrap it in a ChatHuggingFace model for text generation.\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id ='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    task = 'text-generation'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb5334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string output parser to extract plain text from the model's response.\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template to generate a tweet about a given topic.\n",
    "prompt1 = PromptTemplate(\n",
    "    template= \"Generate a tweet about {topic}\",\n",
    "    input_variables=['topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8978403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template to generate a Linkedin post about a given topic.\n",
    "prompt2 = PromptTemplate(\n",
    "    template = 'Generate a Linkedin post about {topic}',\n",
    "    input_variables=['topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a parallel chain that generates both a tweet and a Linkedin post in parallel.\n",
    "parallel_chain = RunnableParallel({\n",
    "    'tweet' : RunnableSequence (prompt1, model, parser),\n",
    "    'linkedin' : RunnableSequence (prompt2, model, parser)\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a0695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The future is here and it's coded. Artificial Intelligence is revolutionizing the way we live, work, and interact. What's the next innovation that AI will bring to the table? #ArtificialIntelligence #AI #FutureTech\"\n",
      "**The Future of Work: How AI is Revolutionizing Industries**\n",
      "\n",
      "As we continue to navigate the ever-evolving landscape of technology, one thing is clear: Artificial Intelligence (AI) is no longer a novelty, but a fundamental aspect of our professional lives.\n",
      "\n",
      "From automating mundane tasks to enhancing decision-making capabilities, AI is transforming industries and shaping the future of work. Whether you're a seasoned executive or a young professional, it's essential to stay ahead of the curve and understand the implications of AI on your career.\n",
      "\n",
      "Here are just a few ways AI is impacting the world of work:\n",
      "\n",
      " **Increased Efficiency**: AI can automate repetitive tasks, freeing up time for more strategic and creative work.\n",
      "\n",
      " **Enhanced Decision-Making**: AI-powered tools can analyze vast amounts of data, providing insights that inform decision-making and drive business growth.\n",
      "\n",
      " **Improved Customer Experience**: AI-driven chatbots and virtual assistants are revolutionizing the way we interact with customers, providing personalized support and improving overall satisfaction.\n",
      "\n",
      "As AI continues to advance, it's essential to develop the skills necessary to thrive in this new landscape. Whether it's learning to work with AI tools, developing data analysis skills, or understanding the ethics of AI, there has never been a more exciting time to be in the workforce.\n",
      "\n",
      "**What do you think about the future of work and AI? Share your thoughts and experiences in the comments below!**\n",
      "\n",
      "**#AI #FutureOfWork #Innovation #CareerDevelopment #Technology #Productivity**\n"
     ]
    }
   ],
   "source": [
    "# Invoke the parallel chain with the topic \"AI\" and print both the tweet and Linkedin post.\n",
    "result = parallel_chain.invoke({'topic': 'AI'})\n",
    "\n",
    "print(result['tweet'])\n",
    "\n",
    "print(result['linkedin'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581f29db",
   "metadata": {},
   "source": [
    "# **3.Runnable Passthrough**\n",
    "\n",
    "Runnable Passthrough is a special Runnable perimitive that simply returns the output as output without modifying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069ae18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules for passthrough, prompt templates, output parsing, and environment variables.\n",
    "from langchain_huggingface import HuggingFaceEndpoint,ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableParallel,RunnablePassthrough\n",
    "# Load environment variables (such as API keys) from a .env file.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d7ead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Saifullah'}\n"
     ]
    }
   ],
   "source": [
    "# Create a passthrough runnable that returns the input as output.\n",
    "passthrough = RunnablePassthrough()\n",
    "\n",
    "\n",
    "# Test the passthrough by passing a dictionary and printing the result.\n",
    "print(passthrough.invoke({'name': 'Saifullah'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HuggingFaceEndpoint and wrap it in a ChatHuggingFace model for text generation.\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id ='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    task = 'text-generation'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbcb7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string output parser to extract plain text from the model's response.\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c9dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template to generate a joke about a given topic.\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a19b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template to explain a given joke.\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template = \"Explain the following joke - {text}\",\n",
    "    input_variables= ['text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a runnable sequence chain that generates a joke.\n",
    "\n",
    "joke_gen_chain = RunnableSequence(prompt1, model, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7224b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a parallel chain that passes the joke through and also generates an explanation for it.\n",
    "parallel_chain = RunnableParallel({\n",
    "    'joke': RunnablePassthrough(),\n",
    "    'explanation' : RunnableSequence(prompt2, model, parser)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aedba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the joke generation and parallel explanation into a final chain.\n",
    "\n",
    "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d03c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': 'Why did the volleyball player bring a ladder to the game?\\n\\nBecause she wanted to take her game to the next level! (get it?)', 'explanation': 'This joke uses a play on words to create the pun. The phrase \"take it to the next level\" is a common idiomatic expression used to convey improvement or advancement in a particular skill or field. \\n\\nIn this joke, the volleyball player takes the phrase literally by bringing a ladder, which is a physical object that allows someone to climb to a higher level. The humor comes from the unexpected twist on the usual meaning of the phrase, creating a clever and silly connection between the volleyball game and the physical ladder. The punchline relies on the dual meaning of \"next level\" to create the comedic effect.'}\n"
     ]
    }
   ],
   "source": [
    "# Invoke the final chain with the topic \"vollyball\" and print both the joke and its explanation.\n",
    "print(final_chain.invoke({'topic': 'vollyball'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2495b100",
   "metadata": {},
   "source": [
    "# **Runnable Lambda**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c390952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RunnableLambda, which allows you to wrap any Python function as a runnable in a chain.\n",
    "\n",
    "from langchain.schema.runnable import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1db899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple Python function that counts the number of words in a string.\n",
    "\n",
    "def word_counter(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b22bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Wrap the word_counter function in a RunnableLambda so it can be used in a chain.\n",
    "\n",
    "runnable_word_counter = RunnableLambda(word_counter)\n",
    "\n",
    "\n",
    "# Test the runnable by invoking it with a sample string and print the word count.\n",
    "print(runnable_word_counter.invoke('Hi there how are you'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea627e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules for HuggingFace LLM, prompt templates, output parsing, environment variables, and all runnable types.\n",
    "from langchain_huggingface import HuggingFaceEndpoint,ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableParallel,RunnablePassthrough,RunnableLambda\n",
    "# Load environment variables (such as API keys) from a .env file.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636bd16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define another word count function for demonstration.\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab31bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HuggingFaceEndpoint and wrap it in a ChatHuggingFace model for text generation.\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id ='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    task = 'text-generation'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a569c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string output parser to extract plain text from the model's response.\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9efff8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template to generate a joke about a given topic.\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be89026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a runnable sequence chain that generates a joke using the prompt, model, and parser.\n",
    "\n",
    "joke_gen_chain = RunnableSequence(prompt, model, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc6877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a parallel chain that passes the joke through unchanged and also computes its word count using a lambda.\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'joke': RunnablePassthrough(),\n",
    "    'word_count' : RunnableLambda(lambda x: len(x.split()))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff257011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the joke generation and parallel word count into a final sequence chain.\n",
    "\n",
    "final_chain= RunnableSequence(joke_gen_chain, parallel_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa36701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the final chain with the topic \"AI\" and store the result.\n",
    "\n",
    "result = final_chain.invoke({'topic': 'AI'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12549850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and print the joke along with its word count.\n",
    "\n",
    "final_result = \"\"\"{} \\n word count - {} \"\"\".format(result['joke'], result ['word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6db8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI go to therapy?\n",
      "\n",
      "Because it was struggling to process its emotions. \n",
      " word count - 15 \n"
     ]
    }
   ],
   "source": [
    "# Print the formatted result showing the joke and its word count.\n",
    "\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0164e197",
   "metadata": {},
   "source": [
    "# **5.Runnable Branch**\n",
    "\n",
    "Runnable branch is a control flow component in langchain that allows you to conditionally route input data to different chains or runnables based on custom logic\n",
    "\n",
    "it functions like an if/ elif / else block for chains - where you define a set of condition functions, each associated with a runnable (e.g LLM call, promptchain, or tool).\n",
    "\n",
    "The first matching condition is executed. if no condition matches, a default runnable is used (if provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b43ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules for HuggingFace LLM, prompt templates, output parsing, environment variables, and all runnable types including RunnableBranch.\n",
    "from langchain_huggingface import HuggingFaceEndpoint,ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableParallel,RunnablePassthrough,RunnableLambda,RunnableBranch\n",
    "# Load environment variables (such as API keys) from a .env file.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c8d7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HuggingFaceEndpoint and wrap it in a ChatHuggingFace model for text generation.\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id ='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    task = 'text-generation'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string output parser to extract plain text from the model's response.\n",
    "\n",
    "parser  = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7831ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template to generate a detailed report about a given topic.\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template = 'write a detailed report on {topic}',\n",
    "    input_variables={'topic'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb34b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template to summarize a given text.\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Summarize the following text \\n {text}',\n",
    "    input_variables=['text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a chain that generates a detailed report using the first prompt, model, and parser.\n",
    "\n",
    "report_gen_chain = RunnableSequence(prompt1, model, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a branch chain:\n",
    "# - If the input text has more than 100 words, summarize it.\n",
    "# - Otherwise, just pass the text through unchanged.\n",
    "\n",
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: len(x.split())>100, RunnableSequence(prompt2, model, parser)),\n",
    "    RunnablePassthrough()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cbf28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the report generation and branching logic into a final sequence chain.\n",
    "\n",
    "final_chain = RunnableSequence(report_gen_chain, branch_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fca31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The report provides a comprehensive analysis of the Pakistan-India relationship, covering historical, cultural, economic, and political aspects. \n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "1.  **Historical Background:** Pakistan and India were born out of the British Indian Empire in 1947 following a bloody partition, with Pakistan created as a homeland for Muslims and India as a homeland for Hindus.\n",
      "2.  **Cultural Exchange:** Despite tensions and conflicts, there has been significant cultural exchange between Pakistan and India, including collaborations in music, literature, and art.\n",
      "3.  **Economic Cooperation:** Trade and economic cooperation between Pakistan and India have increased, with bilateral trade volumes reaching over $2 billion in 2020.\n",
      "4.  **Security and Conflict:** The Pakistan-India relationship remains complex, with security-related issues ongoing, including the Line of Control in Kashmir, terrorism, and militant groups operating in the region.\n",
      "\n",
      "**Key Issues:**\n",
      "\n",
      "1.  **Kashmir:** The dispute over Kashmir remains a major source of tension, with both countries claiming sovereignty over the region.\n",
      "2.  **Terrorism:** The issue of terrorism and militant groups operating in the region remains a major concern.\n",
      "3.  **Border Disputes:** The border between Pakistan and India remains a source of tension, with several disputes over territory and border demarcation.\n",
      "4.  **Water Disputes:** The Indus River and its tributaries are a major source of tension, with Pakistan accusing India of diverting water from the river.\n",
      "\n",
      "**Challenges and Opportunities:**\n",
      "\n",
      "1.  **Economic Cooperation:** Increased trade and economic cooperation could benefit both countries, with the potential for significant economic growth and development.\n",
      "2.  **Cultural Exchange:** The cultural exchange between Pakistan and India has the potential to strengthen people-to-people ties and promote cross-cultural understanding.\n",
      "3.  **Regional Cooperation:** Pakistan and India are both members of several regional organizations, providing opportunities for cooperation on issues such as trade, security, and climate change.\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "1.  **Establish a Dialogue Framework:** Establish a regular dialogue framework between Pakistan and India to address issues such as Kashmir, terrorism, and border disputes.\n",
      "2.  **Promote Economic Cooperation:** Increase trade and economic cooperation between Pakistan and India, with a focus on sectors such as textiles, agriculture, and energy.\n",
      "3.  **Enhance Cultural Exchange:** Promote cultural exchange between Pakistan and India, including people-to-people contacts, cultural exchange programs, and joint artistic and literary projects.\n",
      "4.  **Address Water Disputes:** Address the issue of water disputes between Pakistan and India through dialogue and cooperation, with a focus on shared water resources and sustainable management practices.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the final chain with the topic \"pakistan vs india\" and print the result.\n",
    "\n",
    "print(final_chain.invoke({'topic': 'pakistan vs india'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ddc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The report discusses the sporting rivalry between Pakistan and India, particularly in cricket, which is the most popular sport in both countries. Here's a summary of the key points:\n",
      "\n",
      "**History of the Rivalry:**\n",
      "\n",
      "- The rivalry dates back to the 1940s when the two nations were under British colonial rule.\n",
      "- The first Test match between the two teams was played in 1952, with India emerging as the winners.\n",
      "\n",
      "**Head-to-Head Record:**\n",
      "\n",
      "- In Test matches: Pakistan 68 wins, India 74 wins, Draws 46.\n",
      "- In One-Day Internationals (ODIs): Pakistan 82 wins, India 90 wins, No result 18.\n",
      "- In Twenty20 Internationals (T20Is): Pakistan 7 wins, India 8 wins, No result 2.\n",
      "\n",
      "**Notable Matches:**\n",
      "\n",
      "- 2007 World Cup Semi-Final: Pakistan won by 54 runs.\n",
      "- 2011 World Cup Quarter-Final: India won by 29 runs.\n",
      "- 2015 World Cup Group Stage: India won by 76 runs.\n",
      "- 2017 Champions Trophy Final: Pakistan won by 180 runs.\n",
      "\n",
      "**Key Players:**\n",
      "\n",
      "- Virat Kohli (India): Regarded as one of the greatest batsmen in cricket history.\n",
      "- Mohammad Shami (India): A skilled fast bowler with several wickets against Pakistan.\n",
      "- Babar Azam (Pakistan): A talented batsman with several centuries against India.\n",
      "- Mohammad Amir (Pakistan): A skilled fast bowler with several wickets against India.\n",
      "\n",
      "**Recent Series:**\n",
      "\n",
      "- 2019-2020 Series: India won 2-1.\n",
      "- 2020-2021 Series: Pakistan won 1-0.\n",
      "- 2022-2023 Series: India won 2-1.\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "- Increased security measures to ensure player and spectator safety.\n",
      "- Improved player behavior, with a focus on sportsmanship and respect for opponents.\n",
      "- Enhanced cricket infrastructure, including stadiums and training facilities.\n",
      "\n",
      "**Future Prospects:**\n",
      "\n",
      "- The rivalry is set to continue for many years, with both teams competing at the highest level.\n",
      "- The rise of new talent and development of cricket infrastructure in both countries promises a bright future for the rivalry.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the final chain again with the same topic and print the result.\n",
    "\n",
    "print(final_chain.invoke({'topic': 'pakistan vs india'}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
