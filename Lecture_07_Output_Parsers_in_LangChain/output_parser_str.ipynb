{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06959c0f",
   "metadata": {},
   "source": [
    "# **structured Output using prompt template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5faa4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required modules for using HuggingFace LLMs, environment variables, and prompt templates.\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5636ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HuggingFaceEndpoint with the Mixtral model for text generation.\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "# Wrap the endpoint in a ChatHuggingFace object for chat-style interaction.\n",
    "model = ChatHuggingFace(llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c10e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first prompt template for generating a detailed report on a given topic.\n",
    "# 1st prompt\n",
    "template1 = PromptTemplate(\n",
    "    template='write a detailed report on the topic \"{topic}\"',\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "# 2nd prompt\n",
    "\n",
    "# Define the second prompt template for summarizing a given text in 5 lines.\n",
    "template2 = PromptTemplate(\n",
    "    template='write a 5 line summary on the following text. /n {text}',\n",
    "    input_variables=[\"text\"]\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef466a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a 5-line summary of the report on black holes:\n",
      "\n",
      "Black holes are regions of spacetime where gravity is so strong that nothing, including light, can escape once it falls within a certain radius. They are formed when a massive star collapses in on itself, causing a massive amount of matter to be compressed into an incredibly small point. Black holes have several key properties, including mass, spin, charge, event horizon, and singularity. They have a profound impact on the surrounding environment, affecting the motion of nearby stars and the flow of matter and energy. Despite their elusive nature, black holes can be detected and studied through various methods, including X-rays, radio waves, gravitational waves, and astrometry.\n"
     ]
    }
   ],
   "source": [
    "# Use the first prompt template to create a prompt about \"Black Hole\".\n",
    "\n",
    "prompt1 = template1.invoke({\"topic\": \"Black Hole\"})\n",
    "\n",
    "\n",
    "# Generate a detailed report on \"Black Hole\" using the model.\n",
    "result = model.invoke(prompt1)\n",
    "\n",
    "# Use the second prompt template to create a summary prompt from the generated report.\n",
    "prompt2 = template2.invoke({\"text\": result.content})\n",
    "\n",
    "\n",
    "# Generate a 5-line summary of the report using the model.\n",
    "result = model.invoke(prompt2)\n",
    "\n",
    "# Print the summary content.\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16654a1",
   "metadata": {},
   "source": [
    "# **Sturctured Output Using StrOutputparser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string output parser to extract plain text from the model's response.\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Build a chain that sequentially applies:\n",
    "# 1. template1 (detailed report prompt)\n",
    "# 2. model (LLM)\n",
    "# 3. parser (extracts string)\n",
    "# 4. template2 (summary prompt)\n",
    "# 5. model (LLM)\n",
    "# 6. parser (extracts summary string)\n",
    "\n",
    "\n",
    "chain = template1 | model | parser | template2 | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055860f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a 5-line summary of the text:\n",
      "\n",
      "Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making. AI has a rich history, dating back to the 1950s, and has evolved over the years with advances in computing power, data storage, and machine learning algorithms. Today, AI has a wide range of applications across various industries, including healthcare, finance, transportation, and education. However, AI also poses several challenges, including job displacement, bias and discrimination, privacy and security risks, and explainability and transparency issues. Despite these challenges, AI is expected to have a significant impact on various industries and aspects of our lives in the future.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain with the topic \"artificial intelligence\" to generate a detailed report,\n",
    "# then summarize it in 5 lines, and print the final summary.\n",
    "\n",
    "result = chain.invoke({\"topic\": \"artificial intelligence\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a97c313",
   "metadata": {},
   "source": [
    "# **Json Output Parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "350f49ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required modules for using HuggingFace LLMs, environment variables, and prompt templates.\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4aa49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HuggingFaceEndpoint with the Mixtral model for text generation.\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "# Wrap the endpoint in a ChatHuggingFace object for chat-style interaction.\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5286cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template that asks for a fictional person's name, age, and city,\n",
    "# and includes format instructions from the JSON output parser to ensure structured output.\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Give me the name, age and city of a fictional person.\\n {format_instruction} ',\n",
    "    input_variables=[],\n",
    "    partial_variables={\"format_instruction\": parser.get_format_instructions()}\n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed99d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Emily Wilson', 'age': 32, 'city': 'Seattle'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Format the prompt using the template, send it to the model, and parse the response as JSON.\n",
    "prompt = template.format()\n",
    "\n",
    "result = model.invoke(prompt)\n",
    "\n",
    "final_result = parser.parse(result.content)\n",
    "\n",
    "print(final_result)\n",
    "\n",
    "print(type(final_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3787bda6",
   "metadata": {},
   "source": [
    "#  **Json Output Parser using chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae64e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a JSON object with the requested information:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"name\": \"Emily Wilson\",\n",
      "  \"age\": 32,\n",
      "  \"city\": \"Seattle\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Build a chain that combines the prompt template, model, and JSON output parser.\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "result = chain.invoke({})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de9c669",
   "metadata": {},
   "source": [
    "# **Unstructured output from LLMS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7072ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template that asks for five facts about a given topic,\n",
    "# using format instructions from the JSON output parser for structured output.\n",
    "\n",
    "template = PromptTemplate(\n",
    "    \n",
    "    template='Give me five facts about {topic} \\n {format_instruction}',\n",
    "    \n",
    "    input_variables=['topic'],\n",
    "   \n",
    "    partial_variables={\"format_instruction\": parser.get_format_instructions()}\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d0277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fact1': {'title': 'Machine Learning is a Subset of Artificial Intelligence', 'description': 'Machine learning is a subset of artificial intelligence that involves training algorithms to learn from data and make predictions or decisions without being explicitly programmed.'}, 'fact2': {'title': 'Machine Learning is Used in Many Applications', 'description': 'Machine learning is used in a wide range of applications, including image and speech recognition, natural language processing, recommender systems, and predictive maintenance.'}, 'fact3': {'title': 'Machine Learning Algorithms Can Be Supervised or Unsupervised', 'description': 'Machine learning algorithms can be supervised, meaning they are trained on labeled data, or unsupervised, meaning they are trained on unlabeled data and must find patterns or structure on their own.'}, 'fact4': {'title': 'Deep Learning is a Type of Machine Learning', 'description': 'Deep learning is a type of machine learning that involves the use of artificial neural networks with multiple layers to learn and represent complex patterns in data.'}, 'fact5': {'title': 'Machine Learning Models Can Be Interpreted and Explainable', 'description': 'Machine learning models can be designed to be interpretable and explainable, meaning that the decisions they make and the features they use can be understood and explained by humans.'}}\n"
     ]
    }
   ],
   "source": [
    "# Build a chain with the template, model, and parser to get five facts about \"machine learning\".\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "result = chain.invoke({'topic': 'machine learning'})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e1a58",
   "metadata": {},
   "source": [
    "# **Structured Output Parser**\n",
    "\n",
    "Structured output parser is an output parser in langchain that helps extract\n",
    "structured Json data LLM based on predefined field schemas.\n",
    "\n",
    "it works by defining a list of fields (ResponseSchema) that the model should return, ensuring the output follows a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "459b6414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required modules for using HuggingFace LLMs, environment variables, and prompt templates.\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1284b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HuggingFaceEndpoint with the meta-llama model for text generation.\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "# Wrap the endpoint in a ChatHuggingFace object for chat-style interaction.\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a schema for structured output, specifying five fields for five facts about a topic.\n",
    "\n",
    "schema = [\n",
    "    ResponseSchema(name=\"fact_1\", description=\"Fact 1 about the topic\"),\n",
    "    ResponseSchema(name=\"fact_2\", description=\"Fact 2 about the topic\"),\n",
    "    ResponseSchema(name=\"fact_3\", description=\"Fact 3 about the topic\"),\n",
    "    ResponseSchema(name=\"fact_4\", description=\"Fact 4 about the topic\"),\n",
    "    ResponseSchema(name=\"fact_5\", description=\"Fact 5 about the topic\"),\n",
    "    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6daf3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a structured output parser using the defined schema.\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a62ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template that asks for five facts about a topic,\n",
    "# and includes format instructions for the structured output parser.\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Give me five facts about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={\"format_instruction\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee09f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fact_1': 'Deep learning is a subset of machine learning that uses artificial neural networks to analyze data and make predictions or decisions. It is inspired by the structure and function of the human brain.', 'fact_2': 'Deep learning models can learn to recognize patterns and relationships in data, such as images, speech, and text, by adjusting the connections between artificial neurons and the strength of these connections.', 'fact_3': 'Deep learning has achieved state-of-the-art results in many areas, including image recognition, speech recognition, natural language processing, and game playing, outperforming traditional machine learning methods and human experts in some cases.', 'fact_4': 'Deep learning models are typically trained using large amounts of data and computational resources, and can be prone to overfitting, where the model becomes too specialized to the training data and fails to generalize well to new data.', 'fact_5': \"Deep learning has many applications in industries such as healthcare, finance, and autonomous vehicles, and is being used to develop new products and services that can improve people's lives, such as personalized medicine, fraud detection, and self-driving cars.\"}\n"
     ]
    }
   ],
   "source": [
    "# Generate a prompt for the topic \"deep learning\", invoke the model,\n",
    "# parse the structured response, and print the final result.\n",
    "\n",
    "prompt = template.invoke({\"topic\": \"deep learning\"})\n",
    "\n",
    "result = model.invoke(prompt)\n",
    "\n",
    "final_result = parser.parse(result.content)\n",
    "\n",
    "print(final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df3092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fact_1': 'Quantum computers use quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. This allows them to solve certain problems much faster than classical computers.', 'fact_2': 'Quantum computers can exist in multiple states simultaneously, which is known as a qubit (quantum bit). This is in contrast to classical computers, which can only exist in one of two states (0 or 1).', 'fact_3': 'Quantum computers can perform certain calculations, such as factoring large numbers, much faster than classical computers. This has significant implications for cryptography and encryption.', 'fact_4': \"The first quantum computer was built in 1982 by physicist David Deutsch. However, it wasn't until the 1990s that the concept of quantum computing began to gain widespread attention.\", 'fact_5': 'Quantum computers are still in the early stages of development, and many technical challenges need to be overcome before they can be widely used. However, companies like Google, IBM, and Microsoft are already working on developing practical quantum computers.'}\n"
     ]
    }
   ],
   "source": [
    "# Build a chain to get five structured facts about \"quantum computing\" and print the result.\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "result = chain.invoke({'topic': 'quantum computing'})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c489fa9b",
   "metadata": {},
   "source": [
    "# **Pydantic Output Parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4fb3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required modules for using HuggingFace LLMs, environment variables, and prompt templates\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97557be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HuggingFaceEndpoint with the meta-llama model for text generation.\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c5a5121",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "\n",
    "    name: str = Field(description='Name of the person')\n",
    "    age: int = Field(gt=18, description='Age of the person')\n",
    "    city: str = Field(description='Name of the city the person belongs to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df178d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1578a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template='Generate the name, age and city of a fictional {place} person \\n {format_instruction}',\n",
    "    input_variables=['place'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1133bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Ali Khan' age=32 city='Karachi'\n"
     ]
    }
   ],
   "source": [
    "chain = template | model | parser\n",
    "\n",
    "final_result = chain.invoke({'place':'Pakistan'})\n",
    "\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30d32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Generate the name, age and city of a fictional united states person \\n The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"description\": \"Name of the person\", \"title\": \"Name\", \"type\": \"string\"}, \"age\": {\"description\": \"Age of the person\", \"exclusiveMinimum\": 18, \"title\": \"Age\", \"type\": \"integer\"}, \"city\": {\"description\": \"Name of the city the person belongs to\", \"title\": \"City\", \"type\": \"string\"}}, \"required\": [\"name\", \"age\", \"city\"]}\\n```'\n"
     ]
    }
   ],
   "source": [
    "# Generate and print a prompt for a fictional person from the United States using the template.\n",
    "\n",
    "prompt = template.invoke({\"place\": \"united states\"})\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a23528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Emily Wilson' age=32 city='Denver'\n"
     ]
    }
   ],
   "source": [
    "# Invoke the model with the generated prompt, parse the response using the Pydantic parser,\n",
    "# and print the final structured result.\n",
    "\n",
    "result = model.invoke(prompt)\n",
    "\n",
    "final_result = parser.parse(result.content)\n",
    "\n",
    "print(final_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
