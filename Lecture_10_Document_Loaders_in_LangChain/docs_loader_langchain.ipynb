{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79ad5a2",
   "metadata": {},
   "source": [
    "# **Document Loaders in Langchain**\n",
    "\n",
    "Document laoders are components in langchain used to laod data from various sources into a standardized format (usually as Document objects), which can then be used for chunking, embedding , retrieval, and generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776bc2e4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# **TextLoader**\n",
    "\n",
    "Text Loader is a sample and commonly used document loader in langchain that reads plain text (.txt) files and coverts them into langchain document objects\n",
    "\n",
    " **Use Case**\n",
    "\n",
    " ideal for loading chat logs, scraped text, transcripts, code snippets or any plain text data into a langchain pipeline\n",
    "\n",
    "\n",
    " **limitation**\n",
    "\n",
    " Works only with .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c002e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Import the TextLoader for loading plain text files as LangChain document objects.\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2cf885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Create a TextLoader instance for the file 'datascience.txt' and load the documents.\n",
    "\n",
    "loader = TextLoader('datascience.txt', encoding='utf-8')\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Print the type of the loaded docs object (should be a list of Document objects).\n",
    "\n",
    "print(type(docs)) # Print the metadata of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed50294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "The Self-Taught Data Scientist\n",
      "\n",
      "In quiet rooms where pixels glow,\n",
      "A curious mind begins to grow.\n",
      "No lecture hall, no rigid pace,\n",
      "Just eager steps through data’s maze.\n",
      "\n",
      "A dusty book, a midnight screen,\n",
      "A question sparks in spaces between:\n",
      "“What secrets hide in rows and charts?\n",
      "What truth does data speak in parts?”\n",
      "\n",
      "Python scripts and messy code,\n",
      "Errors stacked a heavy load.\n",
      "Yet in each bug, a lesson found—\n",
      "Persistence is the battleground.\n",
      "\n",
      "Statistics whispers gentle clues,\n",
      "Probabilities, hidden truths.\n",
      "Linear lines and curves that bend,\n",
      "Regressions that predict, or end.\n",
      "\n",
      "A scatterplot, a clustering sphere,\n",
      "Machine’s learning, patterns clear.\n",
      "A forest random, tangled trees,\n",
      "Decision splits with cryptic ease.\n",
      "\n",
      "Stack Overflow, a trusted friend,\n",
      "To help confusion meet its end.\n",
      "Blogs and MOOCs, and podcasts too,\n",
      "Each puzzle piece reveals the view.\n",
      "\n",
      "From wrangling data’s jagged mess,\n",
      "To crafting insights, no less.\n",
      "Transforming noise into a song,\n",
      "To show the world where it belongs.\n",
      "\n",
      "And slowly, day by day it seems,\n",
      "A novice grows toward bigger dreams.\n",
      "A journey fueled by self-made spark—\n",
      "A data scientist leaves their mark.\n",
      "\n",
      "So here’s to nights of learning late,\n",
      "To algorithms small and great.\n",
      "To those who teach themselves the art,\n",
      "Of reading data’s beating heart.\n",
      "{'source': 'datascience.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Explore the loaded documents.\n",
    "print(len(docs)) # Print the number of documents loaded\n",
    "\n",
    "# print(docs[0]) # Uncomment to print the first document object\n",
    "\n",
    "# print(type(docs[0])) # Uncomment to print the type of the first document\n",
    "\n",
    "print(docs[0].page_content) # Print the content of the first document\n",
    "\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679f458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import additional modules for using HuggingFace LLM, output parsing, and prompt templates.\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (such as API keys) from a .env file.\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beacb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize the HuggingFaceEndpoint and wrap it in a ChatHuggingFace model for text generation.\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    task='text-generation'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624cde21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for summarizing a poem.\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='write a summary for the following poem - \\n {poem} ',\n",
    "    input_variables=['poem']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4495e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a string output parser to extract plain text from the model's response.\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e090007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the text file to get the document content for summarization.\n",
    "\n",
    "loader = TextLoader('datascience.txt', encoding='utf-8')\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc06383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a chain that applies the prompt, model, and parser in sequence.\n",
    "\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b54bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The poem \"The Self-Taught Data Scientist\" is a celebration of an individual who has taught themselves the skills and knowledge to become a data scientist. The poem describes the self-directed learning process, from curiosity and eagerness to engage with data, to overcoming obstacles and finding lessons in each step. It highlights the tools, techniques, and resources used by the self-taught data scientist, such as Python scripts, statistical analysis, machine learning, and online resources like Stack Overflow and MOOCs.\n",
      "\n",
      "Throughout the poem, the speaker emphasizes the importance of persistence, self-motivation, and determination in overcoming the challenges of data science. The poem also touches on the journey of transformation, from a novice to a skilled data scientist, and the satisfaction of leaving a mark on the world through insights and discoveries.\n",
      "\n",
      "Ultimately, the poem is a tribute to the self-taught data scientist, acknowledging their dedication, creativity, and passion for learning and exploration. It encourages others to pursue their own path in data science, fueled by a self-made spark and a desire to uncover the secrets and insights hidden in data.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain with the content of the first document and print the summary.\n",
    "\n",
    "print(chain.invoke({'poem':docs[0].page_content}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9cb74",
   "metadata": {},
   "source": [
    "# **PyPDF Loader**\n",
    "\n",
    "Pypdf loader is a document loader in langchain used to load content from pdf files and convert page into a document object\n",
    "\n",
    "**limitations**\n",
    "\n",
    "it uses the Pypdf library under the hood not great with scanned pdfs or complex layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f1c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.7.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Downloading pypdf-5.7.0-py3-none-any.whl (305 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.7.0\n"
     ]
    }
   ],
   "source": [
    "# Install the pypdf package for PDF document loading.\n",
    "! pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced08a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "CampusXDeepLearningCurriculum\n",
      "A.ArtificialNeuralNetworkandhowtoimprovethem\n",
      "1.BiologicalInspiration\n",
      "● Understandingtheneuronstructure● Synapsesandsignaltransmission● Howbiologicalconceptstranslatetoartificialneurons\n",
      "2.HistoryofNeuralNetworks\n",
      "● Earlymodels(Perceptron)● BackpropagationandMLPs● The\"AIWinter\"andresurgenceofneuralnetworks● Emergenceofdeeplearning\n",
      "3.PerceptronandMultilayerPerceptrons(MLP)\n",
      "● Single-layerperceptronlimitations● XORproblemandtheneedforhiddenlayers● MLParchitecture\n",
      "4. LayersandTheirFunctions\n",
      "● InputLayer○ Acceptinginputdata● HiddenLayers○ Featureextraction● OutputLayer○ Producingfinalpredictions\n",
      "5.ActivationFunctions\n",
      "{'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'dl-curriculum.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "# Import PyPDFLoader and load a PDF file as LangChain document objects.\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('dl-curriculum.pdf')\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# print(docs)  # Uncomment to print all loaded documents\n",
    "# print(docs)\n",
    "\n",
    "print(len(docs)) # Print the number of pages/documents loaded\n",
    "\n",
    "print(docs[0].page_content) # Print the content of the first page\n",
    "\n",
    "print(docs[1].metadata) # Print the metadata of the second page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cf64e8",
   "metadata": {},
   "source": [
    "pdf with tables/columns =====> PDFPlumberloader\n",
    "\n",
    "scanned/image PDFs=========> UnstrucherPDFLoader/AmazomTextracPDFLoader\n",
    "\n",
    "Need layout and image data ========> PymuPDFLoader\n",
    "\n",
    "https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_pdf.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
