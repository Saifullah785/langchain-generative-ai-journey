{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79ad5a2",
   "metadata": {},
   "source": [
    "# **Document Loaders in Langchain**\n",
    "\n",
    "Document laoders are components in langchain used to laod data from various sources into a standardized format (usually as Document objects), which can then be used for chunking, embedding , retrieval, and generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776bc2e4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# **TextLoader**\n",
    "\n",
    "Text Loader is a sample and commonly used document loader in langchain that reads plain text (.txt) files and coverts them into langchain document objects\n",
    "\n",
    " **Use Case**\n",
    "\n",
    " ideal for loading chat logs, scraped text, transcripts, code snippets or any plain text data into a langchain pipeline\n",
    "\n",
    "\n",
    " **limitation**\n",
    "\n",
    " Works only with .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c002e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Import the TextLoader for loading plain text files as LangChain document objects.\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2cf885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Create a TextLoader instance for the file 'datascience.txt' and load the documents.\n",
    "\n",
    "loader = TextLoader('datascience.txt', encoding='utf-8')\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Print the type of the loaded docs object (should be a list of Document objects).\n",
    "\n",
    "print(type(docs)) # Print the metadata of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed50294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "The Self-Taught Data Scientist\n",
      "\n",
      "In quiet rooms where pixels glow,\n",
      "A curious mind begins to grow.\n",
      "No lecture hall, no rigid pace,\n",
      "Just eager steps through data’s maze.\n",
      "\n",
      "A dusty book, a midnight screen,\n",
      "A question sparks in spaces between:\n",
      "“What secrets hide in rows and charts?\n",
      "What truth does data speak in parts?”\n",
      "\n",
      "Python scripts and messy code,\n",
      "Errors stacked a heavy load.\n",
      "Yet in each bug, a lesson found—\n",
      "Persistence is the battleground.\n",
      "\n",
      "Statistics whispers gentle clues,\n",
      "Probabilities, hidden truths.\n",
      "Linear lines and curves that bend,\n",
      "Regressions that predict, or end.\n",
      "\n",
      "A scatterplot, a clustering sphere,\n",
      "Machine’s learning, patterns clear.\n",
      "A forest random, tangled trees,\n",
      "Decision splits with cryptic ease.\n",
      "\n",
      "Stack Overflow, a trusted friend,\n",
      "To help confusion meet its end.\n",
      "Blogs and MOOCs, and podcasts too,\n",
      "Each puzzle piece reveals the view.\n",
      "\n",
      "From wrangling data’s jagged mess,\n",
      "To crafting insights, no less.\n",
      "Transforming noise into a song,\n",
      "To show the world where it belongs.\n",
      "\n",
      "And slowly, day by day it seems,\n",
      "A novice grows toward bigger dreams.\n",
      "A journey fueled by self-made spark—\n",
      "A data scientist leaves their mark.\n",
      "\n",
      "So here’s to nights of learning late,\n",
      "To algorithms small and great.\n",
      "To those who teach themselves the art,\n",
      "Of reading data’s beating heart.\n",
      "{'source': 'datascience.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Explore the loaded documents.\n",
    "print(len(docs)) # Print the number of documents loaded\n",
    "\n",
    "# print(docs[0]) # Uncomment to print the first document object\n",
    "\n",
    "# print(type(docs[0])) # Uncomment to print the type of the first document\n",
    "\n",
    "print(docs[0].page_content) # Print the content of the first document\n",
    "\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679f458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import additional modules for using HuggingFace LLM, output parsing, and prompt templates.\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (such as API keys) from a .env file.\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beacb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize the HuggingFaceEndpoint and wrap it in a ChatHuggingFace model for text generation.\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    task='text-generation'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624cde21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for summarizing a poem.\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='write a summary for the following poem - \\n {poem} ',\n",
    "    input_variables=['poem']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4495e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a string output parser to extract plain text from the model's response.\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e090007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the text file to get the document content for summarization.\n",
    "\n",
    "loader = TextLoader('datascience.txt', encoding='utf-8')\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc06383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a chain that applies the prompt, model, and parser in sequence.\n",
    "\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b54bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The poem \"The Self-Taught Data Scientist\" is a celebration of an individual who has taught themselves the skills and knowledge to become a data scientist. The poem describes the self-directed learning process, from curiosity and eagerness to engage with data, to overcoming obstacles and finding lessons in each step. It highlights the tools, techniques, and resources used by the self-taught data scientist, such as Python scripts, statistical analysis, machine learning, and online resources like Stack Overflow and MOOCs.\n",
      "\n",
      "Throughout the poem, the speaker emphasizes the importance of persistence, self-motivation, and determination in overcoming the challenges of data science. The poem also touches on the journey of transformation, from a novice to a skilled data scientist, and the satisfaction of leaving a mark on the world through insights and discoveries.\n",
      "\n",
      "Ultimately, the poem is a tribute to the self-taught data scientist, acknowledging their dedication, creativity, and passion for learning and exploration. It encourages others to pursue their own path in data science, fueled by a self-made spark and a desire to uncover the secrets and insights hidden in data.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain with the content of the first document and print the summary.\n",
    "\n",
    "print(chain.invoke({'poem':docs[0].page_content}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9cb74",
   "metadata": {},
   "source": [
    "# **PyPDF Loader**\n",
    "\n",
    "Pypdf loader is a document loader in langchain used to load content from pdf files and convert page into a document object\n",
    "\n",
    "**limitations**\n",
    "\n",
    "it uses the Pypdf library under the hood not great with scanned pdfs or complex layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f1c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.7.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Downloading pypdf-5.7.0-py3-none-any.whl (305 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.7.0\n"
     ]
    }
   ],
   "source": [
    "# Install the pypdf package for PDF document loading.\n",
    "! pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced08a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "CampusXDeepLearningCurriculum\n",
      "A.ArtificialNeuralNetworkandhowtoimprovethem\n",
      "1.BiologicalInspiration\n",
      "● Understandingtheneuronstructure● Synapsesandsignaltransmission● Howbiologicalconceptstranslatetoartificialneurons\n",
      "2.HistoryofNeuralNetworks\n",
      "● Earlymodels(Perceptron)● BackpropagationandMLPs● The\"AIWinter\"andresurgenceofneuralnetworks● Emergenceofdeeplearning\n",
      "3.PerceptronandMultilayerPerceptrons(MLP)\n",
      "● Single-layerperceptronlimitations● XORproblemandtheneedforhiddenlayers● MLParchitecture\n",
      "4. LayersandTheirFunctions\n",
      "● InputLayer○ Acceptinginputdata● HiddenLayers○ Featureextraction● OutputLayer○ Producingfinalpredictions\n",
      "5.ActivationFunctions\n",
      "{'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'dl-curriculum.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "# Import PyPDFLoader and load a PDF file as LangChain document objects.\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('dl-curriculum.pdf')\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# print(docs)  # Uncomment to print all loaded documents\n",
    "# print(docs)\n",
    "\n",
    "print(len(docs)) # Print the number of pages/documents loaded\n",
    "\n",
    "print(docs[0].page_content) # Print the content of the first page\n",
    "\n",
    "print(docs[1].metadata) # Print the metadata of the second page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cf64e8",
   "metadata": {},
   "source": [
    "pdf with tables/columns =====> PDFPlumberloader\n",
    "\n",
    "scanned/image PDFs=========> UnstrucherPDFLoader/AmazomTextracPDFLoader\n",
    "\n",
    "Need layout and image data ========> PymuPDFLoader\n",
    "\n",
    "https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_pdf.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108896e",
   "metadata": {},
   "source": [
    "# **directory_loader**\n",
    "\n",
    "Load vs Lazy load\n",
    "\n",
    "DirectoryLoader in LangChain is a utility that allows you to efficiently load multiple documents from a directory, applying a specified loader (such as for PDFs or text files) to each file that matches a given pattern. \n",
    "\n",
    "The distinction between Load vs Lazy load is important: \n",
    "\n",
    "\"Load\" reads all documents into memory at once, which is fast for small datasets but can be memory-intensive for large ones, while \"Lazy load\" processes documents one at a time as needed, making it more memory-efficient and suitable for handling large collections of files without overwhelming system resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f4cd107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DirectoryLoader for loading multiple files from a directory,\n",
    "# and PyPDFLoader for loading PDF files as LangChain document objects.\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70b7807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DirectoryLoader instance to load all PDF files from the 'books' directory using PyPDFLoader.\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path='books',               # Directory containing PDF files\n",
    "    glob='*.pdf',               # Pattern to match PDF files\n",
    "    loader_cls=PyPDFLoader      # Loader class to use for each file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2b7fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all documents from the specified directory using the loader.\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b636c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iants, such as train_on_batch() or fit_generator()), plus the get_layers()\n",
      "method (which can return any of the model’s layers by name or by index), and the\n",
      "save() method (and support for keras.models.load_model() and keras.mod\n",
      "els.clone_model()). So if models provide more functionalities than layers, why not\n",
      "just define every layer as a model? Well, technically you could, but it is probably\n",
      "cleaner to distinguish the internal components of your model (layers or reusable\n",
      "blocks of layers) from the model itself. The former should subclass the Layer class,\n",
      "while the latter should subclass the Model class.\n",
      "With that, you can quite naturally and concisely build almost any model that you find\n",
      "in a paper, either using the sequential API, the functional API, the subclassing API, or\n",
      "even a mix of these. “ Almost” any model? Y es, there are still a couple things that we\n",
      "need to look at: first, how to define losses or metrics based on model internals, and\n",
      "second how to build a custom training loop.\n",
      "Losses and Metrics Based on Model Internals\n",
      "The custom losses and metrics we defined earlier were all based on the labels and the\n",
      "predictions (and optionally sample weights). However, you will occasionally want to\n",
      "define losses based on other parts of your model, such as the weights or activations of\n",
      "its hidden layers. This may be useful for regularization purposes, or to monitor some\n",
      "internal aspect of your model.\n",
      "To define a custom loss based on model internals, just compute it based on any part\n",
      "of the model you want, then pass the result to the add_loss() method. For example,\n",
      "the following custom model represents a standard MLP regressor with 5 hidden lay‐\n",
      "ers, except it also implements a reconstruction loss (see ???): we add an extra Dense\n",
      "layer on top of the last hidden layer, and its role is to try to reconstruct the inputs of\n",
      "the model. Since the reconstruction must have the same shape as the model’s inputs,\n",
      "we need to create this Dense layer in the build() method to have access to the shape\n",
      "of the inputs. In the call() method, we compute both the regular output of the MLP ,\n",
      "plus the output of the reconstruction layer. We then compute the mean squared dif‐\n",
      "ference between the reconstructions and the inputs, and we add this value (times\n",
      "0.05) to the model’s list of losses by calling add_loss(). During training, Keras will\n",
      "add this loss to the main loss (which is why we scaled down the reconstruction loss,\n",
      "to ensure the main loss dominates). As a result, the model will be forced to preserve\n",
      "as much information as possible through the hidden layers, even information that is\n",
      "not directly useful for the regression task itself. In practice, this loss sometimes\n",
      "improves generalization; it is a regularization loss:\n",
      "class ReconstructingRegressor(keras.models.Model):\n",
      "    def __init__(self, output_dim, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
      "                                          kernel_initializer=\"lecun_normal\")\n",
      "388 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "for _ in range(5)]\n",
      "        self.out = keras.layers.Dense(output_dim)\n",
      "    def build(self, batch_input_shape):\n",
      "        n_inputs = batch_input_shape[-1]\n",
      "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
      "        super().build(batch_input_shape)\n",
      "    def call(self, inputs):\n",
      "        Z = inputs\n",
      "        for layer in self.hidden:\n",
      "            Z = layer(Z)\n",
      "        reconstruction = self.reconstruct(Z)\n",
      "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
      "        self.add_loss(0.05 * recon_loss)\n",
      "        return self.out(Z)\n",
      "Similarly, you can add a custom metric based on model internals by computing it in\n",
      "any way you want, as long at the result is the output of a metric object. For example,\n",
      "you can create a keras.metrics.Mean() object in the constructor, then call it in the\n",
      "call() method, passing it the recon_loss, and finally add it to the model by calling\n",
      "the model’s add_metric() method. This way, when you train the model, Keras will\n",
      "display both the mean loss over each epoch (the loss is the sum of the main loss plus\n",
      "0.05 times the reconstruction loss) and the mean reconstruction error over each\n",
      "epoch. Both will go down during training:\n",
      "Epoch 1/5\n",
      "11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\n",
      "Epoch 2/5\n",
      "11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\n",
      "[...]\n",
      "In over 99% of the cases, everything we have discussed so far will be sufficient to\n",
      "implement whatever model you want to build, even with complex architectures, los‐\n",
      "ses, metrics, and so on. However, in some rare cases you may need to customize the\n",
      "training loop itself. However, before we get there, we need to look at how to compute\n",
      "gradients automatically in TensorFlow.\n",
      "Computing Gradients Using Autodiff\n",
      "To understand how to use autodiff (see Chapter 10  and ???) to compute gradients\n",
      "automatically, let’s consider a simple toy function:\n",
      "def f(w1, w2):\n",
      "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
      "If you know calculus, you can analytically find that the partial derivative of this func‐\n",
      "tion with regards to w1 is 6 * w1 + 2 * w2. Y ou can also find that its partial derivative\n",
      "with regards to w2 is 2 * w1. For example, at the point (w1, w2) = (5, 3), these par‐\n",
      "Customizing Models and Training Algorithms | 389\n",
      "tial derivatives are equal to 36 and 10, respectively, so the gradient vector at this point\n",
      "is (36, 10). But if this were a neural network, the function would be much more com‐\n",
      "plex, typically with tens of thousands of parameters, and finding the partial deriva‐\n",
      "tives analytically by hand would be an almost impossible task. One solution could be\n",
      "to compute an approximation of each partial derivative by measuring how much the\n",
      "function’s output changes when you tweak the corresponding parameter:\n",
      ">>> w1, w2 = 5, 3\n",
      ">>> eps = 1e-6\n",
      ">>> (f(w1 + eps, w2) - f(w1, w2)) / eps\n",
      "36.000003007075065\n",
      ">>> (f(w1, w2 + eps) - f(w1, w2)) / eps\n",
      "10.000000003174137\n",
      "Looks about right! This works rather well and it is trivial to implement, but it is just\n",
      "an approximation, and importantly you need to call f() at least once per parameter\n",
      "(not twice, since we could compute f(w1, w2) just once). This makes this approach\n",
      "intractable for large neural networks. So instead we should use autodiff (see Chap‐\n",
      "ter 10 and ???). TensorFlow makes this pretty simple:\n",
      "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
      "with tf.GradientTape() as tape:\n",
      "    z = f(w1, w2)\n",
      "gradients = tape.gradient(z, [w1, w2])\n",
      "We first define two variables w1 and w2, then we create a tf.GradientTape context\n",
      "that will automatically record every operation that involves a variable, and finally we\n",
      "ask this tape to compute the gradients of the result z with regards to both variables\n",
      "[w1, w2]. Let’s take a look at the gradients that TensorFlow computed:\n",
      ">>> gradients\n",
      "[<tf.Tensor: id=828234, shape=(), dtype=float32, numpy=36.0>,\n",
      " <tf.Tensor: id=828229, shape=(), dtype=float32, numpy=10.0>]\n",
      "Perfect! Not only is the result accurate (the precision is only limited by the floating\n",
      "point errors), but the gradient() method only goes through the recorded computa‐\n",
      "tions once (in reverse order), no matter how many variables there are, so it is incredi‐\n",
      "bly efficient. It’s like magic!\n",
      "Only put the strict minimum inside the tf.GradientTape() block,\n",
      "to save memory. Alternatively, you can pause recording by creating\n",
      "a with tape.stop_recording()  block inside the tf.Gradient\n",
      "Tape() block.\n",
      "The tape is automatically erased immediately after you call its gradient() method, so\n",
      "you will get an exception if you try to call gradient() twice:\n",
      "390 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "with tf.GradientTape() as tape:\n",
      "    z = f(w1, w2)\n",
      "dz_dw1 = tape.gradient(z, w1) # => tensor 36.0\n",
      "dz_dw2 = tape.gradient(z, w2) # RuntimeError!\n",
      "If you need to call gradient() more than once, you must make the tape persistent,\n",
      "and delete it when you are done with it to free resources:\n",
      "with tf.GradientTape(persistent=True) as tape:\n",
      "    z = f(w1, w2)\n",
      "dz_dw1 = tape.gradient(z, w1) # => tensor 36.0\n",
      "dz_dw2 = tape.gradient(z, w2) # => tensor 10.0, works fine now!\n",
      "del tape\n",
      "By default, the tape will only track operations involving variables, so if you try to\n",
      "compute the gradient of z with regards to anything else than a variable, the result will\n",
      "be None:\n",
      "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
      "with tf.GradientTape() as tape:\n",
      "    z = f(c1, c2)\n",
      "gradients = tape.gradient(z, [c1, c2]) # returns [None, None]\n",
      "However, you can force the tape to watch any tensors you like, to record every opera‐\n",
      "tion that involves them. Y ou can then compute gradients with regards to these ten‐\n",
      "sors, as if they were variables:\n",
      "with tf.GradientTape() as tape:\n",
      "    tape.watch(c1)\n",
      "    tape.watch(c2)\n",
      "    z = f(c1, c2)\n",
      "gradients = tape.gradient(z, [c1, c2]) # returns [tensor 36., tensor 10.]\n",
      "This can be useful in some cases, for example if you want to implement a regulariza‐\n",
      "tion loss that penalizes activations that vary a lot when the inputs vary little: the loss\n",
      "will be based on the gradient of the activations with regards to the inputs. Since the\n",
      "inputs are not variables, you would need to tell the tape to watch them.\n",
      "If you compute the gradient of a list of tensors (e.g., [z1, z2, z3]) with regards to\n",
      "some variables (e.g., [w1, w2]), TensorFlow actually efficiently computes the sum of\n",
      "the gradients of these tensors (i.e., gradient(z1, [w1, w2]) , plus gradient(z2,\n",
      "[w1, w2]), plus gradient(z3, [w1, w2])). Due to the way reverse-mode autodiff\n",
      "works, it is not possible to compute the individual gradients ( z1, z2 and z3) without\n",
      "actually calling gradient() multiple times (once for z1, once for z2 and once for z3),\n",
      "which requires making the tape persistent (and deleting it afterwards).\n",
      "Customizing Models and Training Algorithms | 391\n",
      "Moreover, it is actually possible to compute second order partial derivatives (the Hes‐\n",
      "sians, i.e., the partial derivatives of the partial derivatives)! To do this, we need to\n",
      "record the operations that are performed when computing the first-order partial\n",
      "derivatives (the Jacobians): this requires a second tape. Here is how it works:\n",
      "with tf.GradientTape(persistent=True) as hessian_tape:\n",
      "    with tf.GradientTape() as jacobian_tape:\n",
      "        z = f(w1, w2)\n",
      "    jacobians = jacobian_tape.gradient(z, [w1, w2])\n",
      "hessians = [hessian_tape.gradient(jacobian, [w1, w2])\n",
      "            for jacobian in jacobians]\n",
      "del hessian_tape\n",
      "The inner tape is used to compute the Jacobians, as we did earlier. The outer tape is\n",
      "used to compute the partial derivatives of each Jacobian. Since we need to call gradi\n",
      "ent() once for each Jacobian (or else we would get the sum of the partial derivatives\n",
      "over all the Jabobians, as explained earlier), we need the outer tape to be persistent, so\n",
      "we delete it at the end. The Jacobians are obviously the same as earlier (36 and 5), but\n",
      "now we also have the Hessians:\n",
      ">>> hessians # dz_dw1_dw1, dz_dw1_dw2, dz_dw2_dw1, dz_dw2_dw2\n",
      "[[<tf.Tensor: id=830578, shape=(), dtype=float32, numpy=6.0>,\n",
      "  <tf.Tensor: id=830595, shape=(), dtype=float32, numpy=2.0>],\n",
      " [<tf.Tensor: id=830600, shape=(), dtype=float32, numpy=2.0>, None]]\n",
      "Let’s verify these Hessians. The first two are the partial derivatives of 6 * w1 + 2 * w2\n",
      "(which is, as we saw earlier, the partial derivative of f with regards to w1), with\n",
      "regards to w1 and w2. The result is correct: 6 for w1 and 2 for w2. The next two are the\n",
      "partial derivatives of 2 * w1  (the partial derivative of f with regards to w2), with\n",
      "regards to w1 and w2, which are 2 for w1 and 0 for w2. Note that TensorFlow returns\n",
      "None instead of 0 since w2 does not appear at all in 2 * w1. TensorFlow also returns\n",
      "None when you use an operation whose gradients are not defined (e.g., tf.argmax()).\n",
      "In some rare cases you may want to stop gradients from backpropagating through\n",
      "some part of your neural network. To do this, you must use the tf.stop_gradient()\n",
      "function: it just returns its inputs during the forward pass (like tf.identity()), but\n",
      "it does not let gradients through during backpropagation (it acts like a constant). For\n",
      "example:\n",
      "def f(w1, w2):\n",
      "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
      "with tf.GradientTape() as tape:\n",
      "    z = f(w1, w2) # same result as without stop_gradient()\n",
      "gradients = tape.gradient(z, [w1, w2]) # => returns [tensor 30., None]\n",
      "392 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "Finally, you may occasionally run into some numerical issues when computing gradi‐\n",
      "ents. For example, if you compute the gradients of the my_softplus() function for\n",
      "large inputs, the result will be NaN:\n",
      ">>> x = tf.Variable([100.])\n",
      ">>> with tf.GradientTape() as tape:\n",
      "...     z = my_softplus(x)\n",
      "...\n",
      ">>> tape.gradient(z, [x])\n",
      "<tf.Tensor: [...] numpy=array([nan], dtype=float32)>\n",
      "This is because computing the gradients of this function using autodiff leads to some\n",
      "numerical difficulties: due to floating point precision errors, autodiff ends up com‐\n",
      "puting infinity divided by infinity (which returns NaN). Fortunately, we can analyti‐\n",
      "cally find that the derivative of the softplus function is just 1 / (1 + 1 / exp(x)), which\n",
      "is numerically stable. Next, we can tell TensorFlow to use this stable function when\n",
      "computing the gradients of the my_softplus() function, by decorating it with\n",
      "@tf.custom_gradient, and making it return both its normal output and the function\n",
      "that computes the derivatives (note that it will receive as input the gradients that were\n",
      "backpropagated so far, down to the softplus function, and according to the chain rule\n",
      "we should multiply them with this function’s gradients):\n",
      "@tf.custom_gradient\n",
      "def my_better_softplus(z):\n",
      "    exp = tf.exp(z)\n",
      "    def my_softplus_gradients(grad):\n",
      "        return grad / (1 + 1 / exp)\n",
      "    return tf.math.log(exp + 1), my_softplus_gradients\n",
      "Now when we compute the gradients of the my_better_softplus() function, we get\n",
      "the proper result, even for large input values (however, the main output still explodes\n",
      "because of the exponential: one workaround is to use tf.where() to just return the\n",
      "inputs when they are large).\n",
      "Congratulations! Y ou can now compute the gradients of any function (provided it is\n",
      "differentiable at the point where you compute it), you can even compute Hessians,\n",
      "block backpropagation when needed and even write your own gradient functions!\n",
      "This is probably more flexibility than you will ever need, even if you build your own\n",
      "custom training loops, as we will see now.\n",
      "Custom Training Loops\n",
      "In some rare cases, the fit() method may not be flexible enough for what you need\n",
      "to do. For example, the Wide and Deep paper we discussed in Chapter 10 actually\n",
      "uses two different optimizers: one for the wide path and the other for the deep path.\n",
      "Since the fit() method only uses one optimizer (the one that we specify when\n",
      "Customizing Models and Training Algorithms | 393\n",
      "compiling the model), implementing this paper requires writing your own custom\n",
      "loop.\n",
      "Y ou may also like to write your own custom training loops simply to feel more confi‐\n",
      "dent that it does precisely what you intent it to do (perhaps you are unsure about\n",
      "some details of the fit() method). It can sometimes feel safer to make everything\n",
      "explicit. However, remember that writing a custom training loop will make your code\n",
      "longer, more error prone and harder to maintain.\n",
      "Unless you really need the extra flexibility, you should prefer using\n",
      "the fit() method rather than implementing your own training\n",
      "loop, especially if you work in a team.\n",
      "First, let’s build a simple model. No need to compile it, since we will handle the train‐\n",
      "ing loop manually:\n",
      "l2_reg = keras.regularizers.l2(0.05)\n",
      "model = keras.models.Sequential([\n",
      "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
      "                       kernel_regularizer=l2_reg),\n",
      "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
      "])\n",
      "Next, let’s create a tiny function that will randomly sample a batch of instances from\n",
      "the training set (in Chapter 13 we will discuss the Data API, which offers a much bet‐\n",
      "ter alternative):\n",
      "def random_batch(X, y, batch_size=32):\n",
      "    idx = np.random.randint(len(X), size=batch_size)\n",
      "    return X[idx], y[idx]\n",
      "Let’s also define a function that will display the training status, including the number\n",
      "of steps, the total number of steps, the mean loss since the start of the epoch (i.e., we\n",
      "will use the Mean metric to compute it), and other metrics:\n",
      "def print_status_bar(iteration, total, loss, metrics=None):\n",
      "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
      "                         for m in [loss] + (metrics or [])])\n",
      "    end = \"\" if iteration < total else \"\\n\"\n",
      "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
      "          end=end)\n",
      "This code is self-explanatory, unless you are unfamiliar with Python string format‐\n",
      "ting: {:.4f} will format a float with 4 digits after the decimal point. Moreover, using\n",
      "\\r (carriage return) along with end=\"\" ensures that the status bar always gets printed\n",
      "on the same line. In the notebook, the print_status_bar() function also includes a\n",
      "progress bar, but you could use the handy tqdm library instead.\n",
      "394 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "With that, let’s get down to business! First, we need to define some hyperparameters,\n",
      "choose the optimizer, the loss function and the metrics (just the MAE in this exam‐\n",
      "ple):\n",
      "n_epochs = 5\n",
      "batch_size = 32\n",
      "n_steps = len(X_train) // batch_size\n",
      "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
      "loss_fn = keras.losses.mean_squared_error\n",
      "mean_loss = keras.metrics.Mean()\n",
      "metrics = [keras.metrics.MeanAbsoluteError()]\n",
      "And now we are ready to build the custom loop!\n",
      "for epoch in range(1, n_epochs + 1):\n",
      "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
      "    for step in range(1, n_steps + 1):\n",
      "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
      "        with tf.GradientTape() as tape:\n",
      "            y_pred = model(X_batch, training=True)\n",
      "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
      "            loss = tf.add_n([main_loss] + model.losses)\n",
      "        gradients = tape.gradient(loss, model.trainable_variables)\n",
      "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
      "        mean_loss(loss)\n",
      "        for metric in metrics:\n",
      "            metric(y_batch, y_pred)\n",
      "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
      "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
      "    for metric in [mean_loss] + metrics:\n",
      "        metric.reset_states()\n",
      "There’s a lot going on in this code, so let’s walk through it:\n",
      "• We create two nested loops: one for the epochs, the other for the batches within\n",
      "an epoch.\n",
      "• Then we sample a random batch from the training set.\n",
      "• Inside the tf.GradientTape() block, we make a prediction for one batch (using\n",
      "the model as a function), and we compute the loss: it is equal to the main loss\n",
      "plus the other losses (in this model, there is one regularization loss per layer).\n",
      "Since the mean_squared_error() function returns one loss per instance, we\n",
      "compute the mean over the batch using tf.reduce_mean() (if you wanted to\n",
      "apply different weights to each instance, this is where you would do it). The regu‐\n",
      "larization losses are already reduced to a single scalar each, so we just need to\n",
      "sum them (using tf.add_n(), which sums multiple tensors of the same shape\n",
      "and data type).\n",
      "Customizing Models and Training Algorithms | 395\n",
      "11 The truth is we did not process every single instance in the training set because we sampled instances ran‐\n",
      "domly, so some were processed more than once while others were not processed at all. In practice that’s fine.\n",
      "Moreover, if the training set size is not a multiple of the batch size, we will miss a few instances.\n",
      "12 Alternatively, check out K.learning_phase(), K.set_learning_phase() and K.learning_phase_scope().\n",
      "13 With the exception of optimizers, as very few people ever customize these: see the notebook for an example.\n",
      "• Next, we ask the tape to compute the gradient of the loss with regards to each\n",
      "trainable variable (not all variables!), and we apply them to the optimizer to per‐\n",
      "form a Gradient Descent step.\n",
      "• Next we update the mean loss and the metrics (over the current epoch), and we\n",
      "display the status bar.\n",
      "• At the end of each epoch, we display the status bar again to make it look com‐\n",
      "plete11 and to print a line feed, and we reset the states of the mean loss and the\n",
      "metrics.\n",
      "If you set the optimizer’s clipnorm or clipvalue hyperparameters, it will take care of\n",
      "this for you. If you want to apply any other transformation to the gradients, simply do\n",
      "so before calling the apply_gradients() method.\n",
      "If you add weight constraints to your model (e.g., by setting kernel_constraint or\n",
      "bias_constraint when creating a layer), you should update the training loop to\n",
      "apply these constraints just after apply_gradients():\n",
      "for variable in model.variables:\n",
      "    if variable.constraint is not None:\n",
      "        variable.assign(variable.constraint(variable))\n",
      "Most importantly, this training loop does not handle layers that behave differently\n",
      "during training and testing (e.g., BatchNormalization or Dropout). To handle these,\n",
      "you need to call the model with training=True and make sure it propagates this to\n",
      "every layer that needs it.12\n",
      "As you can see, there are quite a lot of things you need to get right, it is easy to make a\n",
      "mistake. But on the bright side, you get full control, so it’s your call.\n",
      "Now that you know how to customize any part of your models 13 and training algo‐\n",
      "rithms, let’s see how you can use TensorFlow’s automatic graph generation feature: it\n",
      "can speed up your custom code considerably, and it will also make it portable to any\n",
      "platform supported by TensorFlow (see ???).\n",
      "TensorFlow Functions and Graphs\n",
      "In TensorFlow 1, graphs were unavoidable (as were the complexities that came with\n",
      "them): they were a central part of TensorFlow’s API. In TensorFlow 2, they are still\n",
      "396 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "there, but not as central, and much (much!) simpler to use. To demonstrate this, let’s\n",
      "start with a trivial function that just computes the cube of its input:\n",
      "def cube(x):\n",
      "    return x ** 3\n",
      "We can obviously call this function with a Python value, such as an int or a float, or\n",
      "we can call it with a tensor:\n",
      ">>> cube(2)\n",
      "8\n",
      ">>> cube(tf.constant(2.0))\n",
      "<tf.Tensor: id=18634148, shape=(), dtype=float32, numpy=8.0>\n",
      "Now, let’s use tf.function() to convert this Python function to a TensorFlow Func‐\n",
      "tion:\n",
      ">>> tf_cube = tf.function(cube)\n",
      ">>> tf_cube\n",
      "<tensorflow.python.eager.def_function.Function at 0x1546fc080>\n",
      "This TF Function can then be used exactly like the original Python function, and it\n",
      "will return the same result (but as tensors):\n",
      ">>> tf_cube(2)\n",
      "<tf.Tensor: id=18634201, shape=(), dtype=int32, numpy=8>\n",
      ">>> tf_cube(tf.constant(2.0))\n",
      "<tf.Tensor: id=18634211, shape=(), dtype=float32, numpy=8.0>\n",
      "Under the hood, tf.function() analyzed the computations performed by the cube()\n",
      "function and generated an equivalent computation graph! As you can see, it was\n",
      "rather painless (we will see how this works shortly). Alternatively, we could have used\n",
      "tf.function as a decorator; this is actually more common:\n",
      "@tf.function\n",
      "def tf_cube(x):\n",
      "    return x ** 3\n",
      "The original Python function is still available via the TF Function’s python_function\n",
      "attribute, in case you ever need it:\n",
      ">>> tf_cube.python_function(2)\n",
      "8\n",
      "TensorFlow optimizes the computation graph, pruning unused nodes, simplifying\n",
      "expressions (e.g., 1 + 2 would get replaced with 3), and more. Once the optimized\n",
      "graph is ready, the TF Function efficiently executes the operations in the graph, in the\n",
      "appropriate order (and in parallel when it can). As a result, a TF Function will usually\n",
      "run much faster than the original Python function, especially if it performs complex\n",
      "TensorFlow Functions and Graphs | 397\n",
      "14 However, in this trivial example, the computation graph is so small that there is nothing at all to optimize, so\n",
      "tf_cube() actually runs much slower than cube().\n",
      "computations.14 Most of the time you will not really need to know more than that:\n",
      "when you want to boost a Python function, just transform it into a TF Function.\n",
      "That’s all!\n",
      "Moreover, when you write a custom loss function, a custom metric, a custom layer or\n",
      "any other custom function, and you use it in a Keras model (as we did throughout\n",
      "this chapter), Keras automatically converts your function into a TF Function, no need\n",
      "to use tf.function(). So most of the time, all this magic is 100% transparent.\n",
      "Y ou can tell Keras not to convert your Python functions to TF\n",
      "Functions by setting dynamic=True when creating a custom layer\n",
      "or a custom model. Alternatively, you can set run_eagerly=True\n",
      "when calling the model’s compile() method.\n",
      "TF Function generates a new graph for every unique set of input shapes and data\n",
      "types, and it caches it for subsequent calls. For example, if you call tf_cube(tf.con\n",
      "stant(10)), a graph will be generated for int32 tensors of shape []. Then if you call\n",
      "tf_cube(tf.constant(20)), the same graph will be reused. But if you then call\n",
      "tf_cube(tf.constant([10, 20])), a new graph will be generated for int32 tensors\n",
      "of shape [2]. This is how TF Functions handle polymorphism (i.e., varying argument\n",
      "types and shapes). However, this is only true for tensor arguments: if you pass numer‐\n",
      "ical Python values to a TF Function, a new graph will be generated for every distinct\n",
      "value: for example, calling tf_cube(10) and tf_cube(20) will generate two graphs.\n",
      "If you call a TF Function many times with different numerical\n",
      "Python values, then many graphs will be generated, slowing down\n",
      "your program and using up a lot of RAM. Python values should be\n",
      "reserved for arguments that will have few unique values, such as\n",
      "hyperparameters like the number of neurons per layer. This allows\n",
      "TensorFlow to better optimize each variant of your model.\n",
      "Autograph and Tracing\n",
      "So how does TensorFlow generate graphs? Well, first it starts by analyzing the Python\n",
      "function’s source code to capture all the control flow statements, such as for loops\n",
      "and while loops, if statements, as well as break, continue and return statements.\n",
      "This first step is called autograph. The reason TensorFlow has to analyze the source\n",
      "code is that Python does not provide any other way to capture control flow state‐\n",
      "ments: it offers magic methods like __add__() or __mul__() to capture operators like\n",
      "398 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "+ and *, but there are no __while__() or __if__() magic methods. After analyzing\n",
      "the function’s code, autograph outputs an upgraded version of that function in which\n",
      "all the control flow statements are replaced by the appropriate TensorFlow opera‐\n",
      "tions, such as tf.while_loop() for loops and tf.cond() for if statements. For\n",
      "example, in Figure 12-4 , autograph analyzes the source code of the sum_squares()\n",
      "Python function, and it generates the tf__sum_squares() function. In this function,\n",
      "the for loop is replaced by the definition of the loop_body() function (containing\n",
      "the body of the original for loop), followed by a call to the for_stmt() function. This\n",
      "call will build the appropriate tf.while_loop() operation in the computation graph.\n",
      "Figure 12-4. How TensorFlow generates graphs using autograph and tracing\n",
      "Next, TensorFlow calls this “upgraded” function, but instead of passing the actual\n",
      "argument, it passes a symbolic tensor, meaning a tensor without any actual value, only\n",
      "a name, a data type, and a shape. For example, if you call sum_squares(tf.con\n",
      "stant(10)), then the tf__sum_squares() function will actually be called with a sym‐\n",
      "bolic tensor of type int32 and shape []. The function will run in graph mode, meaning\n",
      "that each TensorFlow operation will just add a node in the graph to represent itself\n",
      "and its output tensor(s) (as opposed to the regular mode, called eager execution, or\n",
      "eager mode). In graph mode, TF operations do not perform any actual computations.\n",
      "This should feel familiar if you know TensorFlow 1, as graph mode was the default\n",
      "mode. In Figure 12-4, you can see the tf__sum_squares() function being called with\n",
      "a symbolic tensor as argument (in this case, an int32 tensor of shape []), and the final\n",
      "graph generated during tracing. The ellipses represent operations, and the arrows\n",
      "represent tensors (both the generated function and the graph are simplified).\n",
      "TensorFlow Functions and Graphs | 399\n",
      "To view the generated function’s source code, you can call tf.auto\n",
      "graph.to_code(sum_squares.python_function). The code is not\n",
      "meant to be pretty, but it can sometimes help for debugging.\n",
      "TF Function Rules\n",
      "Most of the time, converting a Python function that performs TensorFlow operations\n",
      "into a TF Function is trivial: just decorate it with @tf.function or let Keras take care\n",
      "of it for you. However, there are a few rules to respect:\n",
      "• If you call any external library, including NumPy or even the standard library,\n",
      "this call will run only during tracing, it will not be part of the graph. Indeed, a\n",
      "TensorFlow graph can only include TensorFlow constructs (tensors, operations,\n",
      "variables, datasets, and so on). So make sure you use tf.reduce_sum() instead of\n",
      "np.sum(), and tf.sort() instead of the built-in sorted() function, and so on\n",
      "(unless you really want the code to run only during tracing).\n",
      "— For example, if you define a TF function f(x) that just returns np.ran\n",
      "dom.rand(), a random number will only be generated when the function is\n",
      "traced, so f(tf.constant(2.)) and f(tf.constant(3.)) will return the\n",
      "same random number, but f(tf.constant([2., 3.])) will return a different\n",
      "one. If you replace np.random.rand() with tf.random.uniform([]), then a\n",
      "new random number will be generated upon every call, since the operation\n",
      "will be part of the graph.\n",
      "— If your non-TensorFlow code has side-effects (such as logging something or\n",
      "updating a Python counter), then you should not expect that side-effect to\n",
      "occur every time you call the TF Function, as it will only occur when the func‐\n",
      "tion is traced.\n",
      "— Y ou can wrap arbitrary Python code in a tf.py_function() operation, but\n",
      "this will hinder performance, as TensorFlow will not be able to do any graph\n",
      "optimization on this code, and it will also reduce portability, as the graph will\n",
      "only run on platforms where Python is available (and the right libraries\n",
      "installed).\n",
      "• Y ou can call other Python functions or TF Functions, but they should follow the\n",
      "same rules, as TensorFlow will also capture their operations in the computation\n",
      "graph. Note that these other functions do not need to be decorated with\n",
      "@tf.function.\n",
      "• If the function creates a TensorFlow variable (or any other stateful TensorFlow\n",
      "object, such as a dataset or a queue), it must do so upon the very first call, and\n",
      "only then, or else you will get an exception. It is usually preferable to create vari‐\n",
      "ables outside of the TF Function (e.g., in the build() method of a custom layer).\n",
      "400 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "• The source code of your Python function should be available to TensorFlow. If\n",
      "the source code is unavailable (for example, if you define your function in the\n",
      "Python shell, which does not give access to the source code, or if you deploy only\n",
      "the compiled Python files *.pyc to production), then the graph generation pro‐\n",
      "cess will fail or have limited functionality.\n",
      "• TensorFlow will only capture for loops that iterate over a tensor or a Dataset. So\n",
      "make sure you use for i in tf.range(10) rather than for i in range(10), or\n",
      "else the loop will not be captured in the graph. Instead, it will run during tracing.\n",
      "This may be what you want, if the for loop is meant to build the graph, for exam‐\n",
      "ple to create each layer in a neural network.\n",
      "• And as always, for performance reasons, you should prefer a vectorized imple‐\n",
      "mentation whenever you can, rather than using loops.\n",
      "It’s time to sum up! In this chapter we started with a brief overview of TensorFlow,\n",
      "then we looked at TensorFlow’s low-level API, including tensors, operations, variables\n",
      "and special data structures. We then used these tools to customize almost every com‐\n",
      "ponent in tf.keras. Finally, we looked at how TF Functions can boost performance,\n",
      "how graphs are generated using autograph and tracing, and what rules to follow when\n",
      "you write TF Functions (if you would like to open the black box a bit further, for\n",
      "example to explore the generated graphs, you will find further technical details\n",
      "in ???).\n",
      "In the next chapter, we will look at how to efficiently load and preprocess data with\n",
      "TensorFlow.\n",
      "TensorFlow Functions and Graphs | 401\n",
      "\n",
      "CHAPTER 13\n",
      "Loading and Preprocessing Data with\n",
      "TensorFlow\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 13 in the final\n",
      "release of the book.\n",
      "So far we have used only datasets that fit in memory, but Deep Learning systems are\n",
      "often trained on very large datasets that will not fit in RAM. Ingesting a large dataset\n",
      "and preprocessing it efficiently can be tricky to implement with other Deep Learning\n",
      "libraries, but TensorFlow makes it easy thanks to the Data API: you just create a data‐\n",
      "set object, tell it where to get the data, then transform it in any way you want, and\n",
      "TensorFlow takes care of all the implementation details, such as multithreading,\n",
      "queuing, batching, prefetching, and so on.\n",
      "Off the shelf, the Data API can read from text files (such as CSV files), binary files\n",
      "with fixed-size records, and binary files that use TensorFlow’s TFRecord format,\n",
      "which supports records of varying sizes. TFRecord is a flexible and efficient binary\n",
      "format based on Protocol Buffers (an open source binary format). The Data API also\n",
      "has support for reading from SQL databases. Moreover, many Open Source exten‐\n",
      "sions are available to read from all sorts of data sources, such as Google’s BigQuery\n",
      "service.\n",
      "However, reading huge datasets efficiently is not the only difficulty: the data also\n",
      "needs to be preprocessed. Indeed, it is not always composed strictly of convenient\n",
      "numerical fields: sometimes there will be text features, categorical features, and so on.\n",
      "To handle this, TensorFlow provides the Features API: it lets you easily convert these\n",
      "features to numerical features that can be consumed by your neural network. For\n",
      "403\n",
      "example, categorical features with a large number of categories (such as cities, or\n",
      "words) can be encoded using embeddings (as we will see, an embedding is a trainable\n",
      "dense vector that represents a category).\n",
      "Both the Data API and the Features API work seamlessly with\n",
      "tf.keras.\n",
      "In this chapter, we will cover the Data API, the TFRecord format and the Features\n",
      "API in detail. We will also take a quick look at a few related projects from Tensor‐\n",
      "Flow’s ecosystem:\n",
      "• TF Transform ( tf.Transform) makes it possible to write a single preprocessing\n",
      "function that can be run both in batch mode on your full training set, before\n",
      "training (to speed it up), and then exported to a TF Function and incorporated\n",
      "into your trained model, so that once it is deployed in production, it can take\n",
      "care of preprocessing new instances on the fly.\n",
      "• TF Datasets (TFDS) provides a convenient function to download many common\n",
      "datasets of all kinds, including large ones like ImageNet, and it provides conve‐\n",
      "nient dataset objects to manipulate them using the Data API.\n",
      "So let’s get started!\n",
      "The Data API\n",
      "The whole Data API revolves around the concept of a dataset: as you might suspect,\n",
      "this represents a sequence of data items. Usually you will use datasets that gradually\n",
      "read data from disk, but for simplicity let’s just create a dataset entirely in RAM using\n",
      "tf.data.Dataset.from_tensor_slices():\n",
      ">>> X = tf.range(10)  # any data tensor\n",
      ">>> dataset = tf.data.Dataset.from_tensor_slices(X)\n",
      ">>> dataset\n",
      "<TensorSliceDataset shapes: (), types: tf.int32>\n",
      "The from_tensor_slices() function takes a tensor and creates a tf.data.Dataset\n",
      "whose elements are all the slices of X (along the first dimension), so this dataset con‐\n",
      "tains 10 items: tensors 0, 1, 2, …, 9. In this case we would have obtained the same\n",
      "dataset if we had used tf.data.Dataset.range(10).\n",
      "Y ou can simply iterate over a dataset’s items like this:\n",
      ">>> for item in dataset:\n",
      "...     print(item)\n",
      "404 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "...\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "[...]\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "Chaining Transformations\n",
      "Once you have a dataset, you can apply all sorts of transformations to it by calling its\n",
      "transformation methods. Each method returns a new dataset, so you can chain trans‐\n",
      "formations like this (this chain is illustrated in Figure 13-1):\n",
      ">>> dataset = dataset.repeat(3).batch(7)\n",
      ">>> for item in dataset:\n",
      "...     print(item)\n",
      "...\n",
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n",
      "Figure 13-1. Chaining Dataset Transformations\n",
      "In this example, we first call the repeat() method on the original dataset, and it\n",
      "returns a new dataset that will repeat the items of the original dataset 3 times. Of\n",
      "course, this will not copy the whole data in memory 3 times! In fact, if you call this\n",
      "method with no arguments, the new dataset will repeat the source dataset forever.\n",
      "Then we call the batch() method on this new dataset, and again this creates a new\n",
      "dataset. This one will group the items of the previous dataset in batches of 7 items.\n",
      "Finally, we iterate over the items of this final dataset. As you can see, the batch()\n",
      "method had to output a final batch of size 2 instead of 7, but you can call it with\n",
      "drop_remainder=True if you want it to drop this final batch so that all batches have\n",
      "the exact same size.\n",
      "The Data API | 405\n",
      "The dataset methods do not modify datasets, they create new ones,\n",
      "so make sure to keep a reference to these new datasets (e.g., data\n",
      "set = ...), or else nothing will happen.\n",
      "Y ou can also apply any transformation you want to the items by calling the map()\n",
      "method. For example, this creates a new dataset with all items doubled:\n",
      ">>> dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]\n",
      "This function is the one you will call to apply any preprocessing you want to your\n",
      "data. Sometimes, this will include computations that can be quite intensive, such as\n",
      "reshaping or rotating an image, so you will usually want to spawn multiple threads to\n",
      "speed things up: it’s as simple as setting the num_parallel_calls argument.\n",
      "While the map() applies a transformation to each item, the apply() method applies a\n",
      "transformation to the dataset as a whole. For example, the following code “unbatches”\n",
      "the dataset, by applying the unbatch() function to the dataset (this function is cur‐\n",
      "rently experimental, but it will most likely move to the core API in a future release).\n",
      "Each item in the new dataset will be a single integer tensor instead of a batch of 7\n",
      "integers:\n",
      ">>> dataset = dataset.apply(tf.data.experimental.unbatch()) # Items: 0,2,4,...\n",
      "It is also possible to simply filter the dataset using the filter() method:\n",
      ">>> dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\n",
      "Y ou will often want to look at just a few items from a dataset. Y ou can use the take()\n",
      "method for that:\n",
      ">>> for item in dataset.take(3):\n",
      "...     print(item)\n",
      "...\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "Shuffling the Data\n",
      "As you know, Gradient Descent works best when the instances in the training set are\n",
      "independent and identically distributed (see Chapter 4). A simple way to ensure this\n",
      "is to shuffle the instances. For this, you can just use the shuffle() method. It will\n",
      "create a new dataset that will start by filling up a buffer with the first items of the\n",
      "source dataset, then whenever it is asked for an item, it will pull one out randomly\n",
      "from the buffer, and replace it with a fresh one from the source dataset, until it has\n",
      "iterated entirely through the source dataset. At this point it continues to pull out\n",
      "items randomly from the buffer until it is empty. Y ou must specify the buffer size, and\n",
      "406 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "1 Imagine a sorted deck of cards on your left: suppose you just take the top 3 cards and shuffle them, then pick\n",
      "one randomly and put it to your right, keeping the other 2 in your hands. Take another card on your left,\n",
      "shuffle the 3 cards in your hands and pick one of them randomly, and put it on your right. When you are\n",
      "done going through all the cards like this, you will have a deck of cards on your right: do you think it will be\n",
      "perfectly shuffled?\n",
      "it is important to make it large enough or else shuffling will not be very efficient. 1\n",
      "However, obviously do not exceed the amount of RAM you have, and even if you\n",
      "have plenty of it, there’s no need to go well beyond the dataset’s size. Y ou can provide\n",
      "a random seed if you want the same random order every time you run your program.\n",
      ">>> dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\n",
      ">>> dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
      ">>> for item in dataset:\n",
      "...     print(item)\n",
      "...\n",
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n",
      "If you call repeat() on a shuffled dataset, by default it will generate\n",
      "a new order at every iteration. This is generally a good idea, but if\n",
      "you prefer to reuse the same order at each iteration (e.g., for tests\n",
      "or debugging), you can set reshuffle_each_iteration=False.\n",
      "For a large dataset that does not fit in memory, this simple shuffling-buffer approach\n",
      "may not be sufficient, since the buffer will be small compared to the dataset. One sol‐\n",
      "ution is to shuffle the source data itself (for example, on Linux you can shuffle text\n",
      "files using the shuf command). This will definitely improve shuffling a lot! However,\n",
      "even if the source data is shuffled, you will usually want to shuffle it some more, or\n",
      "else the same order will be repeated at each epoch, and the model may end up being\n",
      "biased (e.g., due to some spurious patterns present by chance in the source data’s\n",
      "order). To shuffle the instances some more, a common approach is to split the source\n",
      "data into multiple files, then read them in a random order during training. However,\n",
      "instances located in the same file will still end up close to each other. To avoid this\n",
      "you can pick multiple files randomly, and read them simultaneously, interleaving\n",
      "their lines. Then on top of that you can add a shuffling buffer using the shuffle()\n",
      "method. If all this sounds like a lot of work, don’t worry: the Data API actually makes\n",
      "all this possible in just a few lines of code. Let’s see how to do this.\n",
      "The Data API | 407\n",
      "Interleaving Lines From Multiple Files\n",
      "First, let’s suppose that you loaded the California housing dataset, you shuffled it\n",
      "(unless it was already shuffled), you split it into a training set, a validation set and a\n",
      "test set, then you split each set into many CSV files that each look like this (each row\n",
      "contains 8 input features plus the target median house value):\n",
      "MedInc,HouseAge,AveRooms,AveBedrms,Popul,AveOccup,Lat,Long,MedianHouseValue\n",
      "3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.4900,0.9910,3464.0,3.4433,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423,1.5915,1328.0,2.2508,38.44,-122.98,1.621\n",
      "[...]\n",
      "Let’s also suppose train_filepaths contains the list of file paths (and you also have\n",
      "valid_filepaths and test_filepaths):\n",
      ">>> train_filepaths\n",
      "['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv',...]\n",
      "Now let’s create a dataset containing only these file paths:\n",
      "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
      "By default, the list_files() function returns a dataset that shuffles the file paths. In\n",
      "general this is a good thing, but you can set shuffle=False if you do not want that,\n",
      "for some reason.\n",
      "Next, we can call the interleave() method to read from 5 files at a time and inter‐\n",
      "leave their lines (skipping the first line of each file, which is the header row, using the\n",
      "skip() method):\n",
      "n_readers = 5\n",
      "dataset = filepath_dataset.interleave(\n",
      "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
      "    cycle_length=n_readers)\n",
      "The interleave() method will create a dataset that will pull 5 file paths from the\n",
      "filepath_dataset, and for each one it will call the function we gave it (a lambda in\n",
      "this example) to create a new dataset, in this case a TextLineDataset. It will then\n",
      "cycle through these 5 datasets, reading one line at a time from each until all datasets\n",
      "are out of items. Then it will get the next 5 file paths from the filepath_dataset, and\n",
      "interleave them the same way, and so on until it runs out of file paths.\n",
      "For interleaving to work best, it is preferable to have files of identi‐\n",
      "cal length, or else the end of the longest files will not be interleaved.\n",
      "408 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "By default, interleave() does not use parallelism, it just reads one line at a time\n",
      "from each file, sequentially. However, if you want it to actually read files in parallel,\n",
      "you can set the num_parallel_calls argument to the number of threads you want.\n",
      "Y ou can even set it to tf.data.experimental.AUTOTUNE to make TensorFlow choose\n",
      "the right number of threads dynamically based on the available CPU (however, this is\n",
      "an experimental feature for now). Let’s look at what the dataset contains now:\n",
      ">>> for line in dataset.take(5):\n",
      "...     print(line.numpy())\n",
      "...\n",
      "b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'\n",
      "b'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'\n",
      "b'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'\n",
      "b'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'\n",
      "b'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442'\n",
      "These are the first rows (ignoring the header row) of 5 CSV files, chosen randomly.\n",
      "Looks good! But as you can see, these are just byte strings, we need to parse them,\n",
      "and also scale the data.\n",
      "Preprocessing the Data\n",
      "Let’s implement a small function that will perform this preprocessing:\n",
      "X_mean, X_std = [...] # mean and scale of each feature in the training set\n",
      "n_inputs = 8\n",
      "def preprocess(line):\n",
      "  defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
      "  fields = tf.io.decode_csv(line, record_defaults=defs)\n",
      "  x = tf.stack(fields[:-1])\n",
      "  y = tf.stack(fields[-1:])\n",
      "  return (x - X_mean) / X_std, y\n",
      "Let’s walk through this code:\n",
      "• First, we assume that you have precomputed the mean and standard deviation of\n",
      "each feature in the training set. X_mean and X_std are just 1D tensors (or NumPy\n",
      "arrays) containing 8 floats, one per input feature.\n",
      "• The preprocess() function takes one CSV line, and starts by parsing it. For this,\n",
      "it uses the tf.io.decode_csv() function, which takes two arguments: the first is\n",
      "the line to parse, and the second is an array containing the default value for each\n",
      "column in the CSV file. This tells TensorFlow not only the default value for each\n",
      "column, but also the number of columns and the type of each column. In this\n",
      "example, we tell it that all feature columns are floats and missing values should\n",
      "default to 0, but we provide an empty array of type tf.float32 as the default\n",
      "value for the last column (the target): this tells TensorFlow that this column con‐\n",
      "The Data API | 409\n",
      "tains floats, but that there is no default value, so it will raise an exception if it\n",
      "encounters a missing value.\n",
      "• The decode_csv() function returns a list of scalar tensors (one per column) but\n",
      "we need to return 1D tensor arrays. So we call tf.stack() on all tensors except\n",
      "for the last one (the target): this will stack these tensors into a 1D array. We then\n",
      "do the same for the target value (this makes it a 1D tensor array with a single\n",
      "value, rather than a scalar tensor).\n",
      "• Finally, we scale the input features by subtracting the feature means and then\n",
      "dividing by the feature standard deviations, and we return a tuple containing the\n",
      "scaled features and the target.\n",
      "Let’s test this preprocessing function:\n",
      ">>> preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')\n",
      "(<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy=\n",
      " array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\n",
      "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
      " <tf.Tensor: [...], numpy=array([2.782], dtype=float32)>)\n",
      "We can now apply this preprocessing function to the dataset.\n",
      "Putting Everything Together\n",
      "To make the code reusable, let’s put together everything we have discussed so far into\n",
      "a small helper function: it will create and return a dataset that will efficiently load Cal‐\n",
      "ifornia housing data from multiple CSV files, then shuffle it, preprocess it and batch it\n",
      "(see Figure 13-2):\n",
      "def csv_reader_dataset(filepaths, repeat=None, n_readers=5,\n",
      "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
      "                       n_parse_threads=5, batch_size=32):\n",
      "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
      "    dataset = dataset.interleave(\n",
      "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
      "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
      "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
      "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
      "    dataset = dataset.batch(batch_size)\n",
      "    return dataset.prefetch(1)\n",
      "410 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "2 In general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more. Alterna‐\n",
      "tively, you can let TensorFlow decide automatically by passing tf.data.experimental.AUTOTUNE (this is an\n",
      "experimental feature for now).\n",
      "Figure 13-2. Loading and Preprocessing Data From Multiple CSV Files\n",
      "Everything should make sense in this code, except the very last line ( prefetch(1)),\n",
      "which is actually quite important for performance.\n",
      "Prefetching\n",
      "By calling prefetch(1) at the end, we are creating a dataset that will do its best to\n",
      "always be one batch ahead 2. In other words, while our training algorithm is working\n",
      "on one batch, the dataset will already be working in parallel on getting the next batch\n",
      "ready. This can improve performance dramatically, as is illustrated on Figure 13-3. If\n",
      "we also ensure that loading and preprocessing are multithreaded (by setting num_par\n",
      "allel_calls when calling interleave() and map()), we can exploit multiple cores\n",
      "on the CPU and hopefully make preparing one batch of data shorter than running a\n",
      "training step on the GPU: this way the GPU will be almost 100% utilized (except for\n",
      "the data transfer time from the CPU to the GPU), and training will run much faster.\n",
      "The Data API | 411\n",
      "Figure 13-3. Speedup Training Thanks to Prefetching and Multithreading\n",
      "If you plan to purchase a GPU card, its processing power and its\n",
      "memory size are of course very important (in particular, a large\n",
      "RAM is crucial for computer vision), but its memory bandwidth is\n",
      "just as important as the processing power to get good performance:\n",
      "this is the number of gigabytes of data it can get in or out of its\n",
      "RAM per second.\n",
      "With that, you can now build efficient input pipelines to load and preprocess data\n",
      "from multiple text files. We have discussed the most common dataset methods, but\n",
      "there are a few more you may want to look at: concatenate(), zip(), window(),\n",
      "reduce(), cache(), shard(), flat_map() and padded_batch(). There are also a cou‐\n",
      "ple more class methods: from_generator() and from_tensors(), which create a new\n",
      "dataset from a Python generator or a list of tensors respectively. Please check the API\n",
      "documentation for more details. Also note that there are experimental features avail‐\n",
      "able in tf.data.experimental, many of which will most likely make it to the core\n",
      "API in future releases (e.g., check out the CsvDataset class and the SqlDataset\n",
      "classes).\n",
      "412 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "3 Support for datasets is specific to tf.keras, it will not work on other implementations of the Keras API.\n",
      "4 The number of steps per epoch is optional if the dataset just goes through the data once, but if you do not\n",
      "specify it, the progress bar will not be displayed during the first epoch.\n",
      "5 Note that for now the dataset must be created within the TF Function. This may be fixed by the time you read\n",
      "these lines (see TensorFlow issue #25414).\n",
      "Using the Dataset With tf.keras\n",
      "Now we can use the csv_reader_dataset() function to create a dataset for the train‐\n",
      "ing set (ensuring it repeats the data forever), the validation set and the test set:\n",
      "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
      "valid_set = csv_reader_dataset(valid_filepaths)\n",
      "test_set = csv_reader_dataset(test_filepaths)\n",
      "And now we can simply build and train a Keras model using these datasets. 3 All we\n",
      "need to do is to call the fit() method with the datasets instead of X_train and\n",
      "y_train, and specify the number of steps per epoch for each set:4\n",
      "model = keras.models.Sequential([...])\n",
      "model.compile([...])\n",
      "model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\n",
      "          validation_data=valid_set,\n",
      "          validation_steps=len(X_valid) // batch_size)\n",
      "Similarly, we can pass a dataset to the evaluate() and predict() methods (and again\n",
      "specify the number of steps per epoch):\n",
      "model.evaluate(test_set, steps=len(X_test) // batch_size)\n",
      "model.predict(new_set, steps=len(X_new) // batch_size)\n",
      "Unlike the other sets, the new_set will usually not contain labels (if it does, Keras will\n",
      "just ignore them). Note that in all these cases, you can still use NumPy arrays instead\n",
      "of datasets if you want (but of course they need to have been loaded and preprocessed\n",
      "first).\n",
      "If you want to build your own custom training loop (as in Chapter 12), you can just\n",
      "iterate over the training set, very naturally:\n",
      "for X_batch, y_batch in train_set:\n",
      "    [...] # perform one gradient descent step\n",
      "In fact, it is even possible to create a tf.function (see Chapter 12) that performs the\n",
      "whole training loop!5\n",
      "@tf.function\n",
      "def train(model, optimizer, loss_fn, n_epochs, [...]):\n",
      "    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, [...])\n",
      "    for X_batch, y_batch in train_set:\n",
      "        with tf.GradientTape() as tape:\n",
      "The Data API | 413\n",
      "y_pred = model(X_batch)\n",
      "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
      "            loss = tf.add_n([main_loss] + model.losses)\n",
      "        grads = tape.gradient(loss, model.trainable_variables)\n",
      "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
      "Congratulations, you now know how to build powerful input pipelines using the Data\n",
      "API! However, so far we have used CSV files, which are common, simple and conve‐\n",
      "nient, but they are not really efficient, and they do not support large or complex data\n",
      "structures very well, such as images or audio. So let’s use TFRecords instead.\n",
      "If you are happy with CSV files (or whatever other format you are\n",
      "using), you do not have to use TFRecords. As the saying goes, if it\n",
      "ain’t broke, don’t fix it! TFRecords are useful when the bottleneck\n",
      "during training is loading and parsing the data.\n",
      "The TFRecord Format\n",
      "The TFRecord format is TensorFlow’s preferred format for storing large amounts of\n",
      "data and reading it efficiently. It is a very simple binary format that just contains a\n",
      "sequence of binary records of varying sizes (each record just has a length, a CRC\n",
      "checksum to check that the length was not corrupted, then the actual data, and finally\n",
      "a CRC checksum for the data). Y ou can easily create a TFRecord file using the\n",
      "tf.io.TFRecordWriter class:\n",
      "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
      "    f.write(b\"This is the first record\")\n",
      "    f.write(b\"And this is the second record\")\n",
      "And you can then use a tf.data.TFRecordDataset to read one or more TFRecord\n",
      "files:\n",
      "filepaths = [\"my_data.tfrecord\"]\n",
      "dataset = tf.data.TFRecordDataset(filepaths)\n",
      "for item in dataset:\n",
      "    print(item)\n",
      "This will output:\n",
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n",
      "By default, a TFRecordDataset will read files one by one, but you\n",
      "can make it read multiple files in parallel and interleave their\n",
      "records by setting num_parallel_reads. Alternatively, you could\n",
      "obtain the same result by using list_files() and interleave()\n",
      "as we did earlier to read multiple CSV files.\n",
      "414 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "6 Since protobuf objects are meant to be serialized and transmitted, they are called messages.\n",
      "Compressed TFRecord Files\n",
      "It can sometimes be useful to compress your TFRecord files, especially if they need to\n",
      "be loaded via a network connection. Y ou can create a compressed TFRecord file by\n",
      "setting the options argument:\n",
      "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
      "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
      "  [...]\n",
      "When reading a compressed TFRecord file, you need to specify the compression type:\n",
      "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
      "                                  compression_type=\"GZIP\")\n",
      "A Brief Introduction to Protocol Buffers\n",
      "Even though each record can use any binary format you want, TFRecord files usually\n",
      "contain serialized Protocol Buffers (also called protobufs). This is a portable, extensi‐\n",
      "ble and efficient binary format developed at Google back in 2001 and Open Sourced\n",
      "in 2008, and they are now widely used, in particular in gRPC, Google’s remote proce‐\n",
      "dure call system. Protocol Buffers are defined using a simple language that looks like\n",
      "this:\n",
      "syntax = \"proto3\";\n",
      "message Person {\n",
      "  string name = 1;\n",
      "  int32 id = 2;\n",
      "  repeated string email = 3;\n",
      "}\n",
      "This definition says we are using the protobuf format version 3, and it specifies that\n",
      "each Person object6 may (optionally) have a name of type string, an id of type int32,\n",
      "and zero or more email fields, each of type string. The numbers 1, 2 and 3 are the\n",
      "field identifiers: they will be used in each record’s binary representation. Once you\n",
      "have a definition in a .proto file, you can compile it. This requires protoc, the proto‐\n",
      "buf compiler, to generate access classes in Python (or some other language). Note that\n",
      "the protobuf definitions we will use have already been compiled for you, and their\n",
      "Python classes are part of TensorFlow, so you will not need to use protoc. All you\n",
      "need to know is how to use protobuf access classes in Python. To illustrate the basics,\n",
      "let’s look at a simple example that uses the access classes generated for the Person\n",
      "protobuf (the code is explained in the comments):\n",
      ">>> from person_pb2 import Person  # import the generated access class\n",
      ">>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\"])  # create a Person\n",
      ">>> print(person)  # display the Person\n",
      "The TFRecord Format | 415\n",
      "7 This chapter contains the bare minimum you need to know about protobufs to use TFRecords. To learn more\n",
      "about protobufs, please visit https://homl.info/protobuf.\n",
      "name: \"Al\"\n",
      "id: 123\n",
      "email: \"a@b.com\"\n",
      ">>> person.name  # read a field\n",
      "\"Al\"\n",
      ">>> person.name = \"Alice\"  # modify a field\n",
      ">>> person.email[0]  # repeated fields can be accessed like arrays\n",
      "\"a@b.com\"\n",
      ">>> person.email.append(\"c@d.com\")  # add an email address\n",
      ">>> s = person.SerializeToString()  # serialize the object to a byte string\n",
      ">>> s\n",
      "b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'\n",
      ">>> person2 = Person()  # create a new Person\n",
      ">>> person2.ParseFromString(s)  # parse the byte string (27 bytes long)\n",
      "27\n",
      ">>> person == person2  # now they are equal\n",
      "True\n",
      "In short, we import the Person class generated by protoc, we create an instance and\n",
      "we play with it, visualizing it, reading and writing some fields, then we serialize it\n",
      "using the SerializeToString() method. This is the binary data that is ready to be\n",
      "saved or transmitted over the network. When reading or receiving this binary data,\n",
      "we can parse it using the ParseFromString() method, and we get a copy of the object\n",
      "that was serialized.7\n",
      "We could save the serialized Person object to a TFRecord file, then we could load and\n",
      "parse it: everything would work fine. However, SerializeToString() and ParseFrom\n",
      "String() are not TensorFlow operations (and neither are the other operations in this\n",
      "code), so they cannot be included in a TensorFlow Function (except by wrapping\n",
      "them in a tf.py_function() operation, which would make the code slower and less\n",
      "portable, as we saw in Chapter 12). Fortunately, TensorFlow does include special pro‐\n",
      "tobuf definitions for which it provides parsing operations.\n",
      "TensorFlow Protobufs\n",
      "The main protobuf typically used in a TFRecord file is the Example protobuf, which\n",
      "represents one instance in a dataset. It contains a list of named features, where each\n",
      "feature can either be a list of byte strings, a list of floats or a list of integers. Here is the\n",
      "protobuf definition:\n",
      "syntax = \"proto3\";\n",
      "message BytesList { repeated bytes value = 1; }\n",
      "message FloatList { repeated float value = 1 [packed = true]; }\n",
      "message Int64List { repeated int64 value = 1 [packed = true]; }\n",
      "416 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "8 Why was Example even defined since it contains no more than a Features object? Well, TensorFlow may one\n",
      "day decide to add more fields to it. As long as the new Example definition still contains the features field,\n",
      "with the same id, it will be backward compatible. This extensibility is one of the great features of protobufs.\n",
      "message Feature {\n",
      "    oneof kind {\n",
      "        BytesList bytes_list = 1;\n",
      "        FloatList float_list = 2;\n",
      "        Int64List int64_list = 3;\n",
      "    }\n",
      "};\n",
      "message Features { map<string, Feature> feature = 1; };\n",
      "message Example { Features features = 1; };\n",
      "The definitions of BytesList, FloatList and Int64List are straightforward enough\n",
      "([packed = true] is used for repeated numerical fields, for a more efficient encod‐\n",
      "ing). A Feature either contains a BytesList, a FloatList or an Int64List. A Fea\n",
      "tures (with an s) contains a dictionary that maps a feature name to the\n",
      "corresponding feature value. And finally, an Example just contains a Features object.8\n",
      "Here is how you could create a tf.train.Example representing the same person as\n",
      "earlier, and write it to TFRecord file:\n",
      "from tensorflow.train import BytesList, FloatList, Int64List\n",
      "from tensorflow.train import Feature, Features, Example\n",
      "person_example = Example(\n",
      "    features=Features(\n",
      "        feature={\n",
      "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
      "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
      "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\",\n",
      "                                                          b\"c@d.com\"]))\n",
      "        }))\n",
      "The code is a bit verbose and repetitive, but it’s rather straightforward (and you could\n",
      "easily wrap it inside a small helper function). Now that we have an Example protobuf,\n",
      "we can serialize it by calling its SerializeToString() method, then write the result‐\n",
      "ing data to a TFRecord file:\n",
      "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
      "    f.write(person_example.SerializeToString())\n",
      "Normally you would write much more than just one example! Typically, you would\n",
      "create a conversion script that reads from your current format (say, CSV files), creates\n",
      "an Example protobuf for each instance, serializes them and saves them to several\n",
      "TFRecord files, ideally shuffling them in the process. This requires a bit of work, so\n",
      "once again make sure it is really necessary (perhaps your pipeline works fine with\n",
      "CSV files).\n",
      "The TFRecord Format | 417\n",
      "Now that we have a nice TFRecord file containing a serialized Example, let’s try to\n",
      "load it.\n",
      "Loading and Parsing Examples\n",
      "To load the serialized Example protobufs, we will use a tf.data.TFRecordDataset\n",
      "once again, and we will parse each Example using tf.io.parse_single_example().\n",
      "This is a TensorFlow operation so it can be included in a TF Function. It requires at\n",
      "least two arguments: a string scalar tensor containing the serialized data, and a\n",
      "description of each feature. The description is a dictionary that maps each feature\n",
      "name to either a tf.io.FixedLenFeature descriptor indicating the feature’s shape,\n",
      "type and default value, or a tf.io.VarLenFeature descriptor indicating only the type\n",
      "(if the length may vary, such as for the \"emails\" feature). For example:\n",
      "feature_description = {\n",
      "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
      "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
      "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
      "}\n",
      "for serialized_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\n",
      "    parsed_example = tf.io.parse_single_example(serialized_example,\n",
      "                                                feature_description)\n",
      "The fixed length features are parsed as regular tensors, but the variable length fea‐\n",
      "tures are parsed as sparse tensors. Y ou can convert a sparse tensor to a dense tensor\n",
      "using tf.sparse.to_dense(), but in this case it is simpler to just access its values:\n",
      ">>> tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")\n",
      "<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>\n",
      ">>> parsed_example[\"emails\"].values\n",
      "<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>\n",
      "A BytesList can contain any binary data you want, including any serialized object.\n",
      "For example, you can use tf.io.encode_jpeg() to encode an image using the JPEG\n",
      "format, and put this binary data in a BytesList. Later, when your code reads the\n",
      "TFRecord, it will start by parsing the Example, then you will need to call\n",
      "tf.io.decode_jpeg() to parse the data and get the original image (or you can use\n",
      "tf.io.decode_image(), which can decode any BMP , GIF , JPEG or PNG image). Y ou\n",
      "can also store any tensor you want in a BytesList by serializing the tensor using\n",
      "tf.io.serialize_tensor(), then putting the resulting byte string in a BytesList\n",
      "feature. Later, when you parse the TFRecord, you can parse this data using\n",
      "tf.io.parse_tensor().\n",
      "Instead of parsing examples one by one using tf.io.parse_single_example(), you\n",
      "may want to parse them batch by batch using tf.io.parse_example():\n",
      "418 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(10)\n",
      "for serialized_examples in dataset:\n",
      "    parsed_examples = tf.io.parse_example(serialized_examples,\n",
      "                                          feature_description)\n",
      "As you can see, the Example proto will probably be sufficient for most use cases.\n",
      "However, it may be a bit cumbersome to use when you are dealing with lists of lists.\n",
      "For example, suppose you want to classify text documents. Each document may be\n",
      "represented as a list of sentences, where each sentence is represented as a list of\n",
      "words. And perhaps each document also has a list of comments, where each com‐\n",
      "ment is also represented as a list of words. Moreover, there may be some contextual\n",
      "data as well, such as the document’s author, title and publication date. TensorFlow’s\n",
      "SequenceExample protobuf is designed for such use cases.\n",
      "Handling Lists of Lists Using the SequenceExample Protobuf\n",
      "Here is the definition of the SequenceExample protobuf:\n",
      "message FeatureList { repeated Feature feature = 1; };\n",
      "message FeatureLists { map<string, FeatureList> feature_list = 1; };\n",
      "message SequenceExample {\n",
      "    Features context = 1;\n",
      "    FeatureLists feature_lists = 2;\n",
      "};\n",
      "A SequenceExample contains a Features object for the contextual data and a Fea\n",
      "tureLists object which contains one or more named FeatureList objects (e.g., a\n",
      "FeatureList named \"content\" and another named \"comments\"). Each FeatureList\n",
      "just contains a list of Feature objects, each of which may be a list of byte strings, a list\n",
      "of 64-bit integers or a list of floats (in this example, each Feature would represent a\n",
      "sentence or a comment, perhaps in the form of a list of word identifiers). Building a\n",
      "SequenceExample, serializing it and parsing it is very similar to building, serializing\n",
      "and parsing an Example, but you must use tf.io.parse_single_sequence_exam\n",
      "ple() to parse a single SequenceExample or tf.io.parse_sequence_example() to\n",
      "parse a batch, and both functions return a tuple containing the context features (as a\n",
      "dictionary) and the feature lists (also as a dictionary). If the feature lists contain\n",
      "sequences of varying sizes (as in the example above), you may want to convert them\n",
      "to ragged tensors using tf.RaggedTensor.from_sparse() (see the notebook for the\n",
      "full code):\n",
      "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
      "    serialized_sequence_example, context_feature_descriptions,\n",
      "    sequence_feature_descriptions)\n",
      "parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])\n",
      "Now that you know how to efficiently store, load and parse data, the next step is to\n",
      "prepare it so that it can be fed to a neural network. This means converting all features\n",
      "The TFRecord Format | 419\n",
      "into numerical features (ideally not too sparse), scaling them, and more. In particular,\n",
      "if your data contains categorical features or text features, they need to be converted to\n",
      "numbers. For this, the Features API can help.\n",
      "The Features API\n",
      "Preprocessing your data can be performed in many ways: it can be done ahead of\n",
      "time when preparing your data files, using any tool you like. Or you can preprocess\n",
      "your data on the fly when loading it with the Data API (e.g., using the dataset’s map()\n",
      "method, as we saw earlier). Or you can include a preprocessing layer directly in your\n",
      "model. Whichever solution you prefer, the Features API can help you: it is a set of\n",
      "functions available in the tf.feature_column package, which let you define how\n",
      "each feature (or group of features) in your data should be preprocessed (therefore you\n",
      "can think of this API as the analog of Scikit-Learn’s ColumnTransformer class). We\n",
      "will start by looking at the different types of columns available, and then we will look\n",
      "at how to use them.\n",
      "Let’s go back to the variant of the California housing dataset that we used in Chap‐\n",
      "ter 2, since it includes a categorical feature and missing data. Here is a simple numeri‐\n",
      "cal column named \"housing_median_age\":\n",
      "housing_median_age = tf.feature_column.numeric_column(\"housing_median_age\")\n",
      "Numeric columns let you specify a normalization function using the normalizer_fn\n",
      "argument. For example, let’s tweak the \"housing_median_age\" column to define how\n",
      "it should be scaled. Note that this requires computing ahead of time the mean and\n",
      "standard deviation of this feature in the training set:\n",
      "age_mean, age_std = X_mean[1], X_std[1]  # The median age is column in 1\n",
      "housing_median_age = tf.feature_column.numeric_column(\n",
      "    \"housing_median_age\", normalizer_fn=lambda x: (x - age_mean) / age_std)\n",
      "In some cases, it might improve performance to bucketize some numerical features,\n",
      "effectively transforming a numerical feature into a categorical feature. For example,\n",
      "let’s create a bucketized column based on the median_income column, with 5 buckets:\n",
      "less than 1.5 ($15,000), then 1.5 to 3, 3 to 4.5, 4.5 to 6., and above 6. (notice that when\n",
      "you specify 4 boundaries, there are actually 5 buckets):\n",
      "median_income = tf.feature_column.numeric_column(\"median_income\")\n",
      "bucketized_income = tf.feature_column.bucketized_column(\n",
      "    median_income, boundaries=[1.5, 3., 4.5, 6.])\n",
      "If the median_income feature is equal to, say, 3.2, then the bucketized_income feature\n",
      "will automatically be equal to 2 (i.e., the index of the corresponding income bucket).\n",
      "Choosing the right boundaries can be somewhat of an art, but one approach is to just\n",
      "use percentiles of the data (e.g., the 10th percentile, the 20th percentile, and so on). If\n",
      "a feature is multimodal, meaning it has separate peaks in its distribution, you may\n",
      "420 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "want to define a bucket for each mode, placing the boundaries in between the peaks.\n",
      "Whether you use the percentiles or the modes, you need to analyze the distribution of\n",
      "your data ahead of time, just like we had to measure the mean and standard deviation\n",
      "ahead of time to normalize the housing_median_age column.\n",
      "Categorical Features\n",
      "For categorical features such as ocean_proximity, there are several options. If it is\n",
      "already represented as a category ID (i.e., an integer from 0 to the max ID), then you\n",
      "can use the categorical_column_with_identity() function (specifying the max\n",
      "ID). If not, and you know the list of all possible categories, then you can use categori\n",
      "cal_column_with_vocabulary_list():\n",
      "ocean_prox_vocab = ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']\n",
      "ocean_proximity = tf.feature_column.categorical_column_with_vocabulary_list(\n",
      "    \"ocean_proximity\", ocean_prox_vocab)\n",
      "If you prefer to have TensorFlow load the vocabulary from a file, you can call catego\n",
      "rical_column_with_vocabulary_file() instead. As you might expect, these two\n",
      "functions will simply map each category to its index in the vocabulary (e.g., NEAR\n",
      "BAY will be mapped to 3), and unknown categories will be mapped to -1.\n",
      "For categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\n",
      "products, users, etc.), it may not be convenient to get the full list of possible cate‐\n",
      "gories, or perhaps categories may be added or removed so frequently that using cate‐\n",
      "gory indices would be too unreliable. In this case, you may prefer to use a\n",
      "categorical_column_with_hash_bucket(). If we had a \"city\" feature in the dataset,\n",
      "we could encode it like this:\n",
      "city_hash = tf.feature_column.categorical_column_with_hash_bucket(\n",
      "    \"city\", hash_bucket_size=1000)\n",
      "This feature will compute a hash for each category (i.e., for each city), modulo the\n",
      "number of hash buckets ( hash_bucket_size). Y ou must set the number of buckets\n",
      "high enough to avoid getting too many collisions (i.e., different categories ending up\n",
      "in the same bucket), but the higher you set it, the more RAM will be used (by the\n",
      "embedding table, as we will see shortly).\n",
      "Crossed Categorical Features\n",
      "If you suspect that two (or more) categorical features are more meaningful when used\n",
      "jointly, then you can create a crossed column. For example, suppose people are partic‐\n",
      "ularly fond of old houses inland and new houses near the ocean, then it might help to\n",
      "The Features API | 421\n",
      "9 Since the housing_median_age feature was normalized, the boundaries are for normalized ages.\n",
      "create a bucketized column for the housing_median_age feature9, and cross it with\n",
      "the ocean_proximity column. The crossed column will compute a hash of every age\n",
      "& ocean proximity combination it comes across, modulo the hash_bucket_size, and\n",
      "this will give it the cross category ID. Y ou may then choose to use only this crossed\n",
      "column in your model, or also include the individual columns.\n",
      "bucketized_age = tf.feature_column.bucketized_column(\n",
      "    housing_median_age, boundaries=[-1., -0.5, 0., 0.5, 1.]) # age was scaled\n",
      "age_and_ocean_proximity = tf.feature_column.crossed_column(\n",
      "    [bucketized_age, ocean_proximity], hash_bucket_size=100)\n",
      "Another common use case for crossed columns is to cross latitude and longitude into\n",
      "a single categorical feature: you start by bucketizing the latitude and longitude, for\n",
      "example into 20 buckets each, then you cross these bucketized features into a loca\n",
      "tion column. This will create a 20×20 grid over California, and each cell in the grid\n",
      "will correspond to one category:\n",
      "latitude = tf.feature_column.numeric_column(\"latitude\")\n",
      "longitude = tf.feature_column.numeric_column(\"longitude\")\n",
      "bucketized_latitude = tf.feature_column.bucketized_column(\n",
      "    latitude, boundaries=list(np.linspace(32., 42., 20 - 1)))\n",
      "bucketized_longitude = tf.feature_column.bucketized_column(\n",
      "    longitude, boundaries=list(np.linspace(-125., -114., 20 - 1)))\n",
      "location = tf.feature_column.crossed_column(\n",
      "    [bucketized_latitude, bucketized_longitude], hash_bucket_size=1000)\n",
      "Encoding Categorical Features Using One-Hot Vectors\n",
      "No matter which option you choose to build a categorical feature (categorical col‐\n",
      "umns, bucketized columns or crossed columns), it must be encoded before you can\n",
      "feed it to a neural network. There are two options to encode a categorical feature:\n",
      "one-hot vectors or embeddings. For the first option, simply use the indicator_col\n",
      "umn() function:\n",
      "ocean_proximity_one_hot = tf.feature_column.indicator_column(ocean_proximity)\n",
      "A one-hot vector encoding has the size of the vocabulary length, which is fine if there\n",
      "are just a few possible categories, but if the vocabulary is large, you will end up with\n",
      "too many inputs fed to your neural network: it will have too many weights to learn\n",
      "and it will probably not perform very well. In particular, this will typically be the case\n",
      "when you use hash buckets. In this case, you should probably encode them using\n",
      "embeddings instead.\n",
      "422 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "As a rule of thumb (but your mileage may vary!), if the number of\n",
      "categories is lower than 10, then one-hot encoding is generally the\n",
      "way to go. If the number of categories is greater than 50 (which is\n",
      "often the case when you use hash buckets), then embeddings are\n",
      "usually preferable. In between 10 and 50 categories, you may want\n",
      "to experiment with both options and see which one works best for\n",
      "your use case. Also, embeddings typically require more training\n",
      "data, unless you can reuse pretrained embeddings.\n",
      "Encoding Categorical Features Using Embeddings\n",
      "An embedding is a trainable dense vector that represents a category. By default,\n",
      "embeddings are initialized randomly, so for example the \"NEAR BAY\" category could\n",
      "be represented initially by a random vector such as [0.131, 0.890], while the \"NEAR\n",
      "OCEAN\" category may be represented by another random vector such as [0.631,\n",
      "0.791] (in this example, we are using 2D embeddings, but the number of dimensions\n",
      "is a hyperparameter you can tweak). Since these embeddings are trainable, they will\n",
      "gradually improve during training, and as they represent fairly similar categories,\n",
      "Gradient Descent will certainly end up pushing them closer together, while it will\n",
      "tend to move them away from the \"INLAND\" category’s embedding (see Figure 13-4).\n",
      "Indeed, the better the representation, the easier it will be for the neural network to\n",
      "make accurate predictions, so training tends to make embeddings useful representa‐\n",
      "tions of the categories. This is called representation learning (we will see other types of\n",
      "representation learning in ???).\n",
      "The Features API | 423\n",
      "10 “Distributed Representations of Words and Phrases and their Compositionality” , T. Mikolov et al. (2013).\n",
      "Figure 13-4. Embeddings Will Gradually Improve During Training\n",
      "Word Embeddings\n",
      "Not only will embeddings generally be useful representations for the task at hand, but\n",
      "quite often these same embeddings can be reused successfully for other tasks as well.\n",
      "The most common example of this is word embeddings (i.e., embeddings of individual\n",
      "words): when you are working on a natural language processing task, you are often\n",
      "better off reusing pretrained word embeddings than training your own. The idea of\n",
      "using vectors to represent words dates back to the 1960s, and many sophisticated\n",
      "techniques have been used to generate useful vectors, including using neural net‐\n",
      "works, but things really took off in 2013, when Tomáš Mikolov and other Google\n",
      "researchers published a paper10 describing how to learn word embeddings using deep\n",
      "neural networks, much faster than previous attempts. This allowed them to learn\n",
      "embeddings on a very large corpus of text: they trained a deep neural network to pre‐\n",
      "dict the words near any given word. This allowed them to obtain astounding word\n",
      "embeddings. For example, synonyms had very close embeddings, and semantically\n",
      "related words such as France, Spain, Italy, and so on, ended up clustered together. But\n",
      "it’s not just about proximity: word embeddings were also organized along meaningful\n",
      "axes in the embedding space. Here is a famous example: if you compute King – Man\n",
      "+ Woman (adding and subtracting the embedding vectors of these words), then the\n",
      "result will be very close to the embedding of the word Queen (see Figure 13-5 ). In\n",
      "other words, the word embeddings encode the concept of gender! Similarly, you can\n",
      "compute Madrid – Spain + France, and of course the result is close to Paris, which\n",
      "seems to show that the notion of capital city was also encoded in the embeddings.\n",
      "424 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "Figure 13-5. Word Embeddings\n",
      "Let’s go back to the Features API. Here is how you could encode the ocean_proxim\n",
      "ity categories as 2D embeddings:\n",
      "ocean_proximity_embed = tf.feature_column.embedding_column(ocean_proximity,\n",
      "                                                           dimension=2)\n",
      "Each of the five ocean_proximity categories will now be represented as a 2D vector.\n",
      "These vectors are stored in an embedding matrix with one row per category, and one\n",
      "column per embedding dimension, so in this example it is a 5×2 matrix. When an\n",
      "embedding column is given a category index as input (say, 3, which corresponds to\n",
      "the category \"NEAR BAY\"), it just performs a lookup in the embedding matrix and\n",
      "returns the corresponding row (say, [0.331, 0.190]). Unfortunately, the embedding\n",
      "matrix can be quite large, especially when you have a large vocabulary: if this is the\n",
      "case, the model can only learn good representations for the categories for which it has\n",
      "sufficient training data. To reduce the size of the embedding matrix, you can of\n",
      "course try lowering the dimension hyperparameter, but if you reduce this parameter\n",
      "too much, the representations may not be as good. Another option is to reduce the\n",
      "vocabulary size (e.g., if you are dealing with text, you can try dropping the rare words\n",
      "from the vocabulary, and replace them all with a token like \"<unknown>\" or \"<UNK>\").\n",
      "If you are using hash buckets, you can also try reducing the hash_bucket_size (but\n",
      "not too much, or else you will get collisions).\n",
      "The Features API | 425\n",
      "If there are no pretrained embeddings that you can reuse for the\n",
      "task you are trying to tackle, and if you do not have enough train‐\n",
      "ing data to learn them, then you can try to learn them on some\n",
      "auxiliary task for which it is easier to obtain plenty of training data.\n",
      "After that, you can reuse the trained embeddings for your main\n",
      "task.\n",
      "Using Feature Columns for Parsing\n",
      "Let’s suppose you have created feature columns for each of your input features, as well\n",
      "as for the target. What can you do with them? Well, for one you can pass them to the\n",
      "make_parse_example_spec() function to generate feature descriptions (so you don’t\n",
      "have to do it manually, as we did earlier):\n",
      "columns = [bucketized_age, ....., median_house_value] # all features + target\n",
      "feature_descriptions = tf.feature_column.make_parse_example_spec(columns)\n",
      "Y ou don’t always have to create a separate feature column for each\n",
      "and every feature. For example, instead of having 2 numerical fea‐\n",
      "ture columns, you could choose to have a single 2D column: just\n",
      "set shape=[2] when calling numerical_column().\n",
      "Y ou can then create a function that parses serialized examples using these feature\n",
      "descriptions, and separates the target column from the input features:\n",
      "def parse_examples(serialized_examples):\n",
      "    examples = tf.io.parse_example(serialized_examples, feature_descriptions)\n",
      "    targets = examples.pop(\"median_house_value\") # separate the targets\n",
      "    return examples, targets\n",
      "Next, you can create a TFRecordDataset that will read batches of serialized examples\n",
      "(assuming the TFRecord file contains serialized Example protobufs with the appropri‐\n",
      "ate features):\n",
      "batch_size = 32\n",
      "dataset = tf.data.TFRecordDataset([\"my_data_with_features.tfrecords\"])\n",
      "dataset = dataset.repeat().shuffle(10000).batch(batch_size).map(parse_examples)\n",
      "Using Feature Columns in Your Models\n",
      "Feature columns can also be used directly in your model, to convert all your input\n",
      "features into a single dense vector which the neural network can then process. For\n",
      "this, all you need to do is add a keras.layers.DenseFeatures layer as the first layer\n",
      "in your model, passing it the list of feature columns (excluding the target column):\n",
      "columns_without_target = columns[:-1]\n",
      "model = keras.models.Sequential([\n",
      "    keras.layers.DenseFeatures(feature_columns=columns_without_target),\n",
      "426 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "keras.layers.Dense(1)\n",
      "])\n",
      "model.compile(loss=\"mse\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
      "steps_per_epoch = len(X_train) // batch_size\n",
      "history = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=5)\n",
      "The DenseFeatures layer will take care of converting every input feature to a dense\n",
      "representation, and it will also apply any extra transformation we specified, such as\n",
      "scaling the housing_median_age using the normalizer_fn function we provided. Y ou\n",
      "can take a closer look at what the DenseFeatures layer does by calling it directly:\n",
      ">>> some_columns = [ocean_proximity_embed, bucketized_income]\n",
      ">>> dense_features = keras.layers.DenseFeatures(some_columns)\n",
      ">>> dense_features({\n",
      "...     \"ocean_proximity\": [[\"NEAR OCEAN\"], [\"INLAND\"], [\"INLAND\"]],\n",
      "...     \"median_income\": [[3.], [7.2], [1.]]\n",
      "... })\n",
      "...\n",
      "<tf.Tensor: id=559790, shape=(3, 7), dtype=float32, numpy=\n",
      "array([[ 0. , 0. , 1. , 0. , 0. ,-0.36277947 , 0.30109018],\n",
      "       [ 0. , 0. , 0. , 0. , 1. , 0.22548223 , 0.33142096],\n",
      "       [ 1. , 0. , 0. , 0. , 0. , 0.22548223 , 0.33142096]], dtype=float32)>\n",
      "In this example, we create a DenseFeatures layer with just two columns, and we call\n",
      "it with some data, in the form of a dictionary of features. In this case, since the bucke\n",
      "tized_income column relies on the median_income column, the dictionary must\n",
      "include the \"median_income\" key, and similarly since the ocean_proximity_embed\n",
      "column is based on the ocean_proximity column, the dictionary must include the\n",
      "\"ocean_proximity\" key. Columns are handled in alphabetical order, so first we look\n",
      "at the bucketized income column (its name is the same as the median_income column\n",
      "name, plus \"_bucketized\"). The incomes 3, 7.2 and 1 get mapped respectively to cat‐\n",
      "egory 2 (for incomes between 1.5 and 3), category 0 (for incomes below 1.5), and cat‐\n",
      "egory 4 (for incomes greater than 6). Then these category IDs get one-hot encoded:\n",
      "category 2 gets encoded as [0., 0., 1., 0., 0.]  and so on (note that bucketized\n",
      "columns get one-hot encoded by default, no need to call indicator_column()). Now\n",
      "on to the ocean_proximity_embed column. The \"NEAR OCEAN\" and \"INLAND\" cate‐\n",
      "gories just get mapped to their respective embeddings (which were initialized ran‐\n",
      "domly). The resulting tensor is the concatenation of the one-hot vectors and the\n",
      "embeddings.\n",
      "Now you can feed all kinds of features to a neural network, including numerical fea‐\n",
      "tures, categorical features, and even text (by splitting the text into words, then using\n",
      "word embedding)! However, performing all the preprocessing on the fly can slow\n",
      "down training. Let’s see how this can be improved.\n",
      "The Features API | 427\n",
      "TF Transform\n",
      "If preprocessing is computationally expensive, then handling it before training rather\n",
      "than on the fly may give you a significant speedup: the data will be preprocessed just\n",
      "once per instance before training, rather than once per instance and per epoch during\n",
      "training. Tools like Apache Beam let you run efficient data processing pipelines over\n",
      "large amounts of data, even distributed across multiple servers, so why not use it to\n",
      "preprocess all the training data? This works great and indeed can speed up training,\n",
      "but there is one problem: once your model is trained, suppose you want to deploy it\n",
      "to a mobile app: you will need to write some code in your app to take care of prepro‐\n",
      "cessing the data before it is fed to the model. And suppose you also want to deploy\n",
      "the model to TensorFlow.js so it runs in a web browser? Once again, you will need to\n",
      "write some preprocessing code. This can become a maintenance nightmare: when‐\n",
      "ever you want to change the preprocessing logic, you will need to update your Apache\n",
      "Beam code, your mobile app code and your Javascript code. It is not only time con‐\n",
      "suming, but also error prone: you may end up with subtle differences between the\n",
      "preprocessing operations performed before training and the ones performed in your\n",
      "app or in the browser. This training/serving skew will lead to bugs or degraded perfor‐\n",
      "mance.\n",
      "One improvement would be to take the trained model (trained on data that was pre‐\n",
      "processed by your Apache Beam code), and before deploying it to your app or the\n",
      "browser, add an extra input layer to take care of preprocessing on the fly (either by\n",
      "writing a custom layer or by using a DenseFeatures layer). That’s definitely better,\n",
      "since now you just have two versions of your preprocessing code: the Apache Beam\n",
      "code and the preprocessing layer’s code.\n",
      "But what if you could define your preprocessing operations just once? This is what\n",
      "TF Transform was designed for. It is part of TensorFlow Extended (TFX), an end-to-\n",
      "end platform for productionizing TensorFlow models. First, to use a TFX component,\n",
      "such as TF Transform, you must install it, it does not come bundled with TensorFlow.\n",
      "Y ou define your preprocessing function just once (in Python), by using TF Transform\n",
      "functions for scaling, bucketizing, crossing features, and more. Y ou can also use any\n",
      "TensorFlow operation you need. Here is what this preprocessing function might look\n",
      "like if we just had two features:\n",
      "import tensorflow_transform as tft\n",
      "def preprocess(inputs):  # inputs is a batch of input features\n",
      "    median_age = inputs[\"housing_median_age\"]\n",
      "    ocean_proximity = inputs[\"ocean_proximity\"]\n",
      "    standardized_age = tft.scale_to_z_score(median_age - tft.mean(median_age))\n",
      "    ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
      "    return {\n",
      "        \"standardized_median_age\": standardized_age,\n",
      "428 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "11 At the time of writing, TFDS requires you to download a few files manually for ImageNet (for legal reasons),\n",
      "but this will hopefully get resolved soon.\n",
      "        \"ocean_proximity_id\": ocean_proximity_id\n",
      "    }\n",
      "Next, TF Transform lets you apply this preprocess() function to the whole training\n",
      "set using Apache Beam (it provides an AnalyzeAndTransformDataset class that you\n",
      "can use for this purpose in your Apache Beam pipeline). In the process, it will also\n",
      "compute all the necessary statistics over the whole training set: in this example, the\n",
      "mean and standard deviation of the housing_median_age feature, and the vocabulary\n",
      "for the ocean_proximity feature. The components that compute these statistics are\n",
      "called analyzers.\n",
      "Importantly, TF Transform will also generate an equivalent TensorFlow Function that\n",
      "you can plug into the model you deploy. This TF Function contains all the necessary\n",
      "statistics computed by Apache Beam (the mean, standard deviation, and vocabulary),\n",
      "simply included as constants.\n",
      "At the time of this writing, TF Transform only supports Tensor‐\n",
      "Flow 1. Moreover, Apache Beam only has partial support for\n",
      "Python 3. That said, both these limitations will likely be fixed by\n",
      "the time your read this.\n",
      "With the Data API, TFRecords, the Features API and TF Transform, you can build\n",
      "highly scalable input pipelines for training, and also benefit from fast and portable\n",
      "data preprocessing in production.\n",
      "But what if you just wanted to use a standard dataset? Well in that case, things are\n",
      "much simpler: just use TFDS!\n",
      "The TensorFlow Datasets (TFDS) Project\n",
      "The TensorFlow Datasets project makes it trivial to download common datasets, from\n",
      "small ones like MNIST or Fashion MNIST, to huge datasets like ImageNet 11 (you will\n",
      "need quite a bit of disk space!). The list includes image datasets, text datasets (includ‐\n",
      "ing translation datasets), audio and video datasets, and more. Y ou can visit https://\n",
      "homl.info/tfds to view the full list, along with a description of each dataset.\n",
      "TFDS is not bundled with TensorFlow, so you need to install the tensorflow-\n",
      "datasets library (e.g., using pip). Then all you need to do is call the tfds.load()\n",
      "function, and it will download the data you want (unless it was already downloaded\n",
      "earlier), and return the data as a dictionary of Datasets (typically one for training,\n",
      "The TensorFlow Datasets (TFDS) Project | 429\n",
      "and one for testing, but this depends on the dataset you choose). For example, let’s\n",
      "download MNIST:\n",
      "import tensorflow_datasets as tfds\n",
      "dataset = tfds.load(name=\"mnist\")\n",
      "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]\n",
      "Y ou can then apply any transformation you want (typically repeating, batching and\n",
      "prefetching), and you’re ready to train your model. Here is a simple example:\n",
      "mnist_train = mnist_train.repeat(5).batch(32).prefetch(1)\n",
      "for item in mnist_train:\n",
      "    images = item[\"image\"]\n",
      "    labels = item[\"label\"]\n",
      "    [...]\n",
      "In general, load() returns a shuffled training set, so there’s no need\n",
      "to shuffle it some more.\n",
      "Note that each item in the dataset is a dictionary containing both the features and the\n",
      "labels. But Keras expects each item to be a tuple containing 2 elements (again, the fea‐\n",
      "tures and the labels). Y ou could transform the dataset using the map() method, like\n",
      "this:\n",
      "mnist_train = mnist_train.repeat(5).batch(32)\n",
      "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
      "mnist_train = mnist_train.prefetch(1)\n",
      "Or you can just ask the load() function to do this for you by setting as_super\n",
      "vised=True (obviously this works only for labeled datasets). Y ou can also specify the\n",
      "batch size if you want. Then the dataset can be passed directly to your tf.keras model:\n",
      "dataset = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\n",
      "mnist_train = dataset[\"train\"].repeat().prefetch(1)\n",
      "model = keras.models.Sequential([...])\n",
      "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
      "model.fit(mnist_train, steps_per_epoch=60000 // 32, epochs=5)\n",
      "This was quite a technical chapter, and you may feel that it is a bit far from the\n",
      "abstract beauty of neural networks, but the fact is deep learning often involves large\n",
      "amounts of data, and knowing how to load, parse and preprocess it efficiently is a\n",
      "crucial skill to have. In the next chapter, we will look at Convolutional Neural Net‐\n",
      "works, which are among the most successful neural net architectures for image pro‐\n",
      "cessing, and many other applications.\n",
      "430 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "CHAPTER 14\n",
      "Deep Computer Vision Using Convolutional\n",
      "Neural Networks\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 14 in the final\n",
      "release of the book.\n",
      "Although IBM’s Deep Blue supercomputer beat the chess world champion Garry Kas‐\n",
      "parov back in 1996, it wasn’t until fairly recently that computers were able to reliably\n",
      "perform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\n",
      "spoken words. Why are these tasks so effortless to us humans? The answer lies in the\n",
      "fact that perception largely takes place outside the realm of our consciousness, within\n",
      "specialized visual, auditory, and other sensory modules in our brains. By the time\n",
      "sensory information reaches our consciousness, it is already adorned with high-level\n",
      "features; for example, when you look at a picture of a cute puppy, you cannot choose\n",
      "not to see the puppy, or not to notice its cuteness. Nor can you explain how you rec‐\n",
      "ognize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective\n",
      "experience: perception is not trivial at all, and to understand it we must look at how\n",
      "the sensory modules work.\n",
      "Convolutional neural networks (CNNs) emerged from the study of the brain’s visual\n",
      "cortex, and they have been used in image recognition since the 1980s. In the last few\n",
      "years, thanks to the increase in computational power, the amount of available training\n",
      "data, and the tricks presented in Chapter 11 for training deep nets, CNNs have man‐\n",
      "aged to achieve superhuman performance on some complex visual tasks. They power\n",
      "image search services, self-driving cars, automatic video classification systems, and\n",
      "more. Moreover, CNNs are not restricted to visual perception: they are also successful\n",
      "431\n",
      "1 “Single Unit Activity in Striate Cortex of Unrestrained Cats, ” D. Hubel and T. Wiesel (1958).\n",
      "2 “Receptive Fields of Single Neurones in the Cat’s Striate Cortex, ” D. Hubel and T. Wiesel (1959).\n",
      "3 “Receptive Fields and Functional Architecture of Monkey Striate Cortex, ” D. Hubel and T. Wiesel (1968).\n",
      "at many other tasks, such as voice recognition or natural language processing (NLP);\n",
      "however, we will focus on visual applications for now.\n",
      "In this chapter we will present where CNNs came from, what their building blocks\n",
      "look like, and how to implement them using TensorFlow and Keras. Then we will dis‐\n",
      "cuss some of the best CNN architectures, and discuss other visual tasks, including\n",
      "object detection (classifying multiple objects in an image and placing bounding boxes\n",
      "around them) and semantic segmentation (classifying each pixel according to the class\n",
      "of the object it belongs to).\n",
      "The Architecture of the Visual Cortex\n",
      "David H. Hubel and Torsten Wiesel performed a series of experiments on cats in\n",
      "19581 and 19592 (and a few years later on monkeys 3), giving crucial insights on the\n",
      "structure of the visual cortex (the authors received the Nobel Prize in Physiology or\n",
      "Medicine in 1981 for their work). In particular, they showed that many neurons in\n",
      "the visual cortex have a small local receptive field, meaning they react only to visual\n",
      "stimuli located in a limited region of the visual field (see Figure 14-1, in which the\n",
      "local receptive fields of five neurons are represented by dashed circles). The receptive\n",
      "fields of different neurons may overlap, and together they tile the whole visual field.\n",
      "Moreover, the authors showed that some neurons react only to images of horizontal\n",
      "lines, while others react only to lines with different orientations (two neurons may\n",
      "have the same receptive field but react to different line orientations). They also\n",
      "noticed that some neurons have larger receptive fields, and they react to more com‐\n",
      "plex patterns that are combinations of the lower-level patterns. These observations\n",
      "led to the idea that the higher-level neurons are based on the outputs of neighboring\n",
      "lower-level neurons (in Figure 14-1, notice that each neuron is connected only to a\n",
      "few neurons from the previous layer). This powerful architecture is able to detect all\n",
      "sorts of complex patterns in any area of the visual field.\n",
      "432 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "4 “Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected\n",
      "by Shift in Position, ” K. Fukushima (1980).\n",
      "5 “Gradient-Based Learning Applied to Document Recognition, ” Y . LeCun et al. (1998).\n",
      "Figure 14-1. Local receptive fields in the visual cortex\n",
      "These studies of the visual cortex inspired the neocognitron, introduced in 1980 ,4\n",
      "which gradually evolved into what we now call convolutional neural networks . An\n",
      "important milestone was a 1998 paper5 by Y ann LeCun, Léon Bottou, Y oshua Bengio,\n",
      "and Patrick Haffner, which introduced the famous LeNet-5 architecture, widely used\n",
      "to recognize handwritten check numbers. This architecture has some building blocks\n",
      "that you already know, such as fully connected layers and sigmoid activation func‐\n",
      "tions, but it also introduces two new building blocks: convolutional layers and pooling\n",
      "layers. Let’s look at them now.\n",
      "Why not simply use a regular deep neural network with fully con‐\n",
      "nected layers for image recognition tasks? Unfortunately, although\n",
      "this works fine for small images (e.g., MNIST), it breaks down for\n",
      "larger images because of the huge number of parameters it\n",
      "requires. For example, a 100 × 100 image has 10,000 pixels, and if\n",
      "the first layer has just 1,000 neurons (which already severely\n",
      "restricts the amount of information transmitted to the next layer),\n",
      "this means a total of 10 million connections. And that’s just the first\n",
      "layer. CNNs solve this problem using partially connected layers and\n",
      "weight sharing.\n",
      "The Architecture of the Visual Cortex | 433\n",
      "6 A convolution is a mathematical operation that slides one function over another and measures the integral of\n",
      "their pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform,\n",
      "and is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very\n",
      "similar to convolutions (see https://homl.info/76 for more details).\n",
      "Convolutional Layer\n",
      "The most important building block of a CNN is the convolutional layer:6 neurons in\n",
      "the first convolutional layer are not connected to every single pixel in the input image\n",
      "(like they were in previous chapters), but only to pixels in their receptive fields (see\n",
      "Figure 14-2 ). In turn, each neuron in the second convolutional layer is connected\n",
      "only to neurons located within a small rectangle in the first layer. This architecture\n",
      "allows the network to concentrate on small low-level features in the first hidden layer,\n",
      "then assemble them into larger higher-level features in the next hidden layer, and so\n",
      "on. This hierarchical structure is common in real-world images, which is one of the\n",
      "reasons why CNNs work so well for image recognition.\n",
      "Figure 14-2. CNN layers with rectangular local receptive fields\n",
      "Until now, all multilayer neural networks we looked at had layers\n",
      "composed of a long line of neurons, and we had to flatten input\n",
      "images to 1D before feeding them to the neural network. Now each\n",
      "layer is represented in 2D, which makes it easier to match neurons\n",
      "with their corresponding inputs.\n",
      "A neuron located in row i, column j of a given layer is connected to the outputs of the\n",
      "neurons in the previous layer located in rows i to i + fh – 1, columns j to j + fw – 1,\n",
      "where fh and fw are the height and width of the receptive field (see Figure 14-3). In\n",
      "order for a layer to have the same height and width as the previous layer, it is com‐\n",
      "434 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "mon to add zeros around the inputs, as shown in the diagram. This is called zero pad‐\n",
      "ding.\n",
      "Figure 14-3. Connections between layers and zero padding\n",
      "It is also possible to connect a large input layer to a much smaller layer by spacing out\n",
      "the receptive fields, as shown in Figure 14-4. The shift from one receptive field to the\n",
      "next is called the stride. In the diagram, a 5 × 7 input layer (plus zero padding) is con‐\n",
      "nected to a 3 × 4 layer, using 3 × 3 receptive fields and a stride of 2 (in this example\n",
      "the stride is the same in both directions, but it does not have to be so). A neuron loca‐\n",
      "ted in row i, column j in the upper layer is connected to the outputs of the neurons in\n",
      "the previous layer located in rows i × sh to i × sh + fh – 1, columns j × sw to j × sw + fw –\n",
      "1, where sh and sw are the vertical and horizontal strides.\n",
      "Convolutional Layer | 435\n",
      "Figure 14-4. Reducing dimensionality using a stride of 2\n",
      "Filters\n",
      "A neuron’s weights can be represented as a small image the size of the receptive field.\n",
      "For example, Figure 14-5 shows two possible sets of weights, called filters (or convolu‐\n",
      "tion kernels). The first one is represented as a black square with a vertical white line in\n",
      "the middle (it is a 7 × 7 matrix full of 0s except for the central column, which is full of\n",
      "1s); neurons using these weights will ignore everything in their receptive field except\n",
      "for the central vertical line (since all inputs will get multiplied by 0, except for the\n",
      "ones located in the central vertical line). The second filter is a black square with a\n",
      "horizontal white line in the middle. Once again, neurons using these weights will\n",
      "ignore everything in their receptive field except for the central horizontal line.\n",
      "Now if all neurons in a layer use the same vertical line filter (and the same bias term),\n",
      "and you feed the network the input image shown in Figure 14-5 (bottom image), the\n",
      "layer will output the top-left image. Notice that the vertical white lines get enhanced\n",
      "while the rest gets blurred. Similarly, the upper-right image is what you get if all neu‐\n",
      "rons use the same horizontal line filter; notice that the horizontal white lines get\n",
      "enhanced while the rest is blurred out. Thus, a layer full of neurons using the same\n",
      "filter outputs a feature map, which highlights the areas in an image that activate the\n",
      "filter the most. Of course you do not have to define the filters manually: instead, dur‐\n",
      "ing training the convolutional layer will automatically learn the most useful filters for\n",
      "its task, and the layers above will learn to combine them into more complex patterns.\n",
      "436 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "Figure 14-5. Applying two different filters to get two feature maps\n",
      "Stacking Multiple Feature Maps\n",
      "Up to now, for simplicity, I have represented the output of each convolutional layer as\n",
      "a thin 2D layer, but in reality a convolutional layer has multiple filters (you decide\n",
      "how many), and it outputs one feature map per filter, so it is more accurately repre‐\n",
      "sented in 3D (see Figure 14-6). To do so, it has one neuron per pixel in each feature\n",
      "map, and all neurons within a given feature map share the same parameters (i.e., the\n",
      "same weights and bias term). However, neurons in different feature maps use differ‐\n",
      "ent parameters. A neuron’s receptive field is the same as described earlier, but it\n",
      "extends across all the previous layers’ feature maps. In short, a convolutional layer\n",
      "simultaneously applies multiple trainable filters to its inputs, making it capable of\n",
      "detecting multiple features anywhere in its inputs.\n",
      "The fact that all neurons in a feature map share the same parame‐\n",
      "ters dramatically reduces the number of parameters in the model.\n",
      "Moreover, once the CNN has learned to recognize a pattern in one\n",
      "location, it can recognize it in any other location. In contrast, once\n",
      "a regular DNN has learned to recognize a pattern in one location, it\n",
      "can recognize it only in that particular location.\n",
      "Moreover, input images are also composed of multiple sublayers: one per color chan‐\n",
      "nel. There are typically three: red, green, and blue (RGB). Grayscale images have just\n",
      "Convolutional Layer | 437\n",
      "one channel, but some images may have much more—for example, satellite images\n",
      "that capture extra light frequencies (such as infrared).\n",
      "Figure 14-6. Convolution layers with multiple feature maps, and images with three color\n",
      "channels\n",
      "Specifically, a neuron located in row i, column j of the feature map k in a given convo‐\n",
      "lutional layer l is connected to the outputs of the neurons in the previous layer l – 1,\n",
      "located in rows i × sh to i × sh + fh – 1 and columns j × sw to j × sw + fw – 1, across all\n",
      "feature maps (in layer l – 1). Note that all neurons located in the same row i and col‐\n",
      "umn j but in different feature maps are connected to the outputs of the exact same\n",
      "neurons in the previous layer.\n",
      "Equation 14-1 summarizes the preceding explanations in one big mathematical equa‐\n",
      "tion: it shows how to compute the output of a given neuron in a convolutional layer.\n",
      "438 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "It is a bit ugly due to all the different indices, but all it does is calculate the weighted\n",
      "sum of all the inputs, plus the bias term.\n",
      "Equation 14-1. Computing the output of a neuron in a convolutional layer\n",
      "zi, j, k = bk + ∑\n",
      "u = 0\n",
      "f h − 1\n",
      "∑\n",
      "v = 0\n",
      "f w − 1\n",
      "∑\n",
      "k′ = 0\n",
      "f n′ − 1\n",
      "xi′, j′, k′ . wu, v, k′, k with\n",
      "i′ = i × sh + u\n",
      "j′ = j × sw + v\n",
      "• zi, j, k is the output of the neuron located in row i, column j in feature map k of the\n",
      "convolutional layer (layer l).\n",
      "• As explained earlier, sh and sw are the vertical and horizontal strides, fh and fw are\n",
      "the height and width of the receptive field, and fn′ is the number of feature maps\n",
      "in the previous layer (layer l – 1).\n",
      "• xi′, j′, k′ is the output of the neuron located in layer l – 1, row i′, column j′, feature\n",
      "map k′ (or channel k′ if the previous layer is the input layer).\n",
      "• bk is the bias term for feature map k (in layer l). Y ou can think of it as a knob that\n",
      "tweaks the overall brightness of the feature map k.\n",
      "• wu, v, k′ ,k is the connection weight between any neuron in feature map k of the layer\n",
      "l and its input located at row u, column v (relative to the neuron’s receptive field),\n",
      "and feature map k′.\n",
      "TensorFlow Implementation\n",
      "In TensorFlow, each input image is typically represented as a 3D tensor of shape\n",
      "[height, width, channels]. A mini-batch is represented as a 4D tensor of shape\n",
      "[mini-batch size, height, width, channels] . The weights of a convolutional\n",
      "layer are represented as a 4D tensor of shape [ fh, fw, fn′, fn]. The bias terms of a convo‐\n",
      "lutional layer are simply represented as a 1D tensor of shape [fn].\n",
      "Let’s look at a simple example. The following code loads two sample images, using\n",
      "Scikit-Learn’s load_sample_images() (which loads two color images, one of a Chi‐\n",
      "nese temple, and the other of a flower). The pixel intensities (for each color channel)\n",
      "is represented as a byte from 0 to 255, so we scale these features simply by dividing by\n",
      "255, to get floats ranging from 0 to 1. Then we create two 7 × 7 filters (one with a\n",
      "vertical white line in the middle, and the other with a horizontal white line in the\n",
      "middle), and we apply them to both images using the tf.nn.conv2d() function,\n",
      "which is part of TensorFlow’s low-level Deep Learning API. In this example, we use\n",
      "zero padding (padding=\"SAME\") and a stride of 2. Finally, we plot one of the resulting\n",
      "feature maps (similar to the top-right image in Figure 14-5).\n",
      "Convolutional Layer | 439\n",
      "from sklearn.datasets import load_sample_image\n",
      "# Load sample images\n",
      "china = load_sample_image(\"china.jpg\") / 255\n",
      "flower = load_sample_image(\"flower.jpg\") / 255\n",
      "images = np.array([china, flower])\n",
      "batch_size, height, width, channels = images.shape\n",
      "# Create 2 filters\n",
      "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
      "filters[:, 3, :, 0] = 1  # vertical line\n",
      "filters[3, :, :, 1] = 1  # horizontal line\n",
      "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\n",
      "plt.imshow(outputs[0, :, :, 1], cmap=\"gray\") # plot 1st image's 2nd feature map\n",
      "plt.show()\n",
      "Most of this code is self-explanatory, but the tf.nn.conv2d() line deserves a bit of\n",
      "explanation:\n",
      "• images is the input mini-batch (a 4D tensor, as explained earlier).\n",
      "• filters is the set of filters to apply (also a 4D tensor, as explained earlier).\n",
      "• strides is equal to 1, but it could also be a 1D array with 4 elements, where the\n",
      "two central elements are the vertical and horizontal strides ( sh and sw). The first\n",
      "and last elements must currently be equal to 1. They may one day be used to\n",
      "specify a batch stride (to skip some instances) and a channel stride (to skip some\n",
      "of the previous layer’s feature maps or channels).\n",
      "• padding must be either \"VALID\" or \"SAME\":\n",
      "— If set to \"VALID\", the convolutional layer does not use zero padding, and may\n",
      "ignore some rows and columns at the bottom and right of the input image,\n",
      "depending on the stride, as shown in Figure 14-7 (for simplicity, only the hor‐\n",
      "izontal dimension is shown here, but of course the same logic applies to the\n",
      "vertical dimension).\n",
      "— If set to \"SAME\", the convolutional layer uses zero padding if necessary. In this\n",
      "case, the number of output neurons is equal to the number of input neurons\n",
      "divided by the stride, rounded up (in this example, 13 / 5 = 2.6, rounded up to\n",
      "3). Then zeros are added as evenly as possible around the inputs.\n",
      "440 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "Figure 14-7. Padding options—input width: 13, filter width: 6, stride: 5\n",
      "In this example, we manually defined the filters, but in a real CNN you would nor‐\n",
      "mally define filters as trainable variables, so the neural net can learn which filters\n",
      "work best, as explained earlier. Instead of manually creating the variables, however,\n",
      "you can simply use the keras.layers.Conv2D layer:\n",
      "conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1,\n",
      "                           padding=\"SAME\", activation=\"relu\")\n",
      "This code creates a Conv2D layer with 32 filters, each 3 × 3, using a stride of 1 (both\n",
      "horizontally and vertically), SAME padding, and applying the ReLU activation func‐\n",
      "tion to its outputs. As you can see, convolutional layers have quite a few hyperpara‐\n",
      "meters: you must choose the number of filters, their height and width, the strides, and\n",
      "the padding type. As always, you can use cross-validation to find the right hyperpara‐\n",
      "meter values, but this is very time-consuming. We will discuss common CNN archi‐\n",
      "tectures later, to give you some idea of what hyperparameter values work best in \n",
      "practice.\n",
      "Memory Requirements\n",
      "Another problem with CNNs is that the convolutional layers require a huge amount\n",
      "of RAM. This is especially true during training, because the reverse pass of backpro‐\n",
      "pagation requires all the intermediate values computed during the forward pass.\n",
      "For example, consider a convolutional layer with 5 × 5 filters, outputting 200 feature\n",
      "maps of size 150 × 100, with stride 1 and SAME padding. If the input is a 150 × 100\n",
      "Convolutional Layer | 441\n",
      "7 A fully connected layer with 150 × 100 neurons, each connected to all 150 × 100 × 3 inputs, would have 1502\n",
      "× 1002 × 3 = 675 million parameters!\n",
      "8 In the international system of units (SI), 1 MB = 1,000 kB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits.\n",
      "RGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200\n",
      "= 15,200 (the +1 corresponds to the bias terms), which is fairly small compared to a\n",
      "fully connected layer.7 However, each of the 200 feature maps contains 150 × 100 neu‐\n",
      "rons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 =\n",
      "75 inputs: that’s a total of 225 million float multiplications. Not as bad as a fully con‐\n",
      "nected layer, but still quite computationally intensive. Moreover, if the feature maps\n",
      "are represented using 32-bit floats, then the convolutional layer’s output will occupy\n",
      "200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM. 8 And that’s just for one\n",
      "instance! If a training batch contains 100 instances, then this layer will use up 1.2 GB\n",
      "of RAM!\n",
      "During inference (i.e., when making a prediction for a new instance) the RAM occu‐\n",
      "pied by one layer can be released as soon as the next layer has been computed, so you\n",
      "only need as much RAM as required by two consecutive layers. But during training\n",
      "everything computed during the forward pass needs to be preserved for the reverse\n",
      "pass, so the amount of RAM needed is (at least) the total amount of RAM required by\n",
      "all layers.\n",
      "If training crashes because of an out-of-memory error, you can try\n",
      "reducing the mini-batch size. Alternatively, you can try reducing\n",
      "dimensionality using a stride, or removing a few layers. Or you can\n",
      "try using 16-bit floats instead of 32-bit floats. Or you could distrib‐\n",
      "ute the CNN across multiple devices.\n",
      "Now let’s look at the second common building block of CNNs: the pooling layer.\n",
      "Pooling Layer\n",
      "Once you understand how convolutional layers work, the pooling layers are quite\n",
      "easy to grasp. Their goal is to subsample (i.e., shrink) the input image in order to\n",
      "reduce the computational load, the memory usage, and the number of parameters\n",
      "(thereby limiting the risk of overfitting).\n",
      "Just like in convolutional layers, each neuron in a pooling layer is connected to the\n",
      "outputs of a limited number of neurons in the previous layer, located within a small\n",
      "rectangular receptive field. Y ou must define its size, the stride, and the padding type,\n",
      "just like before. However, a pooling neuron has no weights; all it does is aggregate the\n",
      "inputs using an aggregation function such as the max or mean. Figure 14-8 shows a\n",
      "max pooling layer, which is the most common type of pooling layer. In this example,\n",
      "442 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "9 Other kernels we discussed so far had weights, but pooling kernels do not: they are just stateless sliding win‐\n",
      "dows.\n",
      "we use a 2 × 2 _pooling kernel_ 9, with a stride of 2, and no padding. Only the max\n",
      "input value in each receptive field makes it to the next layer, while the other inputs\n",
      "are dropped. For example, in the lower left receptive field in Figure 14-8, the input\n",
      "values are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because\n",
      "of the stride of 2, the output image has half the height and half the width of the input\n",
      "image (rounded down since we use no padding).\n",
      "Figure 14-8. Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)\n",
      "A pooling layer typically works on every input channel independ‐\n",
      "ently, so the output depth is the same as the input depth.\n",
      "Other than reducing computations, memory usage and the number of parameters, a\n",
      "max pooling layer also introduces some level of invariance to small translations, as\n",
      "shown in Figure 14-9. Here we assume that the bright pixels have a lower value than\n",
      "dark pixels, and we consider 3 images (A, B, C) going through a max pooling layer\n",
      "with a 2 × 2 kernel and stride 2. Images B and C are the same as image A, but shifted\n",
      "by one and two pixels to the right. As you can see, the outputs of the max pooling\n",
      "layer for images A and B are identical. This is what translation invariance means.\n",
      "However, for image C, the output is different: it is shifted by one pixel to the right\n",
      "(but there is still 75% invariance). By inserting a max pooling layer every few layers in\n",
      "a CNN, it is possible to get some level of translation invariance at a larger scale.\n",
      "Moreover, max pooling also offers a small amount of rotational invariance and a\n",
      "slight scale invariance. Such invariance (even if it is limited) can be useful in cases\n",
      "where the prediction should not depend on these details, such as in classification\n",
      "tasks.\n",
      "Pooling Layer | 443\n",
      "Figure 14-9. Invariance to small translations\n",
      "But max pooling has some downsides: firstly, it is obviously very destructive: even\n",
      "with a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both\n",
      "directions (so its area will be four times smaller), simply dropping 75% of the input\n",
      "values. And in some applications, invariance is not desirable, for example for seman‐\n",
      "tic segmentation: this is the task of classifying each pixel in an image depending on the\n",
      "object that pixel belongs to: obviously, if the input image is translated by 1 pixel to the\n",
      "right, the output should also be translated by 1 pixel to the right. The goal in this case\n",
      "is equivariance, not invariance: a small change to the inputs should lead to a corre‐\n",
      "sponding small change in the output.\n",
      "TensorFlow Implementation\n",
      "Implementing a max pooling layer in TensorFlow is quite easy. The following code\n",
      "creates a max pooling layer using a 2 × 2 kernel. The strides default to the kernel size,\n",
      "so this layer will use a stride of 2 (both horizontally and vertically). By default, it uses\n",
      "V ALID padding (i.e., no padding at all):\n",
      "max_pool = keras.layers.MaxPool2D(pool_size=2)\n",
      "To create an average pooling layer, just use AvgPool2D instead of MaxPool2D. As you\n",
      "might expect, it works exactly like a max pooling layer, except it computes the mean\n",
      "rather than the max. Average pooling layers used to be very popular, but people\n",
      "444 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "mostly use max pooling layers now, as they generally perform better. This may seem\n",
      "surprising, since computing the mean generally loses less information than comput‐\n",
      "ing the max. But on the other hand, max pooling preserves only the strongest feature,\n",
      "getting rid of all the meaningless ones, so the next layers get a cleaner signal to work\n",
      "with. Moreover, max pooling offers stronger translation invariance than average\n",
      "pooling.\n",
      "Note that max pooling and average pooling can be performed along the depth dimen‐\n",
      "sion rather than the spatial dimensions, although this is not as common. This can\n",
      "allow the CNN to learn to be invariant to various features. For example, it could learn\n",
      "multiple filters, each detecting a different rotation of the same pattern, such as hand-\n",
      "written digits (see Figure 14-10), and the depth-wise max pooling layer would ensure\n",
      "that the output is the same regardless of the rotation. The CNN could similarly learn\n",
      "to be invariant to anything else: thickness, brightness, skew, color, and so on.\n",
      "Figure 14-10. Depth-wise max pooling can help the CNN learn any invariance\n",
      "Pooling Layer | 445\n",
      "Keras does not include a depth-wise max pooling layer, but TensorFlow’s low-level\n",
      "Deep Learning API does: just use the tf.nn.max_pool() function, and specify the\n",
      "kernel size and strides as 4-tuples. The first three values of each should be 1: this indi‐\n",
      "cates that the kernel size and stride along the batch, height and width dimensions\n",
      "shoud be 1. The last value should be whatever kernel size and stride you want along\n",
      "the depth dimension, for example 3 (this must be a divisor of the input depth; for\n",
      "example, it will not work if the previous layer outputs 20 feature maps, since 20 is not\n",
      "a multiple of 3):\n",
      "output = tf.nn.max_pool(images,\n",
      "                        ksize=(1, 1, 1, 3),\n",
      "                        strides=(1, 1, 1, 3),\n",
      "                        padding=\"VALID\")\n",
      "If you want to include this as a layer in your Keras models, you can simply wrap it in\n",
      "a Lambda layer (or create a custom Keras layer):\n",
      "depth_pool = keras.layers.Lambda(\n",
      "    lambda X: tf.nn.max_pool(X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3),\n",
      "                             padding=\"VALID\"))\n",
      "One last type of pooling layer that you will often see in modern architectures is the\n",
      "global average pooling layer. It works very differently: all it does is compute the mean\n",
      "of each entire feature map (it’s like an average pooling layer using a pooling kernel\n",
      "with the same spatial dimensions as the inputs). This means that it just outputs a sin‐\n",
      "gle number per feature map and per instance. Although this is of course extremely\n",
      "destructive (most of the information in the feature map is lost), it can be useful as the\n",
      "output layer, as we will see later in this chapter. To create such a layer, simply use the\n",
      "keras.layers.GlobalAvgPool2D class:\n",
      "global_avg_pool = keras.layers.GlobalAvgPool2D()\n",
      "It is actually equivalent to this simple Lamba layer, which computes the mean over the\n",
      "spatial dimensions (height and width):\n",
      "global_avg_pool = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))\n",
      "Now you know all the building blocks to create a convolutional neural network. Let’s\n",
      "see how to assemble them.\n",
      "CNN Architectures\n",
      "Typical CNN architectures stack a few convolutional layers (each one generally fol‐\n",
      "lowed by a ReLU layer), then a pooling layer, then another few convolutional layers\n",
      "(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller\n",
      "as it progresses through the network, but it also typically gets deeper and deeper (i.e.,\n",
      "with more feature maps) thanks to the convolutional layers (see Figure 14-11). At the\n",
      "top of the stack, a regular feedforward neural network is added, composed of a few\n",
      "446 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a\n",
      "softmax layer that outputs estimated class probabilities).\n",
      "Figure 14-11. Typical CNN architecture\n",
      "A common mistake is to use convolution kernels that are too large.\n",
      "For example, instead of using a convolutional layer with a 5 × 5\n",
      "kernel, it is generally preferable to stack two layers with 3 × 3 ker‐\n",
      "nels: it will use less parameters and require less computations, and\n",
      "it will usually perform better. One exception to this recommenda‐\n",
      "tion is for the first convolutional layer: it can typically have a large\n",
      "kernel (e.g., 5 × 5), usually with stride of 2 or more: this will reduce\n",
      "the spatial dimension of the image without losing too much infor‐\n",
      "mation, and since the input image only has 3 channels in general, it\n",
      "will not be too costly.\n",
      "Here is how you can implement a simple CNN to tackle the fashion MNIST dataset\n",
      "(introduced in Chapter 10):\n",
      "from functools import partial\n",
      "DefaultConv2D = partial(keras.layers.Conv2D,\n",
      "                        kernel_size=3, activation='relu', padding=\"SAME\")\n",
      "model = keras.models.Sequential([\n",
      "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
      "    keras.layers.MaxPooling2D(pool_size=2),\n",
      "    DefaultConv2D(filters=128),\n",
      "    DefaultConv2D(filters=128),\n",
      "    keras.layers.MaxPooling2D(pool_size=2),\n",
      "    DefaultConv2D(filters=256),\n",
      "    DefaultConv2D(filters=256),\n",
      "    keras.layers.MaxPooling2D(pool_size=2),\n",
      "    keras.layers.Flatten(),\n",
      "    keras.layers.Dense(units=128, activation='relu'),\n",
      "    keras.layers.Dropout(0.5),\n",
      "    keras.layers.Dense(units=64, activation='relu'),\n",
      "    keras.layers.Dropout(0.5),\n",
      "    keras.layers.Dense(units=10, activation='softmax'),\n",
      "])\n",
      "CNN Architectures | 447\n",
      "• In this code, we start by using the partial() function to define a thin wrapper\n",
      "around the Conv2D class, called DefaultConv2D: it simply avoids having to repeat\n",
      "the same hyperparameter values over and over again.\n",
      "• The first layer uses a large kernel size, but no stride because the input images are\n",
      "not very large. It also sets input_shape=[28, 28, 1], which means the images\n",
      "are 28 × 28 pixels, with a single color channel (i.e., grayscale).\n",
      "• Next, we have a max pooling layer, which divides each spatial dimension by a fac‐\n",
      "tor of two (since pool_size=2).\n",
      "• Then we repeat the same structure twice: two convolutional layers followed by a\n",
      "max pooling layer. For larger images, we could repeat this structure several times\n",
      "(the number of repetitions is a hyperparameter you can tune).\n",
      "• Note that the number of filters grows as we climb up the CNN towards the out‐\n",
      "put layer (it is initially 64, then 128, then 256): it makes sense for it to grow, since\n",
      "the number of low level features is often fairly low (e.g., small circles, horizontal\n",
      "lines, etc.), but there are many different ways to combine them into higher level\n",
      "features. It is a common practice to double the number of filters after each pool‐\n",
      "ing layer: since a pooling layer divides each spatial dimension by a factor of 2, we\n",
      "can afford doubling the number of feature maps in the next layer, without fear of\n",
      "exploding the number of parameters, memory usage, or computational load.\n",
      "• Next is the fully connected network, composed of 2 hidden dense layers and a\n",
      "dense output layer. Note that we must flatten its inputs, since a dense network\n",
      "expects a 1D array of features for each instance. We also add two dropout layers,\n",
      "with a dropout rate of 50% each, to reduce overfitting.\n",
      "This CNN reaches over 92% accuracy on the test set. It’s not the state of the art, but it\n",
      "is pretty good, and clearly much better than what we achieved with dense networks in\n",
      "Chapter 10.\n",
      "Over the years, variants of this fundamental architecture have been developed, lead‐\n",
      "ing to amazing advances in the field. A good measure of this progress is the error rate\n",
      "in competitions such as the ILSVRC ImageNet challenge . In this competition the\n",
      "top-5 error rate for image classification fell from over 26% to less than 2.3% in just six\n",
      "years. The top-five error rate is the number of test images for which the system’s top 5\n",
      "predictions did not include the correct answer. The images are large (256 pixels high)\n",
      "and there are 1,000 classes, some of which are really subtle (try distinguishing 120\n",
      "dog breeds). Looking at the evolution of the winning entries is a good way to under‐\n",
      "stand how CNNs work.\n",
      "We will first look at the classical LeNet-5 architecture (1998), then three of the win‐\n",
      "ners of the ILSVRC challenge: AlexNet (2012), GoogLeNet (2014), and ResNet\n",
      "(2015).\n",
      "448 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "10 “Gradient-Based Learning Applied to Document Recognition” , Y . LeCun, L. Bottou, Y . Bengio and P . Haffner\n",
      "(1998).\n",
      "LeNet-5\n",
      "The LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As\n",
      "mentioned earlier, it was created by Y ann LeCun in 1998 and widely used for hand‐\n",
      "written digit recognition (MNIST). It is composed of the layers shown in Table 14-1.\n",
      "Table 14-1. LeNet-5 architecture\n",
      "Layer Type Maps Size Kernel size Stride Activation\n",
      "Out Fully Connected – 10 – – RBF\n",
      "F6 Fully Connected – 84 – – tanh\n",
      "C5 Convolution 120 1 × 1 5 × 5 1 tanh\n",
      "S4 Avg Pooling 16 5 × 5 2 × 2 2 tanh\n",
      "C3 Convolution 16 10 × 10 5 × 5 1 tanh\n",
      "S2 Avg Pooling 6 14 × 14 2 × 2 2 tanh\n",
      "C1 Convolution 6 28 × 28 5 × 5 1 tanh\n",
      "In Input 1 32 × 32 – – –\n",
      "There are a few extra details to be noted:\n",
      "• MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and\n",
      "normalized before being fed to the network. The rest of the network does not use\n",
      "any padding, which is why the size keeps shrinking as the image progresses\n",
      "through the network.\n",
      "• The average pooling layers are slightly more complex than usual: each neuron\n",
      "computes the mean of its inputs, then multiplies the result by a learnable coeffi‐\n",
      "cient (one per map) and adds a learnable bias term (again, one per map), then\n",
      "finally applies the activation function.\n",
      "• Most neurons in C3 maps are connected to neurons in only three or four S2\n",
      "maps (instead of all six S2 maps). See table 1 (page 8) in the original paper 10 for\n",
      "details.\n",
      "• The output layer is a bit special: instead of computing the matrix multiplication\n",
      "of the inputs and the weight vector, each neuron outputs the square of the Eucli‐\n",
      "dian distance between its input vector and its weight vector. Each output meas‐\n",
      "ures how much the image belongs to a particular digit class. The cross entropy \n",
      "cost function is now preferred, as it penalizes bad predictions much more, pro‐\n",
      "ducing larger gradients and converging faster.\n",
      "CNN Architectures | 449\n",
      "11 “ImageNet Classification with Deep Convolutional Neural Networks, ” A. Krizhevsky et al. (2012).\n",
      "Y ann LeCun’s website (“LENET” section) features great demos of LeNet-5 classifying \n",
      "digits.\n",
      "AlexNet\n",
      "The AlexNet CNN architecture 11 won the 2012 ImageNet ILSVRC challenge by a\n",
      "large margin: it achieved 17% top-5 error rate while the second best achieved only\n",
      "26%! It was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and\n",
      "Geoffrey Hinton. It is quite similar to LeNet-5, only much larger and deeper, and it\n",
      "was the first to stack convolutional layers directly on top of each other, instead of\n",
      "stacking a pooling layer on top of each convolutional layer. Table 14-2 presents this\n",
      "architecture.\n",
      "Table 14-2. AlexNet architecture\n",
      "Layer Type Maps Size Kernel size Stride Padding Activation\n",
      "Out Fully Connected – 1,000 – – – Softmax\n",
      "F9 Fully Connected – 4,096 – – – ReLU\n",
      "F8 Fully Connected – 4,096 – – – ReLU\n",
      "C7 Convolution 256 13 × 13 3 × 3 1 SAME ReLU\n",
      "C6 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\n",
      "C5 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\n",
      "S4 Max Pooling 256 13 × 13 3 × 3 2 VALID –\n",
      "C3 Convolution 256 27 × 27 5 × 5 1 SAME ReLU\n",
      "S2 Max Pooling 96 27 × 27 3 × 3 2 VALID –\n",
      "C1 Convolution 96 55 × 55 11 × 11 4 VALID ReLU\n",
      "In Input 3 (RGB) 227 × 227 – – – –\n",
      "To reduce overfitting, the authors used two regularization techniques: first they\n",
      "applied dropout (introduced in Chapter 11) with a 50% dropout rate during training\n",
      "to the outputs of layers F8 and F9. Second, they performed data augmentation by ran‐\n",
      "domly shifting the training images by various offsets, flipping them horizontally, and\n",
      "changing the lighting conditions.\n",
      "Data Augmentation\n",
      "Data augmentation artificially increases the size of the training set by generating\n",
      "many realistic variants of each training instance. This reduces overfitting, making this\n",
      "a regularization technique. The generated instances should be as realistic as possible:\n",
      "450 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "ideally, given an image from the augmented training set, a human should not be able\n",
      "to tell whether it was augmented or not. Moreover, simply adding white noise will not\n",
      "help; the modifications should be learnable (white noise is not).\n",
      "For example, you can slightly shift, rotate, and resize every picture in the training set\n",
      "by various amounts and add the resulting pictures to the training set (see\n",
      "Figure 14-12). This forces the model to be more tolerant to variations in the position,\n",
      "orientation, and size of the objects in the pictures. If you want the model to be more\n",
      "tolerant to different lighting conditions, you can similarly generate many images with\n",
      "various contrasts. In general, you can also flip the pictures horizontally (except for\n",
      "text, and other non-symmetrical objects). By combining these transformations you\n",
      "can greatly increase the size of your training set.\n",
      "Figure 14-12. Generating new training instances from existing ones\n",
      "AlexNet also uses a competitive normalization step immediately after the ReLU step\n",
      "of layers C1 and C3, called local response normalization. The most strongly activated\n",
      "neurons inhibit other neurons located at the same position in neighboring feature\n",
      "maps (such competitive activation has been observed in biological neurons). This\n",
      "encourages different feature maps to specialize, pushing them apart and forcing them\n",
      "CNN Architectures | 451\n",
      "12 “Going Deeper with Convolutions, ” C. Szegedy et al. (2015).\n",
      "13 In the 2010 movie Inception, the characters keep going deeper and deeper into multiple layers of dreams,\n",
      "hence the name of these modules.\n",
      "to explore a wider range of features, ultimately improving generalization. Equation\n",
      "14-2 shows how to apply LRN.\n",
      "Equation 14-2. Local response normalization\n",
      "bi = ai k + α ∑\n",
      "j = jlow\n",
      "jhigh\n",
      "aj\n",
      "2\n",
      "−β\n",
      "with\n",
      "jhigh = min i + r\n",
      "2, f n − 1\n",
      "jlow = max 0, i − r\n",
      "2\n",
      "• bi is the normalized output of the neuron located in feature map i, at some row u\n",
      "and column v (note that in this equation we consider only neurons located at this\n",
      "row and column, so u and v are not shown).\n",
      "• ai is the activation of that neuron after the ReLU step, but before normalization.\n",
      "• k, α, β, and r are hyperparameters. k is called the bias, and r is called the depth\n",
      "radius.\n",
      "• fn is the number of feature maps.\n",
      "For example, if r = 2 and a neuron has a strong activation, it will inhibit the activation\n",
      "of the neurons located in the feature maps immediately above and below its own.\n",
      "In AlexNet, the hyperparameters are set as follows: r = 2, α = 0.00002, β = 0.75, and k\n",
      "= 1. This step can be implemented using the tf.nn.local_response_normaliza\n",
      "tion() function (which you can wrap in a Lambda layer if you want to use it in a\n",
      "Keras model).\n",
      "A variant of AlexNet called ZF Net was developed by Matthew Zeiler and Rob Fergus\n",
      "and won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked \n",
      "hyperparameters (number of feature maps, kernel size, stride, etc.).\n",
      "GoogLeNet\n",
      "The GoogLeNet architecture was developed by Christian Szegedy et al. from Google\n",
      "Research,12 and it won the ILSVRC 2014 challenge by pushing the top-5 error rate\n",
      "below 7%. This great performance came in large part from the fact that the network\n",
      "was much deeper than previous CNNs (see Figure 14-14). This was made possible by\n",
      "sub-networks called inception modules,13 which allow GoogLeNet to use parameters\n",
      "452 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "much more efficiently than previous architectures: GoogLeNet actually has 10 times\n",
      "fewer parameters than AlexNet (roughly 6 million instead of 60 million).\n",
      "Figure 14-13 shows the architecture of an inception module. The notation “3 × 3 +\n",
      "1(S)” means that the layer uses a 3 × 3 kernel, stride 1, and SAME padding. The input\n",
      "signal is first copied and fed to four different layers. All convolutional layers use the\n",
      "ReLU activation function. Note that the second set of convolutional layers uses differ‐\n",
      "ent kernel sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different\n",
      "scales. Also note that every single layer uses a stride of 1 and SAME padding (even\n",
      "the max pooling layer), so their outputs all have the same height and width as their\n",
      "inputs. This makes it possible to concatenate all the outputs along the depth dimen‐\n",
      "sion in the final depth concat layer (i.e., stack the feature maps from all four top con‐\n",
      "volutional layers). This concatenation layer can be implemented in TensorFlow using\n",
      "the tf.concat() operation, with axis=3 (axis 3 is the depth).\n",
      "Figure 14-13. Inception module\n",
      "Y ou may wonder why inception modules have convolutional layers with 1 × 1 ker‐\n",
      "nels. Surely these layers cannot capture any features since they look at only one pixel\n",
      "at a time? In fact, these layers serve three purposes:\n",
      "• First, although they cannot capture spatial patterns, they can capture patterns\n",
      "along the depth dimension.\n",
      "• Second, they are configured to output fewer feature maps than their inputs, so\n",
      "they serve as bottleneck layers, meaning they reduce dimensionality. This cuts the\n",
      "computational cost and the number of parameters, speeding up training and\n",
      "improving generalization.\n",
      "• Lastly, each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) acts like\n",
      "a single, powerful convolutional layer, capable of capturing more complex pat‐\n",
      "terns. Indeed, instead of sweeping a simple linear classifier across the image (as a\n",
      "CNN Architectures | 453\n",
      "single convolutional layer does), this pair of convolutional layers sweeps a two-\n",
      "layer neural network across the image.\n",
      "In short, you can think of the whole inception module as a convolutional layer on\n",
      "steroids, able to output feature maps that capture complex patterns at various scales.\n",
      "The number of convolutional kernels for each convolutional layer\n",
      "is a hyperparameter. Unfortunately, this means that you have six\n",
      "more hyperparameters to tweak for every inception layer you add.\n",
      "Now let’s look at the architecture of the GoogLeNet CNN (see Figure 14-14 ). The\n",
      "number of feature maps output by each convolutional layer and each pooling layer is\n",
      "shown before the kernel size. The architecture is so deep that it has to be represented\n",
      "in three columns, but GoogLeNet is actually one tall stack, including nine inception\n",
      "modules (the boxes with the spinning tops). The six numbers in the inception mod‐\n",
      "ules represent the number of feature maps output by each convolutional layer in the\n",
      "module (in the same order as in Figure 14-13). Note that all the convolutional layers\n",
      "use the ReLU activation function.\n",
      "454 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "Figure 14-14. GoogLeNet architecture\n",
      "Let’s go through this network:\n",
      "• The first two layers divide the image’s height and width by 4 (so its area is divided\n",
      "by 16), to reduce the computational load. The first layer uses a large kernel size,\n",
      "so that much of the information is still preserved.\n",
      "• Then the local response normalization layer ensures that the previous layers learn\n",
      "a wide variety of features (as discussed earlier).\n",
      "• Two convolutional layers follow, where the first acts like a bottleneck layer. As\n",
      "explained earlier, you can think of this pair as a single smarter convolutional\n",
      "layer.\n",
      "• Again, a local response normalization layer ensures that the previous layers cap‐\n",
      "ture a wide variety of patterns.\n",
      "CNN Architectures | 455\n",
      "14 “Very Deep Convolutional Networks for Large-Scale Image Recognition, ” K. Simonyan and A. Zisserman\n",
      "(2015).\n",
      "• Next a max pooling layer reduces the image height and width by 2, again to speed\n",
      "up computations.\n",
      "• Then comes the tall stack of nine inception modules, interleaved with a couple\n",
      "max pooling layers to reduce dimensionality and speed up the net.\n",
      "• Next, the global average pooling layer simply outputs the mean of each feature\n",
      "map: this drops any remaining spatial information, which is fine since there was\n",
      "not much spatial information left at that point. Indeed, GoogLeNet input images\n",
      "are typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each\n",
      "dividing the height and width by 2, the feature maps are down to 7 × 7. More‐\n",
      "over, it is a classification task, not localization, so it does not matter where the\n",
      "object is. Thanks to the dimensionality reduction brought by this layer, there is\n",
      "no need to have several fully connected layers at the top of the CNN (like in\n",
      "AlexNet), and this considerably reduces the number of parameters in the net‐\n",
      "work and limits the risk of overfitting.\n",
      "• The last layers are self-explanatory: dropout for regularization, then a fully con‐\n",
      "nected layer with 1,000 units, since there are a 1,000 classes, and a softmax acti‐\n",
      "vation function to output estimated class probabilities.\n",
      "This diagram is slightly simplified: the original GoogLeNet architecture also included\n",
      "two auxiliary classifiers plugged on top of the third and sixth inception modules.\n",
      "They were both composed of one average pooling layer, one convolutional layer, two\n",
      "fully connected layers, and a softmax activation layer. During training, their loss\n",
      "(scaled down by 70%) was added to the overall loss. The goal was to fight the vanish‐\n",
      "ing gradients problem and regularize the network. However, it was later shown that\n",
      "their effect was relatively minor.\n",
      "Several variants of the GoogLeNet architecture were later proposed by Google\n",
      "researchers, including Inception-v3 and Inception-v4, using slightly different incep‐\n",
      "tion modules, and reaching even better performance.\n",
      "VGGNet\n",
      "The runner up in the ILSVRC 2014 challenge was VGGNet14, developed by K. Simon‐\n",
      "yan and A. Zisserman. It had a very simple and classical architecture, with 2 or 3 con‐\n",
      "volutional layers, a pooling layer, then again 2 or 3 convolutional layers, a pooling\n",
      "layer, and so on (with a total of just 16 convolutional layers), plus a final dense net‐\n",
      "work with 2 hidden layers and the output layer. It used only 3 × 3 filters, but many\n",
      "filters.\n",
      "456 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "15 “Deep Residual Learning for Image Recognition, ” K. He (2015).\n",
      "ResNet\n",
      "The ILSVRC 2015 challenge was won using a Residual Network (or ResNet), devel‐\n",
      "oped by Kaiming He et al., 15 which delivered an astounding top-5 error rate under\n",
      "3.6%, using an extremely deep CNN composed of 152 layers. It confirmed the general\n",
      "trend: models are getting deeper and deeper, with fewer and fewer parameters. The\n",
      "key to being able to train such a deep network is to use skip connections (also called\n",
      "shortcut connections): the signal feeding into a layer is also added to the output of a\n",
      "layer located a bit higher up the stack. Let’s see why this is useful.\n",
      "When training a neural network, the goal is to make it model a target function h(x).\n",
      "If you add the input x to the output of the network (i.e., you add a skip connection),\n",
      "then the network will be forced to model f(x) = h(x) – x rather than h(x). This is\n",
      "called residual learning (see Figure 14-15).\n",
      "Figure 14-15. Residual learning\n",
      "When you initialize a regular neural network, its weights are close to zero, so the net‐\n",
      "work just outputs values close to zero. If you add a skip connection, the resulting net‐\n",
      "work just outputs a copy of its inputs; in other words, it initially models the identity\n",
      "function. If the target function is fairly close to the identity function (which is often\n",
      "the case), this will speed up training considerably.\n",
      "Moreover, if you add many skip connections, the network can start making progress\n",
      "even if several layers have not started learning yet (see Figure 14-16). Thanks to skip\n",
      "connections, the signal can easily make its way across the whole network. The deep\n",
      "residual network can be seen as a stack of residual units, where each residual unit is a\n",
      "small neural network with a skip connection.\n",
      "CNN Architectures | 457\n",
      "Figure 14-16. Regular deep neural network (left) and deep residual network (right)\n",
      "Now let’s look at ResNet’s architecture (see Figure 14-17 ). It is actually surprisingly\n",
      "simple. It starts and ends exactly like GoogLeNet (except without a dropout layer),\n",
      "and in between is just a very deep stack of simple residual units. Each residual unit is\n",
      "composed of two convolutional layers (and no pooling layer!), with Batch Normaliza‐\n",
      "tion (BN) and ReLU activation, using 3 × 3 kernels and preserving spatial dimensions\n",
      "(stride 1, SAME padding).\n",
      "Figure 14-17. ResNet architecture\n",
      "Note that the number of feature maps is doubled every few residual units, at the same\n",
      "time as their height and width are halved (using a convolutional layer with stride 2).\n",
      "When this happens the inputs cannot be added directly to the outputs of the residual\n",
      "unit since they don’t have the same shape (for example, this problem affects the skip\n",
      "458 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "16 “Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, ” C. Szegedy et al.\n",
      "(2016).\n",
      "17 “Xception: Deep Learning with Depthwise Separable Convolutions, ” François Chollet (2016)\n",
      "connection represented by the dashed arrow in Figure 14-17). To solve this problem,\n",
      "the inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right\n",
      "number of output feature maps (see Figure 14-18).\n",
      "Figure 14-18. Skip connection when changing feature map size and depth\n",
      "ResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and\n",
      "the fully connected layer) containing three residual units that output 64 feature maps,\n",
      "4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐\n",
      "ment this architecture later in this chapter.\n",
      "ResNets deeper than that, such as ResNet-152, use slightly different residual units.\n",
      "Instead of two 3 × 3 convolutional layers with (say) 256 feature maps, they use three\n",
      "convolutional layers: first a 1 × 1 convolutional layer with just 64 feature maps (4\n",
      "times less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer\n",
      "with 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature\n",
      "maps (4 times 64) that restores the original depth. ResNet-152 contains three such\n",
      "RUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024\n",
      "maps, and finally 3 RUs with 2,048 maps.\n",
      "Google’s Inception-v416 architecture merged the ideas of GoogLe‐\n",
      "Net and ResNet and achieved close to 3% top-5 error rate on\n",
      "ImageNet classification.\n",
      "Xception\n",
      "Another variant of the GoogLeNet architecture is also worth noting: Xception17\n",
      "(which stands for Extreme Inception) was proposed in 2016 by François Chollet (the\n",
      "CNN Architectures | 459\n",
      "18 This name can sometimes be ambiguous, since spatially separable convolutions are often called “separable\n",
      "convolutions” as well.\n",
      "author of Keras), and it significantly outperformed Inception-v3 on a huge vision task\n",
      "(350 million images and 17,000 classes). Just like Inception-v4, it also merges the\n",
      "ideas of GoogLeNet and ResNet, but it replaces the inception modules with a special\n",
      "type of layer called a depthwise separable convolution  (or separable convolution for\n",
      "short18). These layers had been used before in some CNN architectures, but they were\n",
      "not as central as in the Xception architecture. While a regular convolutional layer\n",
      "uses filters that try to simultaneously capture spatial patterns (e.g., an oval) and cross-\n",
      "channel patterns (e.g., mouth + nose + eyes = face), a separable convolutional layer\n",
      "makes the strong assumption that spatial patterns and cross-channel patterns can be\n",
      "modeled separately (see Figure 14-19). Thus, it is composed of two parts: the first part\n",
      "applies a single spatial filter for each input feature map, then the second part looks\n",
      "exclusively for cross-channel patterns—it is just a regular convolutional layer with 1 ×\n",
      "1 filters.\n",
      "Figure 14-19. Depthwise Separable Convolutional Layer\n",
      "Since separable convolutional layers only have one spatial filter per input channel,\n",
      "you should avoid using them after layers that have too few channels, such as the input\n",
      "layer (granted, that’s what Figure 14-19 represents, but it is just for illustration pur‐\n",
      "poses). For this reason, the Xception architecture starts with 2 regular convolutional\n",
      "layers, but then the rest of the architecture uses only separable convolutions (34 in\n",
      "460 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "19 “Crafting GBD-Net for Object Detection, ” X. Zeng et al. (2016).\n",
      "20 “Squeeze-and-Excitation Networks, ” Jie Hu et al. (2017)\n",
      "all), plus a few max pooling layers and the usual final layers (a global average pooling\n",
      "layer, and a dense output layer).\n",
      "Y ou might wonder why Xception is considered a variant of GoogLeNet, since it con‐\n",
      "tains no inception module at all? Well, as we discussed earlier, an Inception module\n",
      "contains convolutional layers with 1 × 1 filters: these look exclusively for cross-\n",
      "channel patterns. However, the convolution layers that sit on top of them are regular\n",
      "convolutional layers that look both for spatial and cross-channel patterns. So you can\n",
      "think of an Inception module as an intermediate between a regular convolutional\n",
      "layer (which considers spatial patterns and cross-channel patterns jointly) and a sepa‐\n",
      "rable convolutional layer (which considers them separately). In practice, it seems that\n",
      "separable convolutions generally perform better.\n",
      "Separable convolutions use less parameters, less memory and less\n",
      "computations than regular convolutional layers, and in general\n",
      "they even perform better, so you should consider using them by\n",
      "default (except after layers with few channels).\n",
      "The ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐\n",
      "versity of Hong Kong. They used an ensemble of many different techniques, includ‐\n",
      "ing a sophisticated object-detection system called GBD-Net19, to achieve a top-5 error\n",
      "rate below 3%. Although this result is unquestionably impressive, the complexity of\n",
      "the solution contrasted with the simplicity of ResNets. Moreover, one year later\n",
      "another fairly simple architecture performed even better, as we will see now.\n",
      "SENet\n",
      "The winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-\n",
      "Excitation Network (SENet)20. This architecture extends existing architectures such as\n",
      "inception networks or ResNets, and boosts their performance. This allowed SENet to\n",
      "win the competition with an astonishing 2.25% top-5 error rate! The extended ver‐\n",
      "sions of inception networks and ResNet are called SE-Inception and SE-ResNet respec‐\n",
      "tively. The boost comes from the fact that a SENet adds a small neural network, called\n",
      "a SE Block, to every unit in the original architecture (i.e., every inception module or\n",
      "every residual unit), as shown in Figure 14-20.\n",
      "CNN Architectures | 461\n",
      "Figure 14-20. SE-Inception Module (left) and SE-ResNet Unit (right)\n",
      "A SE Block analyzes the output of the unit it is attached to, focusing exclusively on\n",
      "the depth dimension (it does not look for any spatial pattern), and it learns which fea‐\n",
      "tures are usually most active together. It then uses this information to recalibrate the\n",
      "feature maps, as shown in Figure 14-21 . For example, a SE Block may learn that\n",
      "mouths, noses and eyes usually appear together in pictures: if you see a mouth and a\n",
      "nose, you should expect to see eyes as well. So if a SE Block sees a strong activation in\n",
      "the mouth and nose feature maps, but only mild activation in the eye feature map, it\n",
      "will boost the eye feature map (more accurately, it will reduce irrelevant feature\n",
      "maps). If the eyes were somewhat confused with something else, this feature map\n",
      "recalibration will help resolve the ambiguity.\n",
      "462 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "Figure 14-21. An SE Block Performs Feature Map Recalibration\n",
      "A SE Block is composed of just 3 layers: a global average pooling layer, a hidden dense\n",
      "layer using the ReLU activation function, and a dense output layer using the sigmoid\n",
      "activation function (see Figure 14-22):\n",
      "Figure 14-22. SE Block Architecture\n",
      "CNN Architectures | 463\n",
      "As earlier, the global average pooling layer computes the mean activation for each fea‐\n",
      "ture map: for example, if its input contains 256 feature maps, it will output 256 num‐\n",
      "bers representing the overall level of response for each filter. The next layer is where\n",
      "the “squeeze” happens: this layer has much less than 256 neurons, typically 16 times\n",
      "less than the number of feature maps (e.g., 16 neurons), so the 256 numbers get com‐\n",
      "pressed into a small vector (e.g., 16 dimensional). This is a low-dimensional vector\n",
      "representation (i.e., an embedding) of the distribution of feature responses. This bot‐\n",
      "tleneck step forces the SE Block to learn a general representation of the feature com‐\n",
      "binations (we will see this principle in action again when we discuss autoencoders\n",
      "in ???). Finally, the output layer takes the embedding and outputs a recalibration vec‐\n",
      "tor containing one number per feature map (e.g., 256), each between 0 and 1. The\n",
      "feature maps are then multiplied by this recalibration vector, so irrelevant features\n",
      "(with a low recalibration score) get scaled down while relevant features (with a recali‐\n",
      "bration score close to 1) are left alone.\n",
      "Implementing a ResNet-34 CNN Using Keras\n",
      "Most CNN architectures described so far are fairly straightforward to implement\n",
      "(although generally you would load a pretrained network instead, as we will see). To\n",
      "illustrate the process, let’s implement a ResNet-34 from scratch using Keras. First, let’s\n",
      "create a ResidualUnit layer:\n",
      "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\n",
      "                        padding=\"SAME\", use_bias=False)\n",
      "class ResidualUnit(keras.layers.Layer):\n",
      "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.activation = keras.activations.get(activation)\n",
      "        self.main_layers = [\n",
      "            DefaultConv2D(filters, strides=strides),\n",
      "            keras.layers.BatchNormalization(),\n",
      "            self.activation,\n",
      "            DefaultConv2D(filters),\n",
      "            keras.layers.BatchNormalization()]\n",
      "        self.skip_layers = []\n",
      "        if strides > 1:\n",
      "            self.skip_layers = [\n",
      "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
      "                keras.layers.BatchNormalization()]\n",
      "    def call(self, inputs):\n",
      "        Z = inputs\n",
      "        for layer in self.main_layers:\n",
      "            Z = layer(Z)\n",
      "        skip_Z = inputs\n",
      "        for layer in self.skip_layers:\n",
      "464 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "skip_Z = layer(skip_Z)\n",
      "        return self.activation(Z + skip_Z)\n",
      "As you can see, this code matches Figure 14-18 pretty closely. In the constructor, we\n",
      "create all the layers we will need: the main layers are the ones on the right side of the\n",
      "diagram, and the skip layers are the ones on the left (only needed if the stride is\n",
      "greater than 1). Then in the call() method, we simply make the inputs go through\n",
      "the main layers, and the skip layers (if any), then we add both outputs and we apply\n",
      "the activation function.\n",
      "Next, we can build the ResNet-34 simply using a Sequential model, since it is really\n",
      "just a long sequence of layers (we can treat each residual unit as a single layer now\n",
      "that we have the ResidualUnit class):\n",
      "model = keras.models.Sequential()\n",
      "model.add(DefaultConv2D(64, kernel_size=7, strides=2,\n",
      "                        input_shape=[224, 224, 3]))\n",
      "model.add(keras.layers.BatchNormalization())\n",
      "model.add(keras.layers.Activation(\"relu\"))\n",
      "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\"))\n",
      "prev_filters = 64\n",
      "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
      "    strides = 1 if filters == prev_filters else 2\n",
      "    model.add(ResidualUnit(filters, strides=strides))\n",
      "    prev_filters = filters\n",
      "model.add(keras.layers.GlobalAvgPool2D())\n",
      "model.add(keras.layers.Flatten())\n",
      "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
      "The only slightly tricky part in this code is the loop that adds the ResidualUnit layers\n",
      "to the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs\n",
      "have 128 filters, and so on. We then set the strides to 1 when the number of filters is\n",
      "the same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit,\n",
      "and finally we update prev_filters.\n",
      "It is quite amazing that in less than 40 lines of code, we can build the model that won\n",
      "the ILSVRC 2015 challenge! It demonstrates both the elegance of the ResNet model,\n",
      "and the expressiveness of the Keras API. Implementing the other CNN architectures\n",
      "is not much harder. However, Keras comes with several of these architectures built in,\n",
      "so why not use them instead?\n",
      "Using Pretrained Models From Keras\n",
      "In general, you won’t have to implement standard models like GoogLeNet or ResNet\n",
      "manually, since pretrained networks are readily available with a single line of code, in\n",
      "the keras.applications package. For example:\n",
      "model = keras.applications.resnet50.ResNet50(weights=\"imagenet\")\n",
      "Using Pretrained Models From Keras | 465\n",
      "21 In the ImageNet dataset, each image is associated to a word in the WordNet dataset: the class ID is just a\n",
      "WordNet ID.\n",
      "That’s all! This will create a ResNet-50 model and download weights pretrained on\n",
      "the ImageNet dataset. To use it, you first need to ensure that the images have the right\n",
      "size. A ResNet-50 model expects 224 × 224 images (other models may expect other\n",
      "sizes, such as 299 × 299), so let’s use TensorFlow’s tf.image.resize() function to\n",
      "resize the images we loaded earlier:\n",
      "images_resized = tf.image.resize(images, [224, 224])\n",
      "The tf.image.resize() will not preserve the aspect ratio. If this is\n",
      "a problem, you can try cropping the images to the appropriate\n",
      "aspect ratio before resizing. Both operations can be done in one\n",
      "shot with tf.image.crop_and_resize().\n",
      "The pretrained models assume that the images are preprocessed in a specific way. In\n",
      "some cases they may expect the inputs to be scaled from 0 to 1, or -1 to 1, and so on.\n",
      "Each model provides a preprocess_input() function that you can use to preprocess\n",
      "your images. These functions assume that the pixel values range from 0 to 255, so we\n",
      "must multiply them by 255 (since earlier we scaled them to the 0–1 range):\n",
      "inputs = keras.applications.resnet50.preprocess_input(images_resized * 255)\n",
      "Now we can use the pretrained model to make predictions:\n",
      "Y_proba = model.predict(inputs)\n",
      "As usual, the output Y_proba is a matrix with one row per image and one column per\n",
      "class (in this case, there are 1,000 classes). If you want to display the top K predic‐\n",
      "tions, including the class name and the estimated probability of each predicted class,\n",
      "you can use the decode_predictions() function. For each image, it returns an array\n",
      "containing the top K predictions, where each prediction is represented as an array\n",
      "containing the class identifier21, its name and the corresponding confidence score:\n",
      "top_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
      "for image_index in range(len(images)):\n",
      "    print(\"Image #{}\".format(image_index))\n",
      "    for class_id, name, y_proba in top_K[image_index]:\n",
      "        print(\"  {} - {:12s} {:.2f}%\".format(class_id, name, y_proba * 100))\n",
      "    print()\n",
      "The output looks like this:\n",
      "Image #0\n",
      "  n03877845 - palace       42.87%\n",
      "  n02825657 - bell_cote    40.57%\n",
      "  n03781244 - monastery    14.56%\n",
      "466 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "Image #1\n",
      "  n04522168 - vase         46.83%\n",
      "  n07930864 - cup          7.78%\n",
      "  n11939491 - daisy        4.87%\n",
      "The correct classes (monastery and daisy) appear in the top 3 results for both images.\n",
      "That’s pretty good considering that the model had to choose among 1,000 classes.\n",
      "As you can see, it is very easy to create a pretty good image classifier using a pre‐\n",
      "trained model. Other vision models are available in keras.applications, including\n",
      "several ResNet variants, GoogLeNet variants like InceptionV3 and Xception,\n",
      "VGGNet variants, MobileNet and MobileNetV2 (lightweight models for use in\n",
      "mobile applications), and more.\n",
      "But what if you want to use an image classifier for classes of images that are not part\n",
      "of ImageNet? In that case, you may still benefit from the pretrained models to per‐\n",
      "form transfer learning.\n",
      "Pretrained Models for Transfer Learning\n",
      "If you want to build an image classifier, but you do not have enough training data,\n",
      "then it is often a good idea to reuse the lower layers of a pretrained model, as we dis‐\n",
      "cussed in Chapter 11. For example, let’s train a model to classify pictures of flowers,\n",
      "reusing a pretrained Xception model. First, let’s load the dataset using TensorFlow\n",
      "Datasets (see Chapter 13):\n",
      "import tensorflow_datasets as tfds\n",
      "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
      "dataset_size = info.splits[\"train\"].num_examples # 3670\n",
      "class_names = info.features[\"label\"].names # [\"dandelion\", \"daisy\", ...]\n",
      "n_classes = info.features[\"label\"].num_classes # 5\n",
      "Note that you can get information about the dataset by setting with_info=True. Here,\n",
      "we get the dataset size and the names of the classes. Unfortunately, there is only a\n",
      "\"train\" dataset, no test set or validation set, so we need to split the training set. The\n",
      "TF Datasets project provides an API for this. For example, let’s take the first 10% of\n",
      "the dataset for testing, the next 15% for validation, and the remaining 75% for train‐\n",
      "ing:\n",
      "test_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 15, 75])\n",
      "test_set = tfds.load(\"tf_flowers\", split=test_split, as_supervised=True)\n",
      "valid_set = tfds.load(\"tf_flowers\", split=valid_split, as_supervised=True)\n",
      "train_set = tfds.load(\"tf_flowers\", split=train_split, as_supervised=True)\n",
      "Pretrained Models for Transfer Learning | 467\n",
      "Next we must preprocess the images. The CNN expects 224 × 224 images, so we need\n",
      "to resize them. We also need to run the image through Xception’s prepro\n",
      "cess_input() function:\n",
      "def preprocess(image, label):\n",
      "    resized_image = tf.image.resize(image, [224, 224])\n",
      "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
      "    return final_image, label\n",
      "Let’s apply this preprocessing function to all 3 datasets, and let’s also shuffle & repeat\n",
      "the training set, and add batching & prefetching to all datasets:\n",
      "batch_size = 32\n",
      "train_set = train_set.shuffle(1000).repeat()\n",
      "train_set = train_set.map(preprocess).batch(batch_size).prefetch(1)\n",
      "valid_set = valid_set.map(preprocess).batch(batch_size).prefetch(1)\n",
      "test_set = test_set.map(preprocess).batch(batch_size).prefetch(1)\n",
      "If you want to perform some data augmentation, you can just change the preprocess‐\n",
      "ing function for the training set, adding some random transformations to the training\n",
      "images. For example, use tf.image.random_crop() to randomly crop the images, use\n",
      "tf.image.random_flip_left_right() to randomly flip the images horizontally, and\n",
      "so on (see the notebook for an example).\n",
      "Next let’s load an Xception model, pretrained on ImageNet. We exclude the top of the\n",
      "network (by setting include_top=False): this excludes the global average pooling\n",
      "layer and the dense output layer. We then add our own global average pooling layer,\n",
      "based on the output of the base model, followed by a dense output layer with 1 unit\n",
      "per class, using the softmax activation function. Finally, we create the Keras Model:\n",
      "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
      "                                                  include_top=False)\n",
      "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
      "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
      "model = keras.models.Model(inputs=base_model.input, outputs=output)\n",
      "As explained in Chapter 11, it’s usually a good idea to freeze the weights of the pre‐\n",
      "trained layers, at least at the beginning of training:\n",
      "for layer in base_model.layers:\n",
      "    layer.trainable = False\n",
      "Since our model uses the base model’s layers directly, rather than\n",
      "the base_model object itself, setting base_model.trainable=False\n",
      "would have no effect.\n",
      "Finally, we can compile the model and start training:\n",
      "468 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n",
      "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
      "              metrics=[\"accuracy\"])\n",
      "history = model.fit(train_set,\n",
      "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
      "                    validation_data=valid_set,\n",
      "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
      "                    epochs=5)\n",
      "This will be very slow, unless you have a GPU. If you do not, then\n",
      "you should run this chapter’s notebook in Colab, using a GPU run‐\n",
      "time (it’s free!). See the instructions at https://github.com/ageron/\n",
      "handson-ml2.\n",
      "After training the model for a few epochs, its validation accuracy should reach about\n",
      "75-80%, and stop making much progress. This means that the top layers are now\n",
      "pretty well trained, so we are ready to unfreeze all layers (or you could try unfreezing\n",
      "just the top ones), and continue training (don’t forget to compile the model when you\n",
      "freeze or unfreeze layers). This time we use a much lower learning rate to avoid dam‐\n",
      "aging the pretrained weights:\n",
      "for layer in base_model.layers:\n",
      "    layer.trainable = True\n",
      "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)\n",
      "model.compile(...)\n",
      "history = model.fit(...)\n",
      "It will take a while, but this model should reach around 95% accuracy on the test set.\n",
      "With that, you can start training amazing image classifiers! But there’s more to com‐\n",
      "puter vision than just classification. For example, what if you also want to know where\n",
      "the flower is in the picture? Let’s look at this now.\n",
      "Classification and Localization\n",
      "Localizing an object in a picture can be expressed as a regression task, as discussed in\n",
      "Chapter 10: to predict a bounding box around the object, a common approach is to\n",
      "predict the horizontal and vertical coordinates of the object’s center, as well as its\n",
      "height and width. This means we have 4 numbers to predict. It does not require much\n",
      "change to the model, we just need to add a second dense output layer with 4 units\n",
      "(typically on top of the global average pooling layer), and it can be trained using the\n",
      "MSE loss:\n",
      "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
      "                                                  include_top=False)\n",
      "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
      "class_output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
      "Classification and Localization | 469\n",
      "22 “Crowdsourcing in Computer Vision, ” A. Kovashka et al. (2016).\n",
      "loc_output = keras.layers.Dense(4)(avg)\n",
      "model = keras.models.Model(inputs=base_model.input,\n",
      "                           outputs=[class_output, loc_output])\n",
      "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
      "              loss_weights=[0.8, 0.2], # depends on what you care most about\n",
      "              optimizer=optimizer, metrics=[\"accuracy\"])\n",
      "But now we have a problem: the flowers dataset does not have bounding boxes\n",
      "around the flowers. So we need to add them ourselves. This is often one of the hard‐\n",
      "est and most costly part of a Machine Learning project: getting the labels. It’s a good\n",
      "idea to spend time looking for the right tools. To annotate images with bounding\n",
      "boxes, you may want to use an open source image labeling tool like VGG Image\n",
      "Annotator, LabelImg, OpenLabeler or ImgLab, or perhaps a commercial tool like\n",
      "LabelBox or Supervisely. Y ou may also want to consider crowdsourcing platforms\n",
      "such as Amazon Mechanical Turk or CrowdFlower if you have a very large number of\n",
      "images to annotate. However, it is quite a lot of work to setup a crowdsourcing plat‐\n",
      "form, prepare the form to be sent to the workers, to supervise them and ensure the\n",
      "quality of the bounding boxes they produce is good, so make sure it is worth the\n",
      "effort: if there are just a few thousand images to label, and you don’t plan to do this\n",
      "frequently, it may be preferable to do it yourself. Adriana Kovashka et al. wrote a very\n",
      "practical paper22 about crowdsourcing in Computer Vision, I recommend you check\n",
      "it out, even if you do not plan to use crowdsourcing.\n",
      "So let’s suppose you obtained the bounding boxes for every image in the flowers data‐\n",
      "set (for now we will assume there is a single bounding box per image), you then need\n",
      "to create a dataset whose items will be batches of preprocessed images along with\n",
      "their class labels and their bounding boxes. Each item should be a tuple of the form:\n",
      "(images, (class_labels, bounding_boxes)) . Then you are ready to train your\n",
      "model!\n",
      "The bounding boxes should be normalized so that the horizontal\n",
      "and vertical coordinates, as well as the height and width all range\n",
      "from 0 to 1. Also, it is common to predict the square root of the\n",
      "height and width rather than the height and width directly: this\n",
      "way, a 10 pixel error for a large bounding box will not be penalized\n",
      "as much as a 10 pixel error for a small bounding box.\n",
      "The MSE often works fairly well as a cost function to train the model, but it is not a\n",
      "great metric to evaluate how well the model can predict bounding boxes. The most\n",
      "common metric for this is the Intersection over Union (IoU): it is the area of overlap\n",
      "between the predicted bounding box and the target bounding box, divided by the\n",
      "470 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "area of their union (see Figure 14-23 ). In tf.keras, it is implemented by the\n",
      "tf.keras.metrics.MeanIoU class.\n",
      "Figure 14-23. Intersection over Union (IoU) Metric for Bounding Boxes\n",
      "Classifying and localizing a single object is nice, but what if the images contain multi‐\n",
      "ple objects (as is often the case in the flowers dataset)?\n",
      "Object Detection\n",
      "The task of classifying and localizing multiple objects in an image is called object\n",
      "detection. Until a few years ago, a common approach was to take a CNN that was\n",
      "trained to classify and locate a single object, then slide it across the image, as shown\n",
      "in Figure 14-24. In this example, the image was chopped into a 6 × 8 grid, and we\n",
      "show a CNN (the thick black rectangle) sliding across all 3 × 3 regions. When the\n",
      "CNN was looking at the top left of the image, it detected part of the left-most rose,\n",
      "and then it detected that same rose again when it was first shifted one step to the\n",
      "right. At the next step, it started detecting part of the top-most rose, and then it detec‐\n",
      "ted it again once it was shifted one more step to the right. Y ou would then continue to\n",
      "slide the CNN through the whole image, looking at all 3 × 3 regions. Moreover, since\n",
      "objects can have varying sizes, you would also slide the CNN across regions of differ‐\n",
      "ent sizes. For example, once you are done with the 3 × 3 regions, you might want to\n",
      "slide the CNN across all 4 × 4 regions as well.\n",
      "Object Detection | 471\n",
      "Figure 14-24. Detecting Multiple Objects by Sliding a CNN Across the Image\n",
      "This technique is fairly straightforward, but as you can see it will detect the same\n",
      "object multiple times, at slightly different positions. Some post-processing will then\n",
      "be needed to get rid of all the unnecessary bounding boxes. A common approach for\n",
      "this is called non-max suppression:\n",
      "• First, you need to add an extra objectness output to your CNN, to estimate the\n",
      "probability that a flower is indeed present in the image (alternatively, you could\n",
      "add a “no-flower” class, but this usually does not work as well). It must use the\n",
      "sigmoid activation function and you can train it using the \"binary_crossen\n",
      "tropy\" loss. Then just get rid of all the bounding boxes for which the objectness\n",
      "score is below some threshold: this will drop all the bounding boxes that don’t\n",
      "actually contain a flower.\n",
      "• Second, find the bounding box with the highest objectness score, and get rid of\n",
      "all the other bounding boxes that overlap a lot with it (e.g., with an IoU greater\n",
      "than 60%). For example, in Figure 14-24, the bounding box with the max object‐\n",
      "ness score is the thick bounding box over the top-most rose (the objectness score\n",
      "is represented by the thickness of the bounding boxes). The other bounding box\n",
      "over that same rose overlaps a lot with the max bounding box, so we will get rid\n",
      "of it.\n",
      "472 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "23 “Fully Convolutional Networks for Semantic Segmentation, ” J. Long, E. Shelhamer, T. Darrell (2015).\n",
      "24 There is one small exception: a convolutional layer using V ALID padding will complain if the input size is\n",
      "smaller than the kernel size.\n",
      "• Third, repeat step two until there are no more bounding boxes to get rid of.\n",
      "This simple approach to object detection works pretty well, but it requires running\n",
      "the CNN many times, so it is quite slow. Fortunately, there is a much faster way to\n",
      "slide a CNN across an image: using a Fully Convolutional Network.\n",
      "Fully Convolutional Networks (FCNs)\n",
      "The idea of FCNs was first introduced in a 2015 paper23 by Jonathan Long et al., for\n",
      "semantic segmentation (the task of classifying every pixel in an image according to\n",
      "the class of the object it belongs to). They pointed out that you could replace the\n",
      "dense layers at the top of a CNN by convolutional layers. To understand this, let’s look\n",
      "at an example: suppose a dense layer with 200 neurons sits on top of a convolutional\n",
      "layer that outputs 100 feature maps, each of size 7 × 7 (this is the feature map size, not\n",
      "the kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7 activa‐\n",
      "tions from the convolutional layer (plus a bias term). Now let’s see what happens if we\n",
      "replace the dense layer with a convolution layer using 200 filters, each 7 × 7, and with\n",
      "V ALID padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel\n",
      "is exactly the size of the input feature maps and we are using V ALID padding). In\n",
      "other words, it will output 200 numbers, just like the dense layer did, and if you look\n",
      "closely at the computations performed by a convolutional layer, you will notice that\n",
      "these numbers will be precisely the same as the dense layer produced. The only differ‐\n",
      "ence is that the dense layer’s output was a tensor of shape [batch size, 200] while the\n",
      "convolutional layer will output a tensor of shape [batch size, 1, 1, 200].\n",
      "To convert a dense layer to a convolutional layer, the number of fil‐\n",
      "ters in the convolutional layer must be equal to the number of units\n",
      "in the dense layer, the filter size must be equal to the size of the\n",
      "input feature maps, and you must use V ALID padding. The stride\n",
      "may be set to 1 or more, as we will see shortly.\n",
      "Why is this important? Well, while a dense layer expects a specific input size (since it\n",
      "has one weight per input feature), a convolutional layer will happily process images of\n",
      "any size24 (however, it does expect its inputs to have a specific number of channels,\n",
      "since each kernel contains a different set of weights for each input channel). Since an\n",
      "FCN contains only convolutional layers (and pooling layers, which have the same\n",
      "property), it can be trained and executed on images of any size!\n",
      "Object Detection | 473\n",
      "25 This assumes we used only SAME padding in the network: indeed, V ALID padding would reduce the size of\n",
      "the feature maps. Moreover, 448 can be neatly divided by 2 several times until we reach 7, without any round‐\n",
      "ing error. If any layer uses a different stride than 1 or 2, then there may be some rounding error, so again the\n",
      "feature maps may end up being smaller.\n",
      "For example, suppose we already trained a CNN for flower classification and localiza‐\n",
      "tion. It was trained on 224 × 224 images and it outputs 10 numbers: outputs 0 to 4 are\n",
      "sent through the softmax activation function, and this gives the class probabilities\n",
      "(one per class); output 5 is sent through the logistic activation function, and this gives\n",
      "the objectness score; outputs 6 to 9 do not use any activation function, and they rep‐\n",
      "resent the bounding box’s center coordinates, and its height and width. We can now\n",
      "convert its dense layers to convolutional layers. In fact, we don’t even need to retrain\n",
      "it, we can just copy the weights from the dense layers to the convolutional layers!\n",
      "Alternatively, we could have converted the CNN into an FCN before training.\n",
      "Now suppose the last convolutional layer before the output layer (also called the bot‐\n",
      "tleneck layer) outputs 7 × 7 feature maps when the network is fed a 224 × 224 image\n",
      "(see the left side of Figure 14-25). If we feed the FCN a 448 × 448 image (see the right\n",
      "side of Figure 14-25 ), the bottleneck layer will now output 14 × 14 feature maps. 25\n",
      "Since the dense output layer was replaced by a convolutional layer using 10 filters of\n",
      "size 7 × 7, V ALID padding and stride 1, the output will be composed of 10 features\n",
      "maps, each of size 8 × 8 (since 14 - 7 + 1 = 8). In other words, the FCN will process\n",
      "the whole image only once and it will output an 8 × 8 grid where each cell contains 10\n",
      "numbers (5 class probabilities, 1 objectness score and 4 bounding box coordinates).\n",
      "It’s exactly like taking the original CNN and sliding it across the image using 8 steps\n",
      "per row and 8 steps per column: to visualize this, imagine chopping the original\n",
      "image into a 14 × 14 grid, then sliding a 7 × 7 window across this grid: there will be 8\n",
      "× 8 = 64 possible locations for the window, hence 8 × 8 predictions. However, the\n",
      "FCN approach is much more efficient, since the network only looks at the image\n",
      "once. In fact, You Only Look Once (YOLO) is the name of a very popular object detec‐\n",
      "tion architecture!\n",
      "474 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "26 “Y ou Only Look Once: Unified, Real-Time Object Detection, ” J. Redmon, S. Divvala, R. Girshick, A. Farhadi\n",
      "(2015).\n",
      "27 “YOLO9000: Better, Faster, Stronger, ” J. Redmon, A. Farhadi (2016).\n",
      "28 “YOLOv3: An Incremental Improvement, ” J. Redmon, A. Farhadi (2018).\n",
      "Figure 14-25. A Fully Convolutional Network Processing a Small Image (left) and a\n",
      "Large One (right)\n",
      "You Only Look Once (YOLO)\n",
      "YOLO is an extremely fast and accurate object detection architecture proposed by\n",
      "Joseph Redmon et al. in a 2015 paper 26, and subsequently improved in 2016 27\n",
      "(YOLOv2) and in 201828 (YOLOv3). It is so fast that it can run in realtime on a video\n",
      "(check out this nice demo).\n",
      "YOLOv3’s architecture is quite similar to the one we just discussed, but with a few\n",
      "important differences:\n",
      "Object Detection | 475\n",
      "• First, it outputs 5 bounding boxes for each grid cell (instead of just 1), and each\n",
      "bounding box comes with an objectness score. It also outputs 20 class probabili‐\n",
      "ties per grid cell, as it was trained on the PASCAL VOC dataset, which contains\n",
      "20 classes. That’s a total of 45 numbers per grid cell (5 * 4 bounding box coordi‐\n",
      "nates, plus 5 objectness scores, plus 20 class probabilities).\n",
      "• Second, instead of predicting the absolute coordinates of the bounding box cen‐\n",
      "ters, YOLOv3 predicts an offset relative to the coordinates of the grid cell, where\n",
      "(0, 0) means the top left of that cell, and (1, 1) means the bottom right. For each\n",
      "grid cell, YOLOv3 is trained to predict only bounding boxes whose center lies in\n",
      "that cell (but the bounding box itself generally extends well beyond the grid cell).\n",
      "YOLOv3 applies the logistic activation function to the bounding box coordinates\n",
      "to ensure they remain in the 0 to 1 range.\n",
      "• Third, before training the neural net, YOLOv3 finds 5 representative bounding\n",
      "box dimensions, called anchor boxes  (or bounding box priors ): it does this by\n",
      "applying the K-Means algorithm (see ???) to the height and width of the training\n",
      "set bounding boxes. For example, if the training images contain many pedes‐\n",
      "trians, then one of the anchor boxes will likely have the dimensions of a typical\n",
      "pedestrian. Then when the neural net predicts 5 bounding boxes per grid cell, it\n",
      "actually predicts how much to rescale each of the anchor boxes. For example,\n",
      "suppose one anchor box is 100 pixels tall and 50 pixels wide, and the network\n",
      "predicts, say, a vertical rescaling factor of 1.5 and a horizontal rescaling of 0.9 (for\n",
      "one of the grid cells), this will result in a predicted bounding box of size 150 × 45\n",
      "pixels. To be more precise, for each grid cell and each anchor box, the network\n",
      "predicts the log of the vertical and horizontal rescaling factors. Having these pri‐\n",
      "ors makes the network more likely to predict bounding boxes of the appropriate\n",
      "dimensions, and it also speeds up training since it will more quickly learn what\n",
      "reasonable bounding boxes look like.\n",
      "• Fourth, the network is trained using images of different scales: every few batches\n",
      "during training, the network randomly chooses a new image dimension (from\n",
      "330 × 330 to 608 × 608 pixels). This allows the network to learn to detect objects\n",
      "at different scales. Moreover, it makes it possible to use YOLOv3 at different\n",
      "scales: the smaller scale will be less accurate but faster than the larger scale, so\n",
      "you can choose the right tradeoff for your use case.\n",
      "There are a few more innovations you might be interested in, such as the use of skip\n",
      "connections to recover some of the spatial resolution that is lost in the CNN (we will\n",
      "discuss this shortly when we look at semantic segmentation). Moreover, in the 2016\n",
      "paper, the authors introduce the YOLO9000 model that uses hierarchical classifica‐\n",
      "tion: the model predicts a probability for each node in a visual hierarchy called Word‐\n",
      "Tree. This makes it possible for the network to predict with high confidence that an\n",
      "image represents, say, a dog, even though it is unsure what specific type of dog it is.\n",
      "476 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "So I encourage you to go ahead and read all three papers: they are quite pleasant to\n",
      "read, and it is an excellent example of how Deep Learning systems can be incremen‐\n",
      "tally improved.\n",
      "Mean Average Precision (mAP)\n",
      "A very common metric used in object detection tasks is the mean Average Precision\n",
      "(mAP). “Mean Average” sounds a bit redundant, doesn’t it? To understand this met‐\n",
      "ric, let’s go back to two classification metrics we discussed in Chapter 3: precision and\n",
      "recall. Remember the tradeoff: the higher the recall, the lower the precision. Y ou can\n",
      "visualize this in a Precision/Recall curve (see Figure 3-5 ). To summarize this curve\n",
      "into a single number, we could compute its Area Under the Curve (AUC). But note\n",
      "that the Precision/Recall curve may contain a few sections where precision actually\n",
      "goes up when recall increases, especially at low recall values (you can see this at the\n",
      "top left of Figure 3-5). This is one of the motivations for the mAP metric.\n",
      "Suppose the classifier has a 90% precision at 10% recall, but a 96% precision at 20%\n",
      "recall: there’s really no tradeoff here: it simply makes more sense to use the classifier\n",
      "at 20% recall rather than at 10% recall, as you will get both higher recall and higher\n",
      "precision. So instead of looking at the precision at 10% recall, we should really be\n",
      "looking at the maximum precision that the classifier can offer with at least 10% recall.\n",
      "It would be 96%, not 90%. So one way to get a fair idea of the model’s performance is\n",
      "to compute the maximum precision you can get with at least 0% recall, then 10%\n",
      "recall, 20%, and so on up to 100%, and then calculate the mean of these maximum\n",
      "precisions. This is called the Average Precision (AP) metric. Now when there are more\n",
      "than 2 classes, we can compute the AP for each class, and then compute the mean AP\n",
      "(mAP). That’s it!\n",
      "However, in an object detection systems, there is an additional level of complexity:\n",
      "what if the system detected the correct class, but at the wrong location (i.e., the\n",
      "bounding box is completely off)? Surely we should not count this as a positive predic‐\n",
      "tion. So one approach is to define an IOU threshold: for example, we may consider\n",
      "that a prediction is correct only if the IOU is greater than, say, 0.5, and the predicted\n",
      "class is correct. The corresponding mAP is generally noted mAP@0.5 (or mAP@50%,\n",
      "or sometimes just AP 50). In some competitions (such as the Pascal VOC challenge),\n",
      "this is what is done. In others (such as the COCO competition), the mAP is computed\n",
      "for different IOU thresholds (0.50, 0.55, 0.60, …, 0.95), and the final metric is the\n",
      "mean of all these mAPs (noted AP@[.50:.95] or AP@[.50:0.05:.95]). Y es, that’s a mean\n",
      "mean average.\n",
      "Several YOLO implementations built using TensorFlow are available on github, some\n",
      "with pretrained weights. At the time of writing, they are based on TensorFlow 1, but\n",
      "by the time you read this, TF 2 implementations will certainly be available. Moreover,\n",
      "other object detection models are available in the TensorFlow Models project, many\n",
      "Object Detection | 477\n",
      "29 “SSD: Single Shot MultiBox Detector, ” Wei Liu et al. (2015).\n",
      "30 “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, ” Shaoqing Ren et al.\n",
      "(2015).\n",
      "with pretrained weights, and some have even been ported to TF Hub, making them\n",
      "extremely easy to use, such as SSD29 and Faster-RCNN.30, which are both quite popu‐\n",
      "lar. SSD is also a “single shot” detection model, quite similar to YOLO, while Faster R-\n",
      "CNN is more complex: the image first goes through a CNN, and the output is passed\n",
      "to a Region Proposal Network (RPN) which proposes bounding boxes that are most\n",
      "likely to contain an object, and a classifier is run for each bounding box, based on the\n",
      "cropped output of the CNN.\n",
      "The choice of detection system depends on many factors: speed, accuracy, available\n",
      "pretrained models, training time, complexity, etc. The papers contain tables of met‐\n",
      "rics, but there is quite a lot of variability in the testing environments, and the technol‐\n",
      "ogies evolve so fast that it is difficulty to make a fair comparison that will be useful for\n",
      "most people and remain valid for more than a few months.\n",
      "Great! So we can locate objects by drawing bounding boxes around them. But per‐\n",
      "haps you might want to be a bit more precise. Let’s see how to go down to the pixel\n",
      "level.\n",
      "Semantic Segmentation\n",
      "In semantic segmentation, each pixel is classified according to the class of the object it\n",
      "belongs to (e.g., road, car, pedestrian, building, etc.), as shown in Figure 14-26. Note\n",
      "that different objects of the same class are not distinguished. For example, all the bicy‐\n",
      "cles on the right side of the segmented image end up as one big lump of pixels. The\n",
      "main difficulty in this task is that when images go through a regular CNN, they grad‐\n",
      "ually lose their spatial resolution (due to the layers with strides greater than 1): so a\n",
      "regular CNN may end up knowing that there’s a person in the image, somewhere in\n",
      "the bottom left of the image, but it will not be much more precise than that.\n",
      "478 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "31 This type of layer is sometimes referred to as a deconvolution layer, but it does not perform what mathemati‐\n",
      "cians call a deconvolution, so this name should be avoided.\n",
      "Figure 14-26. Semantic segmentation\n",
      "Just like for object detection, there are many different approaches to tackle this prob‐\n",
      "lem, some quite complex. However, a fairly simple solution was proposed in the 2015\n",
      "paper by Jonathan Long et al. we discussed earlier. They start by taking a pretrained\n",
      "CNN and turning into an FCN, as discussed earlier. The CNN applies a stride of 32 to\n",
      "the input image overall (i.e., if you add up all the strides greater than 1), meaning the\n",
      "last layer outputs feature maps that are 32 times smaller than the input image. This is\n",
      "clearly too coarse, so they add a single upsampling layer that multiplies the resolution\n",
      "by 32. There are several solutions available for upsampling (increasing the size of an\n",
      "image), such as bilinear interpolation, but it only works reasonably well up to ×4 or\n",
      "×8. Instead, they used a transposed convolutional layer :31 it is equivalent to first\n",
      "stretching the image by inserting empty rows and columns (full of zeros), then per‐\n",
      "forming a regular convolution (see Figure 14-27). Alternatively, some people prefer to\n",
      "think of it as a regular convolutional layer that uses fractional strides (e.g., 1/2 in\n",
      "Figure 14-27). The transposed convolutional layer can be initialized to perform some‐\n",
      "thing close to linear interpolation, but since it is a trainable layer, it will learn to do\n",
      "better during training.\n",
      "Semantic Segmentation | 479\n",
      "Figure 14-27. Upsampling Using a Transpose Convolutional Layer\n",
      "In a transposed convolution layer, the stride defines how much the\n",
      "input will be stretched, not the size of the filter steps, so the larger\n",
      "the stride, the larger the output (unlike for convolutional layers or\n",
      "pooling layers).\n",
      "TensorFlow Convolution Operations\n",
      "TensorFlow also offers a few other kinds of convolutional layers:\n",
      "• keras.layers.Conv1D creates a convolutional layer for 1D inputs, such as time\n",
      "series or text (sequences of letters or words), as we will see in ???.\n",
      "• keras.layers.Conv3D creates a convolutional layer for 3D inputs, such as 3D\n",
      "PET scan.\n",
      "• Setting the dilation_rate hyperparameter of any convolutional layer to a value\n",
      "of 2 or more creates an à-trous convolutional layer (“à trous” is French for “with\n",
      "holes”). This is equivalent to using a regular convolutional layer with a filter dila‐\n",
      "ted by inserting rows and columns of zeros (i.e., holes). For example, a 1 × 3 filter\n",
      "equal to [[1,2,3]] may be dilated with a dilation rate of 4, resulting in a dilated\n",
      "filter [[1, 0, 0, 0, 2, 0, 0, 0, 3]] . This allows the convolutional layer to\n",
      "480 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "have a larger receptive field at no computational price and using no extra param‐\n",
      "eters.\n",
      "• tf.nn.depthwise_conv2d() can be used to create a depthwise convolutional layer\n",
      "(but you need to create the variables yourself). It applies every filter to every\n",
      "individual input channel independently. Thus, if there are fn filters and fn′ input\n",
      "channels, then this will output fn × fn′ feature maps.\n",
      "This solution is okay, but still too imprecise. To do better, the authors added skip con‐\n",
      "nections from lower layers: for example, they upsampled the output image by a factor\n",
      "of 2 (instead of 32), and they added the output of a lower layer that had this double\n",
      "resolution. Then they upsampled the result by a factor of 16, leading to a total upsam‐\n",
      "pling factor of 32 (see Figure 14-28 ). This recovered some of the spatial resolution\n",
      "that was lost in earlier pooling layers. In their best architecture, they used a second\n",
      "similar skip connection to recover even finer details from an even lower layer: in\n",
      "short, the output of the original CNN goes through the following extra steps: upscale\n",
      "×2, add the output of a lower layer (of the appropriate scale), upscale ×2, add the out‐\n",
      "put of an even lower layer, and finally upscale ×8. It is even possible to scale up\n",
      "beyond the size of the original image: this can be used to increase the resolution of an\n",
      "image, which is a technique called super-resolution.\n",
      "Figure 14-28. Skip layers recover some spatial resolution from lower layers\n",
      "Once again, many github repositories provide TensorFlow implementations of\n",
      "semantic segmentation (TensorFlow 1 for now), and you will even find a pretrained\n",
      "instance segmentation model in the TensorFlow Models project. Instance segmenta‐\n",
      "tion is similar to semantic segmentation, but instead of merging all objects of the\n",
      "same class into one big lump, each object is distinguished from the others (e.g., it\n",
      "identifies each individual bicycle). At the present, they provide multiple implementa‐\n",
      "tions of the Mask R-CNN  architecture, which was proposed in a 2017 paper : it\n",
      "extends the Faster R-CNN model by additionally producing a pixel-mask for each\n",
      "bounding box. So not only do you get a bounding box around each object, with a set\n",
      "of estimated class probabilities, you also get a pixel mask that locates pixels in the\n",
      "bounding box that belong to the object.\n",
      "Semantic Segmentation | 481\n",
      "32 “Matrix Capsules with EM Routing, ” G. Hinton, S. Sabour, N. Frosst (2018).\n",
      "As you can see, the field of Deep Computer Vision is vast and moving fast, with all\n",
      "sorts of architectures popping out every year, all based on Convolutional Neural Net‐\n",
      "works. The progress made in just a few years has been astounding, and researchers\n",
      "are now focusing on harder and harder problems, such as adversarial learning (which\n",
      "attempts to make the network more resistant to images designed to fool it), explaina‐\n",
      "bility (understanding why the network makes a specific classification), realistic image\n",
      "generation (which we will come back to in ???), single-shot learning (a system that can\n",
      "recognize an object after it has seen it just once), and much more. Some even explore\n",
      "completely novel architectures, such as Geoffrey Hinton’s capsule networks32 (I pre‐\n",
      "sented them in a couple videos, with the corresponding code in a notebook). Now on\n",
      "to the next chapter, where we will look at how to process sequential data such as time\n",
      "series using Recurrent Neural Networks and Convolutional Neural Networks.\n",
      "Exercises\n",
      "1. What are the advantages of a CNN over a fully connected DNN for image classi‐\n",
      "fication?\n",
      "2. Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,\n",
      "a stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the\n",
      "middle one outputs 200, and the top one outputs 400. The input images are RGB\n",
      "images of 200 × 300 pixels. What is the total number of parameters in the CNN?\n",
      "If we are using 32-bit floats, at least how much RAM will this network require\n",
      "when making a prediction for a single instance? What about when training on a\n",
      "mini-batch of 50 images?\n",
      "3. If your GPU runs out of memory while training a CNN, what are five things you\n",
      "could try to solve the problem?\n",
      "4. Why would you want to add a max pooling layer rather than a convolutional\n",
      "layer with the same stride?\n",
      "5. When would you want to add a local response normalization layer?\n",
      "6. Can you name the main innovations in AlexNet, compared to LeNet-5? What\n",
      "about the main innovations in GoogLeNet, ResNet, SENet and Xception?\n",
      "7. What is a Fully Convolutional Network? How can you convert a dense layer into\n",
      "a convolutional layer?\n",
      "8. What is the main technical difficulty of semantic segmentation?\n",
      "9. Build your own CNN from scratch and try to achieve the highest possible accu‐\n",
      "racy on MNIST.\n",
      "482 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "10. Use transfer learning for large image classification.\n",
      "a. Create a training set containing at least 100 images per class. For example, you\n",
      "could classify your own pictures based on the location (beach, mountain, city,\n",
      "etc.), or alternatively you can just use an existing dataset (e.g., from Tensor‐\n",
      "Flow Datasets).\n",
      "b. Split it into a training set, a validation set and a test set.\n",
      "c. Build the input pipeline, including the appropriate preprocessing operations,\n",
      "and optionally add data augmentation.\n",
      "d. Fine-tune a pretrained model on this dataset.\n",
      "11. Go through TensorFlow’s DeepDream tutorial. It is a fun way to familiarize your‐\n",
      "self with various ways of visualizing the patterns learned by a CNN, and to gener‐\n",
      "ate art using Deep Learning.\n",
      "Solutions to these exercises are available in ???.\n",
      "Exercises | 483\n",
      "About the Author\n",
      "Aurélien Géron is a Machine Learning consultant. A former Googler, he led the Y ou‐\n",
      "Tube video classification team from 2013 to 2016. He was also a founder and CTO of\n",
      "Wifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\n",
      "of Polyconseil in 2001, the firm that now manages the electric car sharing service\n",
      "Autolib’ .\n",
      "Before this he worked as an engineer in a variety of domains: finance (JP Morgan and\n",
      "Société Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\n",
      "published a few technical books (on C++, WiFi, and internet architectures), and was\n",
      "a Computer Science lecturer in a French engineering school.\n",
      "A few fun facts: he taught his three children to count in binary with their fingers (up\n",
      "to 1023), he studied microbiology and evolutionary genetics before going into soft‐\n",
      "ware engineering, and his parachute didn’t open on the second jump.\n",
      "Colophon\n",
      "The animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\n",
      "sorFlow is the fire salamander ( Salamandra salamandra), an amphibian found across\n",
      "most of Europe. Its black, glossy skin features large yellow spots on the head and\n",
      "back, signaling the presence of alkaloid toxins. This is a possible source of this\n",
      "amphibian’s common name: contact with these toxins (which they can also spray\n",
      "short distances) causes convulsions and hyperventilation. Either the painful poisons\n",
      "or the moistness of the salamander’s skin (or both) led to a misguided belief that these\n",
      "creatures not only could survive being placed in fire but could extinguish it as well.\n",
      "Fire salamanders live in shaded forests, hiding in moist crevices and under logs near\n",
      "the pools or other freshwater bodies that facilitate their breeding. Though they spend\n",
      "most of their life on land, they give birth to their young in water. They subsist mostly\n",
      "on a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\n",
      "in length, and in captivity, may live as long as 50 years.\n",
      "The fire salamander’s numbers have been reduced by destruction of their forest habi‐\n",
      "tat and capture for the pet trade, but the greatest threat is the susceptibility of their\n",
      "moisture-permeable skin to pollutants and microbes. Since 2014, they have become\n",
      "extinct in parts of the Netherlands and Belgium due to an introduced fungus.\n",
      "Many of the animals on O’Reilly covers are endangered; all of them are important to\n",
      "the world. To learn more about how you can help, go to animals.oreilly.com.\n",
      "The cover image is from Wood’s Illustrated Natural History. The cover fonts are URW\n",
      "Typewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\n",
      "is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.\n",
      "Andreas C. Müller & Sarah Guido\n",
      "Introduction to \n",
      "Machine \n",
      "Learning  \n",
      "with P y thon   \n",
      "A GUIDE FOR DATA SCIENTISTS\n",
      "\n",
      "Andreas C. Müller and Sarah Guido\n",
      "Introduction to Machine Learning\n",
      "with Python\n",
      "A Guide for Data Scientists\n",
      "Boston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing\n",
      "978-1-449-36941-5\n",
      "[LSI]\n",
      "Introduction to Machine Learning with Python\n",
      "by Andreas C. Müller and Sarah Guido\n",
      "Copyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles ( http://safaribooksonline.com). For more information, contact our corporate/\n",
      "institutional sales department: 800-998-9938 or corporate@oreilly.com.\n",
      "Editor: Dawn Schanafelt\n",
      "Production Editor: Kristen Brown\n",
      "Copyeditor: Rachel Head\n",
      "Proofreader: Jasmine Kwityn\n",
      "Indexer: Judy McConville\n",
      "Interior Designer: David Futato\n",
      "Cover Designer: Karen Montgomery\n",
      "Illustrator: Rebecca Demarest\n",
      "October 2016:  First Edition\n",
      "Revision History for the First Edition\n",
      "2016-09-22: First Release\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.\n",
      "The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introduction to Machine Learning with\n",
      "Python, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n",
      "While the publisher and the authors have used good faith efforts to ensure that the information and\n",
      "instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n",
      "or reliance on this work. Use of the information and instructions contained in this work is at your own\n",
      "risk. If any code samples or other technology this work contains or describes is subject to open source\n",
      "licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\n",
      "thereof complies with such licenses and/or rights.\n",
      "Table of Contents\n",
      "Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  vii\n",
      "1. Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\n",
      "Why Machine Learning?                                                                                                   1\n",
      "Problems Machine Learning Can Solve                                                                      2\n",
      "Knowing Y our Task and Knowing Y our Data                                                            4\n",
      "Why Python?                                                                                                                      5\n",
      "scikit-learn                                                                                                                          5\n",
      "Installing scikit-learn                                                                                                     6\n",
      "Essential Libraries and Tools                                                                                            7\n",
      "Jupyter Notebook                                                                                                           7\n",
      "NumPy                                                                                                                             7\n",
      "SciPy                                                                                                                                 8\n",
      "matplotlib                                                                                                                        9\n",
      "pandas                                                                                                                            10\n",
      "mglearn                                                                                                                          11\n",
      "Python 2 Versus Python 3                                                                                               12\n",
      "Versions Used in this Book                                                                                             12\n",
      "A First Application: Classifying Iris Species                                                                13\n",
      "Meet the Data                                                                                                                14\n",
      "Measuring Success: Training and Testing Data                                                        17\n",
      "First Things First: Look at Y our Data                                                                        19\n",
      "Building Y our First Model: k-Nearest Neighbors                                                    20\n",
      "Making Predictions                                                                                                      22\n",
      "Evaluating the Model                                                                                                   22\n",
      "Summary and Outlook                                                                                                   23\n",
      "iii\n",
      "2. Supervised Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25\n",
      "Classification and Regression                                                                                         25\n",
      "Generalization, Overfitting, and Underfitting                                                             26\n",
      "Relation of Model Complexity to Dataset Size                                                         29\n",
      "Supervised Machine Learning Algorithms                                                                  29\n",
      "Some Sample Datasets                                                                                                 30\n",
      "k-Nearest Neighbors                                                                                                    35\n",
      "Linear Models                                                                                                               45\n",
      "Naive Bayes Classifiers                                                                                                 68\n",
      "Decision Trees                                                                                                               70\n",
      "Ensembles of Decision Trees                                                                                      83\n",
      "Kernelized Support Vector Machines                                                                        92\n",
      "Neural Networks (Deep Learning)                                                                          104\n",
      "Uncertainty Estimates from Classifiers                                                                      119\n",
      "The Decision Function                                                                                              120\n",
      "Predicting Probabilities                                                                                             122\n",
      "Uncertainty in Multiclass Classification                                                                 124\n",
      "Summary and Outlook                                                                                                 127\n",
      "3. Unsupervised Learning and Preprocessing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  131\n",
      "Types of Unsupervised Learning                                                                                 131\n",
      "Challenges in Unsupervised Learning                                                                        132\n",
      "Preprocessing and Scaling                                                                                            132\n",
      "Different Kinds of Preprocessing                                                                             133\n",
      "Applying Data Transformations                                                                               134\n",
      "Scaling Training and Test Data the Same Way                                                       136\n",
      "The Effect of Preprocessing on Supervised Learning                                           138\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning              140\n",
      "Principal Component Analysis (PCA)                                                                    140\n",
      "Non-Negative Matrix Factorization (NMF)                                                           156\n",
      "Manifold Learning with t-SNE                                                                                 163\n",
      "Clustering                                                                                                                        168\n",
      "k-Means Clustering                                                                                                    168\n",
      "Agglomerative Clustering                                                                                         182\n",
      "DBSCAN                                                                                                                     187\n",
      "Comparing and Evaluating Clustering Algorithms                                              191\n",
      "Summary of Clustering Methods                                                                             207\n",
      "Summary and Outlook                                                                                                 208\n",
      "4. Representing Data and Engineering Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  211\n",
      "Categorical Variables                                                                                                     212\n",
      "One-Hot-Encoding (Dummy Variables)                                                                213\n",
      "iv | Table of Contents\n",
      "Numbers Can Encode Categoricals                                                                         218\n",
      "Binning, Discretization, Linear Models, and Trees                                                   220\n",
      "Interactions and Polynomials                                                                                      224\n",
      "Univariate Nonlinear Transformations                                                                      232\n",
      "Automatic Feature Selection                                                                                        236\n",
      "Univariate Statistics                                                                                                    236\n",
      "Model-Based Feature Selection                                                                                238\n",
      "Iterative Feature Selection                                                                                         240\n",
      "Utilizing Expert Knowledge                                                                                         242\n",
      "Summary and Outlook                                                                                                 250\n",
      "5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\n",
      "Cross-Validation                                                                                                            252\n",
      "Cross-Validation in scikit-learn                                                                               253\n",
      "Benefits of Cross-Validation                                                                                     254\n",
      "Stratified k-Fold Cross-Validation and Other Strategies                                      254\n",
      "Grid Search                                                                                                                     260\n",
      "Simple Grid Search                                                                                                    261\n",
      "The Danger of Overfitting the Parameters and the Validation Set                     261\n",
      "Grid Search with Cross-Validation                                                                          263\n",
      "Evaluation Metrics and Scoring                                                                                   275\n",
      "Keep the End Goal in Mind                                                                                      275\n",
      "Metrics for Binary Classification                                                                             276\n",
      "Metrics for Multiclass Classification                                                                       296\n",
      "Regression Metrics                                                                                                     299\n",
      "Using Evaluation Metrics in Model Selection                                                        300\n",
      "Summary and Outlook                                                                                                 302\n",
      "6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\n",
      "Parameter Selection with Preprocessing                                                                    306\n",
      "Building Pipelines                                                                                                          308\n",
      "Using Pipelines in Grid Searches                                                                                 309\n",
      "The General Pipeline Interface                                                                                    312\n",
      "Convenient Pipeline Creation with make_pipeline                                              313\n",
      "Accessing Step Attributes                                                                                          314\n",
      "Accessing Attributes in a Grid-Searched Pipeline                                                 315\n",
      "Grid-Searching Preprocessing Steps and Model Parameters                                  317\n",
      "Grid-Searching Which Model To Use                                                                        319\n",
      "Summary and Outlook                                                                                                 320\n",
      "7. Working with Text Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  323\n",
      "Types of Data Represented as Strings                                                                         323\n",
      "Table of Contents | v\n",
      "Example Application: Sentiment Analysis of Movie Reviews                                 325\n",
      "Representing Text Data as a Bag of Words                                                                 327\n",
      "Applying Bag-of-Words to a Toy Dataset                                                               329\n",
      "Bag-of-Words for Movie Reviews                                                                            330\n",
      "Stopwords                                                                                                                       334\n",
      "Rescaling the Data with tf–idf                                                                                      336\n",
      "Investigating Model Coefficients                                                                                 338\n",
      "Bag-of-Words with More Than One Word (n-Grams)                                            339\n",
      "Advanced Tokenization, Stemming, and Lemmatization                                        344\n",
      "Topic Modeling and Document Clustering                                                               347\n",
      "Latent Dirichlet Allocation                                                                                       348\n",
      "Summary and Outlook                                                                                                 355\n",
      "8. Wrapping Up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\n",
      "Approaching a Machine Learning Problem                                                               357\n",
      "Humans in the Loop                                                                                                  358\n",
      "From Prototype to Production                                                                                    359\n",
      "Testing Production Systems                                                                                         359\n",
      "Building Y our Own Estimator                                                                                     360\n",
      "Where to Go from Here                                                                                                361\n",
      "Theory                                                                                                                          361\n",
      "Other Machine Learning Frameworks and Packages                                           362\n",
      "Ranking, Recommender Systems, and Other Kinds of Learning                       363\n",
      "Probabilistic Modeling, Inference, and Probabilistic Programming                  363\n",
      "Neural Networks                                                                                                        364\n",
      "Scaling to Larger Datasets                                                                                         364\n",
      "Honing Y our Skills                                                                                                     365\n",
      "Conclusion                                                                                                                      366\n",
      "Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  367\n",
      "vi | Table of Contents\n",
      "Preface\n",
      "Machine learning is an integral part of many commercial applications and research\n",
      "projects today, in areas ranging from medical diagnosis and treatment to finding your\n",
      "friends on social networks. Many people think that machine learning can only be\n",
      "applied by large companies with extensive research teams. In this book, we want to\n",
      "show you how easy it can be to build machine learning solutions yourself, and how to\n",
      "best go about it. With the knowledge in this book, you can build your own system for\n",
      "finding out how people feel on Twitter, or making predictions about global warming.\n",
      "The applications of machine learning are endless and, with the amount of data avail‐\n",
      "able today, mostly limited by your imagination.\n",
      "Who Should Read This Book\n",
      "This book is for current and aspiring machine learning practitioners looking to\n",
      "implement solutions to real-world machine learning problems. This is an introduc‐\n",
      "tory book requiring no previous knowledge of machine learning or artificial intelli‐\n",
      "gence (AI). We focus on using Python and the scikit-learn library, and work\n",
      "through all the steps to create a successful machine learning application. The meth‐\n",
      "ods we introduce will be helpful for scientists and researchers, as well as data scien‐\n",
      "tists working on commercial applications. Y ou will get the most out of the book if you\n",
      "are somewhat familiar with Python and the NumPy and matplotlib libraries.\n",
      "We made a conscious effort not to focus too much on the math, but rather on the\n",
      "practical aspects of using machine learning algorithms. As mathematics (probability\n",
      "theory, in particular) is the foundation upon which machine learning is built, we\n",
      "won’t go into the analysis of the algorithms in great detail. If you are interested in the\n",
      "mathematics of machine learning algorithms, we recommend the book The Elements\n",
      "of Statistical Learning  (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\n",
      "Friedman, which is available for free at the authors’ website. We will also not describe\n",
      "how to write machine learning algorithms from scratch, and will instead focus on\n",
      "vii\n",
      "how to use the large array of models already implemented in scikit-learn and other\n",
      "libraries.\n",
      "Why We Wrote This Book\n",
      "There are many books on machine learning and AI. However, all of them are meant\n",
      "for graduate students or PhD students in computer science, and they’re full of\n",
      "advanced mathematics. This is in stark contrast with how machine learning is being\n",
      "used, as a commodity tool in research and commercial applications. Today, applying\n",
      "machine learning does not require a PhD. However, there are few resources out there\n",
      "that fully cover all the important aspects of implementing machine learning in prac‐\n",
      "tice, without requiring you to take advanced math courses. We hope this book will\n",
      "help people who want to apply machine learning without reading up on years’ worth\n",
      "of calculus, linear algebra, and probability theory.\n",
      "Navigating This Book\n",
      "This book is organized roughly as follows:\n",
      "• Chapter 1  introduces the fundamental concepts of machine learning and its\n",
      "applications, and describes the setup we will be using throughout the book.\n",
      "• Chapters 2 and 3 describe the actual machine learning algorithms that are most\n",
      "widely used in practice, and discuss their advantages and shortcomings.\n",
      "• Chapter 4 discusses the importance of how we represent data that is processed by\n",
      "machine learning, and what aspects of the data to pay attention to.\n",
      "• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\n",
      "with a particular focus on cross-validation and grid search.\n",
      "• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\n",
      "ing your workflow.\n",
      "• Chapter 7 shows how to apply the methods described in earlier chapters to text\n",
      "data, and introduces some text-specific processing techniques.\n",
      "• Chapter 8 offers a high-level overview, and includes references to more advanced\n",
      "topics.\n",
      "While Chapters 2 and 3 provide the actual algorithms, understanding all of these\n",
      "algorithms might not be necessary for a beginner. If you need to build a machine\n",
      "learning system ASAP , we suggest starting with Chapter 1 and the opening sections of\n",
      "Chapter 2, which introduce all the core concepts. Y ou can then skip to “Summary and\n",
      "Outlook” on page 127 in Chapter 2, which includes a list of all the supervised models\n",
      "that we cover. Choose the model that best fits your needs and flip back to read the\n",
      "viii | Preface\n",
      "section devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\n",
      "uate and tune your model.\n",
      "Online Resources\n",
      "While studying this book, definitely refer to the scikit-learn website for more in-\n",
      "depth documentation of the classes and functions, and many examples. There is also\n",
      "a video course created by Andreas Müller, “ Advanced Machine Learning with scikit-\n",
      "learn, ” that supplements this book. Y ou can find it at http://bit.ly/\n",
      "advanced_machine_learning_scikit-learn.\n",
      "Conventions Used in This Book\n",
      "The following typographical conventions are used in this book:\n",
      "Italic\n",
      "Indicates new terms, URLs, email addresses, filenames, and file extensions.\n",
      "Constant width\n",
      "Used for program listings, as well as within paragraphs to refer to program ele‐\n",
      "ments such as variable or function names, databases, data types, environment\n",
      "variables, statements, and keywords. Also used for commands and module and\n",
      "package names.\n",
      "Constant width bold\n",
      "Shows commands or other text that should be typed literally by the user.\n",
      "Constant width italic\n",
      "Shows text that should be replaced with user-supplied values or by values deter‐\n",
      "mined by context.\n",
      "This element signifies a tip or suggestion.\n",
      "This element signifies a general note.\n",
      "Preface | ix\n",
      "This icon indicates a warning or caution.\n",
      "Using Code Examples\n",
      "Supplemental material (code examples, IPython notebooks, etc.) is available for\n",
      "download at https://github.com/amueller/introduction_to_ml_with_python.\n",
      "This book is here to help you get your job done. In general, if example code is offered\n",
      "with this book, you may use it in your programs and documentation. Y ou do not\n",
      "need to contact us for permission unless you’re reproducing a significant portion of\n",
      "the code. For example, writing a program that uses several chunks of code from this\n",
      "book does not require permission. Selling or distributing a CD-ROM of examples\n",
      "from O’Reilly books does require permission. Answering a question by citing this\n",
      "book and quoting example code does not require permission. Incorporating a signifi‐\n",
      "cant amount of example code from this book into your product’s documentation does\n",
      "require permission.\n",
      "We appreciate, but do not require, attribution. An attribution usually includes the\n",
      "title, author, publisher, and ISBN. For example: “An Introduction to Machine Learning\n",
      "with Python by Andreas C. Müller and Sarah Guido (O’Reilly). Copyright 2017 Sarah\n",
      "Guido and Andreas Müller, 978-1-449-36941-5. ”\n",
      "If you feel your use of code examples falls outside fair use or the permission given\n",
      "above, feel free to contact us at permissions@oreilly.com.\n",
      "Safari® Books Online\n",
      "Safari Books Online is an on-demand digital library that deliv‐\n",
      "ers expert content in both book and video form from the\n",
      "world’s leading authors in technology and business.\n",
      "Technology professionals, software developers, web designers, and business and crea‐\n",
      "tive professionals use Safari Books Online as their primary resource for research,\n",
      "problem solving, learning, and certification training.\n",
      "Safari Books Online offers a range of plans and pricing  for enterprise, government,\n",
      "education, and individuals.\n",
      "Members have access to thousands of books, training videos, and prepublication\n",
      "manuscripts in one fully searchable database from publishers like O’Reilly Media,\n",
      "Prentice Hall Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que,\n",
      "x | Preface\n",
      "Peachpit Press, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kauf‐\n",
      "mann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders,\n",
      "McGraw-Hill, Jones & Bartlett, Course Technology, and hundreds more. For more\n",
      "information about Safari Books Online, please visit us online.\n",
      "How to Contact Us\n",
      "Please address comments and questions concerning this book to the publisher:\n",
      "O’Reilly Media, Inc.\n",
      "1005 Gravenstein Highway North\n",
      "Sebastopol, CA 95472\n",
      "800-998-9938 (in the United States or Canada)\n",
      "707-829-0515 (international or local)\n",
      "707-829-0104 (fax)\n",
      "We have a web page for this book, where we list errata, examples, and any additional\n",
      "information. Y ou can access this page at http://bit.ly/intro-machine-learning-python.\n",
      "To comment or ask technical questions about this book, send email to bookques‐\n",
      "tions@oreilly.com.\n",
      "For more information about our books, courses, conferences, and news, see our web‐\n",
      "site at http://www.oreilly.com.\n",
      "Find us on Facebook: http://facebook.com/oreilly\n",
      "Follow us on Twitter: http://twitter.com/oreillymedia\n",
      "Watch us on Y ouTube: http://www.youtube.com/oreillymedia\n",
      "Acknowledgments\n",
      "From Andreas\n",
      "Without the help and support of a large group of people, this book would never have\n",
      "existed.\n",
      "I would like to thank the editors, Meghan Blanchette, Brian MacDonald, and in par‐\n",
      "ticular Dawn Schanafelt, for helping Sarah and me make this book a reality.\n",
      "I want to thank my reviewers, Thomas Caswell, Olivier Grisel, Stefan van der Walt,\n",
      "and John Myles White, who took the time to read the early versions of this book and\n",
      "provided me with invaluable feedback—in addition to being some of the corner‐\n",
      "stones of the scientific open source ecosystem.\n",
      "Preface | xi\n",
      "I am forever thankful for the welcoming open source scientific Python community,\n",
      "especially the contributors to scikit-learn. Without the support and help from this\n",
      "community, in particular from Gael Varoquaux, Alex Gramfort, and Olivier Grisel, I\n",
      "would never have become a core contributor to scikit-learn or learned to under‐\n",
      "stand this package as well as I do now. My thanks also go out to all the other contrib‐\n",
      "utors who donate their time to improve and maintain this package.\n",
      "I’m also thankful for the discussions with many of my colleagues and peers that hel‐\n",
      "ped me understand the challenges of machine learning and gave me ideas for struc‐\n",
      "turing a textbook. Among the people I talk to about machine learning, I specifically\n",
      "want to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\n",
      "Hugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\n",
      "and Dan Cervone.\n",
      "My thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\n",
      "of an early version of this book, and helped me shape it in many ways.\n",
      "On the personal side, I want to thank my parents, Harald and Margot, and my sister,\n",
      "Miriam, for their continuing support and encouragement. I also want to thank the\n",
      "many people in my life whose love and friendship gave me the energy and support to\n",
      "undertake such a challenging task.\n",
      "From Sarah\n",
      "I would like to thank Meg Blanchette, without whose help and guidance this project\n",
      "would not have even existed. Thanks to Celia La and Brian Carlson for reading in the\n",
      "early days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\n",
      "to DTS, for your everlasting and endless support.\n",
      "xii | Preface\n",
      "CHAPTER 1\n",
      "Introduction\n",
      "Machine learning is about extracting knowledge from data. It is a research field at the\n",
      "intersection of statistics, artificial intelligence, and computer science and is also\n",
      "known as predictive analytics or statistical learning. The application of machine\n",
      "learning methods has in recent years become ubiquitous in everyday life. From auto‐\n",
      "matic recommendations of which movies to watch, to what food to order or which\n",
      "products to buy, to personalized online radio and recognizing your friends in your\n",
      "photos, many modern websites and devices have machine learning algorithms at their\n",
      "core. When you look at a complex website like Facebook, Amazon, or Netflix, it is\n",
      "very likely that every part of the site contains multiple machine learning models.\n",
      "Outside of commercial applications, machine learning has had a tremendous influ‐\n",
      "ence on the way data-driven research is done today. The tools introduced in this book\n",
      "have been applied to diverse scientific problems such as understanding stars, finding\n",
      "distant planets, discovering new particles, analyzing DNA sequences, and providing\n",
      "personalized cancer treatments.\n",
      "Y our application doesn’t need to be as large-scale or world-changing as these exam‐\n",
      "ples in order to benefit from machine learning, though. In this chapter, we will\n",
      "explain why machine learning has become so popular and discuss what kinds of\n",
      "problems can be solved using machine learning. Then, we will show you how to build\n",
      "your first machine learning model, introducing important concepts along the way.\n",
      "Why Machine Learning?\n",
      "In the early days of “intelligent” applications, many systems used handcoded rules of\n",
      "“if ” and “else” decisions to process data or adjust to user input. Think of a spam filter\n",
      "whose job is to move the appropriate incoming email messages to a spam folder. Y ou\n",
      "could make up a blacklist of words that would result in an email being marked as\n",
      "1\n",
      "spam. This would be an example of using an expert-designed rule system to design an\n",
      "“intelligent” application. Manually crafting decision rules is feasible for some applica‐\n",
      "tions, particularly those in which humans have a good understanding of the process\n",
      "to model. However, using handcoded rules to make decisions has two major disad‐\n",
      "vantages:\n",
      "• The logic required to make a decision is specific to a single domain and task.\n",
      "Changing the task even slightly might require a rewrite of the whole system.\n",
      "• Designing rules requires a deep understanding of how a decision should be made\n",
      "by a human expert.\n",
      "One example of where this handcoded approach will fail is in detecting faces in\n",
      "images. Today, every smartphone can detect a face in an image. However, face detec‐\n",
      "tion was an unsolved problem until as recently as 2001. The main problem is that the\n",
      "way in which pixels (which make up an image in a computer) are “perceived” by the\n",
      "computer is very different from how humans perceive a face. This difference in repre‐\n",
      "sentation makes it basically impossible for a human to come up with a good set of\n",
      "rules to describe what constitutes a face in a digital image.\n",
      "Using machine learning, however, simply presenting a program with a large collec‐\n",
      "tion of images of faces is enough for an algorithm to determine what characteristics\n",
      "are needed to identify a face.\n",
      "Problems Machine Learning Can Solve\n",
      "The most successful kinds of machine learning algorithms are those that automate\n",
      "decision-making processes by generalizing from known examples. In this setting,\n",
      "which is known as supervised learning, the user provides the algorithm with pairs of\n",
      "inputs and desired outputs, and the algorithm finds a way to produce the desired out‐\n",
      "put given an input. In particular, the algorithm is able to create an output for an input\n",
      "it has never seen before without any help from a human. Going back to our example\n",
      "of spam classification, using machine learning, the user provides the algorithm with a\n",
      "large number of emails (which are the input), together with information about\n",
      "whether any of these emails are spam (which is the desired output). Given a new\n",
      "email, the algorithm will then produce a prediction as to whether the new email is\n",
      "spam.\n",
      "Machine learning algorithms that learn from input/output pairs are called supervised\n",
      "learning algorithms because a “teacher” provides supervision to the algorithms in the\n",
      "form of the desired outputs for each example that they learn from. While creating a\n",
      "dataset of inputs and outputs is often a laborious manual process, supervised learning\n",
      "algorithms are well understood and their performance is easy to measure. If your\n",
      "application can be formulated as a supervised learning problem, and you are able to\n",
      "2 | Chapter 1: Introduction\n",
      "create a dataset that includes the desired outcome, machine learning will likely be\n",
      "able to solve your problem.\n",
      "Examples of supervised machine learning tasks include:\n",
      "Identifying the zip code from handwritten digits on an envelope\n",
      "Here the input is a scan of the handwriting, and the desired output is the actual\n",
      "digits in the zip code. To create a dataset for building a machine learning model,\n",
      "you need to collect many envelopes. Then you can read the zip codes yourself\n",
      "and store the digits as your desired outcomes.\n",
      "Determining whether a tumor is benign based on a medical image\n",
      "Here the input is the image, and the output is whether the tumor is benign. To\n",
      "create a dataset for building a model, you need a database of medical images. Y ou\n",
      "also need an expert opinion, so a doctor needs to look at all of the images and\n",
      "decide which tumors are benign and which are not. It might even be necessary to\n",
      "do additional diagnosis beyond the content of the image to determine whether\n",
      "the tumor in the image is cancerous or not.\n",
      "Detecting fraudulent activity in credit card transactions\n",
      "Here the input is a record of the credit card transaction, and the output is\n",
      "whether it is likely to be fraudulent or not. Assuming that you are the entity dis‐\n",
      "tributing the credit cards, collecting a dataset means storing all transactions and\n",
      "recording if a user reports any transaction as fraudulent.\n",
      "An interesting thing to note about these examples is that although the inputs and out‐\n",
      "puts look fairly straightforward, the data collection process for these three tasks is\n",
      "vastly different. While reading envelopes is laborious, it is easy and cheap. Obtaining\n",
      "medical imaging and diagnoses, on the other hand, requires not only expensive\n",
      "machinery but also rare and expensive expert knowledge, not to mention the ethical\n",
      "concerns and privacy issues. In the example of detecting credit card fraud, data col‐\n",
      "lection is much simpler. Y our customers will provide you with the desired output, as\n",
      "they will report fraud. All you have to do to obtain the input/output pairs of fraudu‐\n",
      "lent and nonfraudulent activity is wait.\n",
      "Unsupervised algorithms are the other type of algorithm that we will cover in this\n",
      "book. In unsupervised learning, only the input data is known, and no known output\n",
      "data is given to the algorithm. While there are many successful applications of these\n",
      "methods, they are usually harder to understand and evaluate.\n",
      "Examples of unsupervised learning include:\n",
      "Identifying topics in a set of blog posts\n",
      "If you have a large collection of text data, you might want to summarize it and\n",
      "find prevalent themes in it. Y ou might not know beforehand what these topics\n",
      "are, or how many topics there might be. Therefore, there are no known outputs.\n",
      "Why Machine Learning? | 3\n",
      "Segmenting customers into groups with similar preferences\n",
      "Given a set of customer records, you might want to identify which customers are\n",
      "similar, and whether there are groups of customers with similar preferences. For\n",
      "a shopping site, these might be “parents, ” “bookworms, ” or “gamers. ” Because you\n",
      "don’t know in advance what these groups might be, or even how many there are,\n",
      "you have no known outputs.\n",
      "Detecting abnormal access patterns to a website\n",
      "To identify abuse or bugs, it is often helpful to find access patterns that are differ‐\n",
      "ent from the norm. Each abnormal pattern might be very different, and you\n",
      "might not have any recorded instances of abnormal behavior. Because in this\n",
      "example you only observe traffic, and you don’t know what constitutes normal\n",
      "and abnormal behavior, this is an unsupervised problem.\n",
      "For both supervised and unsupervised learning tasks, it is important to have a repre‐\n",
      "sentation of your input data that a computer can understand. Often it is helpful to\n",
      "think of your data as a table. Each data point that you want to reason about (each\n",
      "email, each customer, each transaction) is a row, and each property that describes that\n",
      "data point (say, the age of a customer or the amount or location of a transaction) is a\n",
      "column. Y ou might describe users by their age, their gender, when they created an\n",
      "account, and how often they have bought from your online shop. Y ou might describe\n",
      "the image of a tumor by the grayscale values of each pixel, or maybe by using the size,\n",
      "shape, and color of the tumor.\n",
      "Each entity or row here is known as a sample (or data point) in machine learning,\n",
      "while the columns—the properties that describe these entities—are called features.\n",
      "Later in this book we will go into more detail on the topic of building a good repre‐\n",
      "sentation of your data, which is called feature extraction or feature engineering. Y ou\n",
      "should keep in mind, however, that no machine learning algorithm will be able to\n",
      "make a prediction on data for which it has no information. For example, if the only\n",
      "feature that you have for a patient is their last name, no algorithm will be able to pre‐\n",
      "dict their gender. This information is simply not contained in your data. If you add\n",
      "another feature that contains the patient’s first name, you will have much better luck,\n",
      "as it is often possible to tell the gender by a person’s first name.\n",
      "Knowing Your Task and Knowing Your Data\n",
      "Quite possibly the most important part in the machine learning process is under‐\n",
      "standing the data you are working with and how it relates to the task you want to\n",
      "solve. It will not be effective to randomly choose an algorithm and throw your data at\n",
      "it. It is necessary to understand what is going on in your dataset before you begin\n",
      "building a model. Each algorithm is different in terms of what kind of data and what\n",
      "problem setting it works best for. While you are building a machine learning solution,\n",
      "you should answer, or at least keep in mind, the following questions:\n",
      "4 | Chapter 1: Introduction\n",
      "• What question(s) am I trying to answer? Do I think the data collected can answer\n",
      "that question?\n",
      "• What is the best way to phrase my question(s) as a machine learning problem?\n",
      "• Have I collected enough data to represent the problem I want to solve?\n",
      "• What features of the data did I extract, and will these enable the right\n",
      "predictions?\n",
      "• How will I measure success in my application?\n",
      "• How will the machine learning solution interact with other parts of my research\n",
      "or business product?\n",
      "In a larger context, the algorithms and methods in machine learning are only one\n",
      "part of a greater process to solve a particular problem, and it is good to keep the big\n",
      "picture in mind at all times. Many people spend a lot of time building complex\n",
      "machine learning solutions, only to find out they don’t solve the right problem.\n",
      "When going deep into the technical aspects of machine learning (as we will in this\n",
      "book), it is easy to lose sight of the ultimate goals. While we will not discuss the ques‐\n",
      "tions listed here in detail, we still encourage you to keep in mind all the assumptions\n",
      "that you might be making, explicitly or implicitly, when you start building machine\n",
      "learning models.\n",
      "Why Python?\n",
      "Python has become the lingua franca for many data science applications. It combines\n",
      "the power of general-purpose programming languages with the ease of use of\n",
      "domain-specific scripting languages like MATLAB or R. Python has libraries for data\n",
      "loading, visualization, statistics, natural language processing, image processing, and\n",
      "more. This vast toolbox provides data scientists with a large array of general- and\n",
      "special-purpose functionality. One of the main advantages of using Python is the abil‐\n",
      "ity to interact directly with the code, using a terminal or other tools like the Jupyter\n",
      "Notebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\n",
      "mentally iterative processes, in which the data drives the analysis. It is essential for\n",
      "these processes to have tools that allow quick iteration and easy interaction.\n",
      "As a general-purpose programming language, Python also allows for the creation of\n",
      "complex graphical user interfaces (GUIs) and web services, and for integration into\n",
      "existing systems.\n",
      "scikit-learn\n",
      "scikit-learn is an open source project, meaning that it is free to use and distribute,\n",
      "and anyone can easily obtain the source code to see what is going on behind the\n",
      "Why Python? | 5\n",
      "scenes. The scikit-learn project is constantly being developed and improved, and it\n",
      "has a very active user community. It contains a number of state-of-the-art machine\n",
      "learning algorithms, as well as comprehensive documentation about each algorithm.\n",
      "scikit-learn is a very popular tool, and the most prominent Python library for\n",
      "machine learning. It is widely used in industry and academia, and a wealth of tutori‐\n",
      "als and code snippets are available online. scikit-learn works well with a number of\n",
      "other scientific Python tools, which we will discuss later in this chapter.\n",
      "While reading this, we recommend that you also browse the scikit-learn user guide \n",
      "and API documentation for additional details on and many more options for each\n",
      "algorithm. The online documentation is very thorough, and this book will provide\n",
      "you with all the prerequisites in machine learning to understand it in detail.\n",
      "Installing scikit-learn\n",
      "scikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\n",
      "ting and interactive development, you should also install matplotlib, IPython, and\n",
      "the Jupyter Notebook. We recommend using one of the following prepackaged\n",
      "Python distributions, which will provide the necessary packages:\n",
      "Anaconda\n",
      "A Python distribution made for large-scale data processing, predictive analytics,\n",
      "and scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\n",
      "pandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\n",
      "Windows, and Linux, it is a very convenient solution and is the one we suggest\n",
      "for people without an existing installation of the scientific Python packages. Ana‐\n",
      "conda now also includes the commercial Intel MKL library for free. Using MKL\n",
      "(which is done automatically when Anaconda is installed) can give significant\n",
      "speed improvements for many algorithms in scikit-learn.\n",
      "Enthought Canopy\n",
      "Another Python distribution for scientific computing. This comes with NumPy,\n",
      "SciPy, matplotlib, pandas, and IPython, but the free version does not come with\n",
      "scikit-learn. If you are part of an academic, degree-granting institution, you\n",
      "can request an academic license and get free access to the paid subscription ver‐\n",
      "sion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\n",
      "works on Mac OS, Windows, and Linux.\n",
      "Python(x,y)\n",
      "A free Python distribution for scientific computing, specifically for Windows.\n",
      "Python(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\n",
      "scikit-learn.\n",
      "6 | Chapter 1: Introduction\n",
      "1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\n",
      "ture Notes.\n",
      "If you already have a Python installation set up, you can use pip to install all of these\n",
      "packages:\n",
      "$ pip install numpy scipy matplotlib ipython scikit-learn pandas\n",
      "Essential Libraries and Tools\n",
      "Understanding what scikit-learn is and how to use it is important, but there are a\n",
      "few other libraries that will enhance your experience. scikit-learn is built on top of\n",
      "the NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\n",
      "will be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\n",
      "which is a browser-based interactive programming environment. Briefly, here is what\n",
      "you should know about these tools in order to get the most out of scikit-learn.1\n",
      "Jupyter Notebook\n",
      "The Jupyter Notebook is an interactive environment for running code in the browser.\n",
      "It is a great tool for exploratory data analysis and is widely used by data scientists.\n",
      "While the Jupyter Notebook supports many programming languages, we only need\n",
      "the Python support. The Jupyter Notebook makes it easy to incorporate code, text,\n",
      "and images, and all of this book was in fact written as a Jupyter Notebook. All of the\n",
      "code examples we include can be downloaded from GitHub.\n",
      "NumPy\n",
      "NumPy is one of the fundamental packages for scientific computing in Python. It\n",
      "contains functionality for multidimensional arrays, high-level mathematical func‐\n",
      "tions such as linear algebra operations and the Fourier transform, and pseudorandom\n",
      "number generators.\n",
      "In scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\n",
      "takes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\n",
      "verted to a NumPy array. The core functionality of NumPy is the ndarray class, a\n",
      "multidimensional ( n-dimensional) array. All elements of the array must be of the\n",
      "same type. A NumPy array looks like this:\n",
      "In[2]:\n",
      "import numpy as np\n",
      "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
      "print(\"x:\\n{}\".format(x))\n",
      "Essential Libraries and Tools | 7\n",
      "Out[2]:\n",
      "x:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "We will be using NumPy a lot in this book, and we will refer to objects of the NumPy\n",
      "ndarray class as “NumPy arrays” or just “arrays. ”\n",
      "SciPy\n",
      "SciPy is a collection of functions for scientific computing in Python. It provides,\n",
      "among other functionality, advanced linear algebra routines, mathematical function\n",
      "optimization, signal processing, special mathematical functions, and statistical distri‐\n",
      "butions. scikit-learn draws from SciPy’s collection of functions for implementing\n",
      "its algorithms. The most important part of SciPy for us is scipy.sparse: this provides\n",
      "sparse matrices, which are another representation that is used for data in scikit-\n",
      "learn. Sparse matrices are used whenever we want to store a 2D array that contains\n",
      "mostly zeros:\n",
      "In[3]:\n",
      "from scipy import sparse\n",
      "# Create a 2D NumPy array with a diagonal of ones, and zeros everywhere else\n",
      "eye = np.eye(4)\n",
      "print(\"NumPy array:\\n{}\".format(eye))\n",
      "Out[3]:\n",
      "NumPy array:\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "In[4]:\n",
      "# Convert the NumPy array to a SciPy sparse matrix in CSR format\n",
      "# Only the nonzero entries are stored\n",
      "sparse_matrix = sparse.csr_matrix(eye)\n",
      "print(\"\\nSciPy sparse CSR matrix:\\n{}\".format(sparse_matrix))\n",
      "Out[4]:\n",
      "SciPy sparse CSR matrix:\n",
      "  (0, 0)    1.0\n",
      "  (1, 1)    1.0\n",
      "  (2, 2)    1.0\n",
      "  (3, 3)    1.0\n",
      "8 | Chapter 1: Introduction\n",
      "Usually it is not possible to create dense representations of sparse data (as they would\n",
      "not fit into memory), so we need to create sparse representations directly. Here is a\n",
      "way to create the same sparse matrix as before, using the COO format:\n",
      "In[5]:\n",
      "data = np.ones(4)\n",
      "row_indices = np.arange(4)\n",
      "col_indices = np.arange(4)\n",
      "eye_coo = sparse.coo_matrix((data, (row_indices, col_indices)))\n",
      "print(\"COO representation:\\n{}\".format(eye_coo))\n",
      "Out[5]:\n",
      "COO representation:\n",
      "  (0, 0)    1.0\n",
      "  (1, 1)    1.0\n",
      "  (2, 2)    1.0\n",
      "  (3, 3)    1.0\n",
      "More details on SciPy sparse matrices can be found in the SciPy Lecture Notes.\n",
      "matplotlib\n",
      "matplotlib is the primary scientific plotting library in Python. It provides functions\n",
      "for making publication-quality visualizations such as line charts, histograms, scatter\n",
      "plots, and so on. Visualizing your data and different aspects of your analysis can give\n",
      "you important insights, and we will be using matplotlib for all our visualizations.\n",
      "When working inside the Jupyter Notebook, you can show figures directly in the\n",
      "browser by using the %matplotlib notebook and %matplotlib inline commands.\n",
      "We recommend using %matplotlib notebook, which provides an interactive envi‐\n",
      "ronment (though we are using %matplotlib inline to produce this book). For\n",
      "example, this code produces the plot in Figure 1-1:\n",
      "In[6]:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "# Generate a sequence of numbers from -10 to 10 with 100 steps in between\n",
      "x = np.linspace(-10, 10, 100)\n",
      "# Create a second array using sine\n",
      "y = np.sin(x)\n",
      "# The plot function makes a line chart of one array against another\n",
      "plt.plot(x, y, marker=\"x\")\n",
      "Essential Libraries and Tools | 9\n",
      "Figure 1-1. Simple line plot of the sine function using matplotlib\n",
      "pandas\n",
      "pandas is a Python library for data wrangling and analysis. It is built around a data\n",
      "structure called the DataFrame that is modeled after the R DataFrame. Simply put, a\n",
      "pandas DataFrame is a table, similar to an Excel spreadsheet. pandas provides a great\n",
      "range of methods to modify and operate on this table; in particular, it allows SQL-like\n",
      "queries and joins of tables. In contrast to NumPy, which requires that all entries in an\n",
      "array be of the same type, pandas allows each column to have a separate type (for\n",
      "example, integers, dates, floating-point numbers, and strings). Another valuable tool\n",
      "provided by pandas is its ability to ingest from a great variety of file formats and data‐\n",
      "bases, like SQL, Excel files, and comma-separated values (CSV) files. Going into\n",
      "detail about the functionality of pandas is out of the scope of this book. However,\n",
      "Python for Data Analysis  by Wes McKinney (O’Reilly, 2012) provides a great guide.\n",
      "Here is a small example of creating a DataFrame using a dictionary:\n",
      "In[7]:\n",
      "import pandas as pd\n",
      "# create a simple dataset of people\n",
      "data = {'Name': [\"John\", \"Anna\", \"Peter\", \"Linda\"],\n",
      "        'Location' : [\"New York\", \"Paris\", \"Berlin\", \"London\"],\n",
      "        'Age' : [24, 13, 53, 33]\n",
      "       }\n",
      "data_pandas = pd.DataFrame(data)\n",
      "# IPython.display allows \"pretty printing\" of dataframes\n",
      "# in the Jupyter notebook\n",
      "display(data_pandas)\n",
      "10 | Chapter 1: Introduction\n",
      "This produces the following output:\n",
      "Age Location Name\n",
      "0 24 New York John\n",
      "1 13 Paris Anna\n",
      "2 53 Berlin Peter\n",
      "3 33 London Linda\n",
      "There are several possible ways to query this table. For example:\n",
      "In[8]:\n",
      "# Select all rows that have an age column greater than 30\n",
      "display(data_pandas[data_pandas.Age > 30])\n",
      "This produces the following result:\n",
      "Age Location Name\n",
      "2 53 Berlin Peter\n",
      "3 33 London Linda\n",
      "mglearn\n",
      "This book comes with accompanying code, which you can find on GitHub. The\n",
      "accompanying code includes not only all the examples shown in this book, but also\n",
      "the mglearn library. This is a library of utility functions we wrote for this book, so\n",
      "that we don’t clutter up our code listings with details of plotting and data loading. If\n",
      "you’re interested, you can look up all the functions in the repository, but the details of\n",
      "the mglearn module are not really important to the material in this book. If you see a\n",
      "call to mglearn in the code, it is usually a way to make a pretty picture quickly, or to\n",
      "get our hands on some interesting data.\n",
      "Throughout the book we make ample use of NumPy, matplotlib\n",
      "and pandas. All the code will assume the following imports:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import mglearn\n",
      "We also assume that you will run the code in a Jupyter Notebook\n",
      "with the %matplotlib notebook or %matplotlib inline magic\n",
      "enabled to show plots. If you are not using the notebook or these\n",
      "magic commands, you will have to call plt.show to actually show\n",
      "any of the figures.\n",
      "Essential Libraries and Tools | 11\n",
      "2 The six package can be very handy for that.\n",
      "Python 2 Versus Python 3\n",
      "There are two major versions of Python that are widely used at the moment: Python 2\n",
      "(more precisely, 2.7) and Python 3 (with the latest release being 3.5 at the time of\n",
      "writing). This sometimes leads to some confusion. Python 2 is no longer actively\n",
      "developed, but because Python 3 contains major changes, Python 2 code usually does\n",
      "not run on Python 3. If you are new to Python, or are starting a new project from\n",
      "scratch, we highly recommend using the latest version of Python 3 without changes.\n",
      "If you have a large codebase that you rely on that is written for Python 2, you are\n",
      "excused from upgrading for now. However, you should try to migrate to Python 3 as\n",
      "soon as possible. When writing any new code, it is for the most part quite easy to\n",
      "write code that runs under Python 2 and Python 3.2 If you don’t have to interface with\n",
      "legacy software, you should definitely use Python 3. All the code in this book is writ‐\n",
      "ten in a way that works for both versions. However, the exact output might differ\n",
      "slightly under Python 2.\n",
      "Versions Used in this Book\n",
      "We are using the following versions of the previously mentioned libraries in this\n",
      "book:\n",
      "In[9]:\n",
      "import sys\n",
      "print(\"Python version: {}\".format(sys.version))\n",
      "import pandas as pd\n",
      "print(\"pandas version: {}\".format(pd.__version__))\n",
      "import matplotlib\n",
      "print(\"matplotlib version: {}\".format(matplotlib.__version__))\n",
      "import numpy as np\n",
      "print(\"NumPy version: {}\".format(np.__version__))\n",
      "import scipy as sp\n",
      "print(\"SciPy version: {}\".format(sp.__version__))\n",
      "import IPython\n",
      "print(\"IPython version: {}\".format(IPython.__version__))\n",
      "import sklearn\n",
      "print(\"scikit-learn version: {}\".format(sklearn.__version__))\n",
      "12 | Chapter 1: Introduction\n",
      "Out[9]:\n",
      "Python version: 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  2 2016, 17:53:06)\n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "pandas version: 0.18.1\n",
      "matplotlib version: 1.5.1\n",
      "NumPy version: 1.11.1\n",
      "SciPy version: 0.17.1\n",
      "IPython version: 5.1.0\n",
      "scikit-learn version: 0.18\n",
      "While it is not important to match these versions exactly, you should have a version\n",
      "of scikit-learn that is as least as recent as the one we used.\n",
      "Now that we have everything set up, let’s dive into our first application of machine\n",
      "learning.\n",
      "This book assumes that you have version 0.18 or later of scikit-\n",
      "learn. The model_selection module was added in 0.18, and if you\n",
      "use an earlier version of scikit-learn, you will need to adjust the\n",
      "imports from this module.\n",
      "A First Application: Classifying Iris Species\n",
      "In this section, we will go through a simple machine learning application and create\n",
      "our first model. In the process, we will introduce some core concepts and terms.\n",
      "Let’s assume that a hobby botanist is interested in distinguishing the species of some\n",
      "iris flowers that she has found. She has collected some measurements associated with\n",
      "each iris: the length and width of the petals and the length and width of the sepals, all\n",
      "measured in centimeters (see Figure 1-2).\n",
      "She also has the measurements of some irises that have been previously identified by\n",
      "an expert botanist as belonging to the species setosa, versicolor, or virginica. For these\n",
      "measurements, she can be certain of which species each iris belongs to. Let’s assume\n",
      "that these are the only species our hobby botanist will encounter in the wild.\n",
      "Our goal is to build a machine learning model that can learn from the measurements\n",
      "of these irises whose species is known, so that we can predict the species for a new\n",
      "iris.\n",
      "A First Application: Classifying Iris Species | 13\n",
      "Figure 1-2. Parts of the iris flower\n",
      "Because we have measurements for which we know the correct species of iris, this is a\n",
      "supervised learning problem. In this problem, we want to predict one of several\n",
      "options (the species of iris). This is an example of a classification problem. The possi‐\n",
      "ble outputs (different species of irises) are called classes. Every iris in the dataset\n",
      "belongs to one of three classes, so this problem is a three-class classification problem.\n",
      "The desired output for a single data point (an iris) is the species of this flower. For a\n",
      "particular data point, the species it belongs to is called its label.\n",
      "Meet the Data\n",
      "The data we will use for this example is the Iris dataset, a classical dataset in machine\n",
      "learning and statistics. It is included in scikit-learn in the datasets module. We\n",
      "can load it by calling the load_iris function:\n",
      "In[10]:\n",
      "from sklearn.datasets import load_iris\n",
      "iris_dataset = load_iris()\n",
      "The iris object that is returned by load_iris is a Bunch object, which is very similar\n",
      "to a dictionary. It contains keys and values:\n",
      "14 | Chapter 1: Introduction\n",
      "In[11]:\n",
      "print(\"Keys of iris_dataset: \\n{}\".format(iris_dataset.keys()))\n",
      "Out[11]:\n",
      "Keys of iris_dataset:\n",
      "dict_keys(['target_names', 'feature_names', 'DESCR', 'data', 'target'])\n",
      "The value of the key DESCR is a short description of the dataset. We show the begin‐\n",
      "ning of the description here (feel free to look up the rest yourself):\n",
      "In[12]:\n",
      "print(iris_dataset['DESCR'][:193] + \"\\n...\")\n",
      "Out[12]:\n",
      "Iris Plants Database\n",
      "====================\n",
      "Notes\n",
      "----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive att\n",
      "...\n",
      "----\n",
      "The value of the key target_names is an array of strings, containing the species of\n",
      "flower that we want to predict:\n",
      "In[13]:\n",
      "print(\"Target names: {}\".format(iris_dataset['target_names']))\n",
      "Out[13]:\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "The value of feature_names is a list of strings, giving the description of each feature:\n",
      "In[14]:\n",
      "print(\"Feature names: \\n{}\".format(iris_dataset['feature_names']))\n",
      "Out[14]:\n",
      "Feature names:\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n",
      " 'petal width (cm)']\n",
      "The data itself is contained in the target and data fields. data contains the numeric\n",
      "measurements of sepal length, sepal width, petal length, and petal width in a NumPy\n",
      "array:\n",
      "A First Application: Classifying Iris Species | 15\n",
      "In[15]:\n",
      "print(\"Type of data: {}\".format(type(iris_dataset['data'])))\n",
      "Out[15]:\n",
      "Type of data: <class 'numpy.ndarray'>\n",
      "The rows in the data array correspond to flowers, while the columns represent the\n",
      "four measurements that were taken for each flower:\n",
      "In[16]:\n",
      "print(\"Shape of data: {}\".format(iris_dataset['data'].shape))\n",
      "Out[16]:\n",
      "Shape of data: (150, 4)\n",
      "We see that the array contains measurements for 150 different flowers. Remember\n",
      "that the individual items are called samples in machine learning, and their properties\n",
      "are called features. The shape of the data array is the number of samples multiplied by\n",
      "the number of features. This is a convention in scikit-learn, and your data will\n",
      "always be assumed to be in this shape. Here are the feature values for the first five\n",
      "samples:\n",
      "In[17]:\n",
      "print(\"First five columns of data:\\n{}\".format(iris_dataset['data'][:5]))\n",
      "Out[17]:\n",
      "First five columns of data:\n",
      "[[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 4.6  3.1  1.5  0.2]\n",
      " [ 5.   3.6  1.4  0.2]]\n",
      "From this data, we can see that all of the first five flowers have a petal width of 0.2 cm\n",
      "and that the first flower has the longest sepal, at 5.1 cm.\n",
      "The target array contains the species of each of the flowers that were measured, also\n",
      "as a NumPy array:\n",
      "In[18]:\n",
      "print(\"Type of target: {}\".format(type(iris_dataset['target'])))\n",
      "Out[18]:\n",
      "Type of target: <class 'numpy.ndarray'>\n",
      "target is a one-dimensional array, with one entry per flower:\n",
      "16 | Chapter 1: Introduction\n",
      "In[19]:\n",
      "print(\"Shape of target: {}\".format(iris_dataset['target'].shape))\n",
      "Out[19]:\n",
      "Shape of target: (150,)\n",
      "The species are encoded as integers from 0 to 2:\n",
      "In[20]:\n",
      "print(\"Target:\\n{}\".format(iris_dataset['target']))\n",
      "Out[20]:\n",
      "Target:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "The meanings of the numbers are given by the iris['target_names'] array:\n",
      "0 means setosa, 1 means versicolor, and 2 means virginica.\n",
      "Measuring Success: Training and Testing Data\n",
      "We want to build a machine learning model from this data that can predict the spe‐\n",
      "cies of iris for a new set of measurements. But before we can apply our model to new\n",
      "measurements, we need to know whether it actually works—that is, whether we\n",
      "should trust its predictions.\n",
      "Unfortunately, we cannot use the data we used to build the model to evaluate it. This\n",
      "is because our model can always simply remember the whole training set, and will\n",
      "therefore always predict the correct label for any point in the training set. This\n",
      "“remembering” does not indicate to us whether our model will generalize well (in\n",
      "other words, whether it will also perform well on new data).\n",
      "To assess the model’s performance, we show it new data (data that it hasn’t seen\n",
      "before) for which we have labels. This is usually done by splitting the labeled data we\n",
      "have collected (here, our 150 flower measurements) into two parts. One part of the\n",
      "data is used to build our machine learning model, and is called the training data or\n",
      "training set. The rest of the data will be used to assess how well the model works; this\n",
      "is called the test data, test set, or hold-out set.\n",
      "scikit-learn contains a function that shuffles the dataset and splits it for you: the\n",
      "train_test_split function. This function extracts 75% of the rows in the data as the\n",
      "training set, together with the corresponding labels for this data. The remaining 25%\n",
      "of the data, together with the remaining labels, is declared as the test set. Deciding\n",
      "A First Application: Classifying Iris Species | 17\n",
      "how much data you want to put into the training and the test set respectively is some‐\n",
      "what arbitrary, but using a test set containing 25% of the data is a good rule of thumb.\n",
      "In scikit-learn, data is usually denoted with a capital X, while labels are denoted by\n",
      "a lowercase y. This is inspired by the standard formulation f(x)=y in mathematics,\n",
      "where x is the input to a function and y is the output. Following more conventions\n",
      "from mathematics, we use a capital X because the data is a two-dimensional array (a\n",
      "matrix) and a lowercase y because the target is a one-dimensional array (a vector).\n",
      "Let’s call train_test_split on our data and assign the outputs using this nomencla‐\n",
      "ture:\n",
      "In[21]:\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    iris_dataset['data'], iris_dataset['target'], random_state=0)\n",
      "Before making the split, the train_test_split function shuffles the dataset using a\n",
      "pseudorandom number generator. If we just took the last 25% of the data as a test set,\n",
      "all the data points would have the label 2, as the data points are sorted by the label\n",
      "(see the output for iris['target'] shown earlier). Using a test set containing only\n",
      "one of the three classes would not tell us much about how well our model generalizes,\n",
      "so we shuffle our data to make sure the test data contains data from all classes.\n",
      "To make sure that we will get the same output if we run the same function several\n",
      "times, we provide the pseudorandom number generator with a fixed seed using the\n",
      "random_state parameter. This will make the outcome deterministic, so this line will\n",
      "always have the same outcome. We will always fix the random_state in this way when\n",
      "using randomized procedures in this book.\n",
      "The output of the train_test_split function is X_train, X_test, y_train, and\n",
      "y_test, which are all NumPy arrays. X_train contains 75% of the rows of the dataset,\n",
      "and X_test contains the remaining 25%:\n",
      "In[22]:\n",
      "print(\"X_train shape: {}\".format(X_train.shape))\n",
      "print(\"y_train shape: {}\".format(y_train.shape))\n",
      "Out[22]:\n",
      "X_train shape: (112, 4)\n",
      "y_train shape: (112,)\n",
      "18 | Chapter 1: Introduction\n",
      "In[23]:\n",
      "print(\"X_test shape: {}\".format(X_test.shape))\n",
      "print(\"y_test shape: {}\".format(y_test.shape))\n",
      "Out[23]:\n",
      "X_test shape: (38, 4)\n",
      "y_test shape: (38,)\n",
      "First Things First: Look at Your Data\n",
      "Before building a machine learning model it is often a good idea to inspect the data,\n",
      "to see if the task is easily solvable without machine learning, or if the desired infor‐\n",
      "mation might not be contained in the data.\n",
      "Additionally, inspecting your data is a good way to find abnormalities and peculiari‐\n",
      "ties. Maybe some of your irises were measured using inches and not centimeters, for\n",
      "example. In the real world, inconsistencies in the data and unexpected measurements\n",
      "are very common.\n",
      "One of the best ways to inspect data is to visualize it. One way to do this is by using a\n",
      "scatter plot. A scatter plot of the data puts one feature along the x-axis and another\n",
      "along the y-axis, and draws a dot for each data point. Unfortunately, computer\n",
      "screens have only two dimensions, which allows us to plot only two (or maybe three)\n",
      "features at a time. It is difficult to plot datasets with more than three features this way.\n",
      "One way around this problem is to do a pair plot, which looks at all possible pairs of\n",
      "features. If you have a small number of features, such as the four we have here, this is\n",
      "quite reasonable. Y ou should keep in mind, however, that a pair plot does not show\n",
      "the interaction of all of features at once, so some interesting aspects of the data may\n",
      "not be revealed when visualizing it this way.\n",
      "Figure 1-3 is a pair plot of the features in the training set. The data points are colored\n",
      "according to the species the iris belongs to. To create the plot, we first convert the\n",
      "NumPy array into a pandas DataFrame. pandas has a function to create pair plots\n",
      "called scatter_matrix. The diagonal of this matrix is filled with histograms of each\n",
      "feature:\n",
      "In[24]:\n",
      "# create dataframe from data in X_train\n",
      "# label the columns using the strings in iris_dataset.feature_names\n",
      "iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\n",
      "# create a scatter matrix from the dataframe, color by y_train\n",
      "grr = pd.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker='o',\n",
      "                        hist_kwds={'bins': 20}, s=60, alpha=.8, cmap=mglearn.cm3)\n",
      "A First Application: Classifying Iris Species | 19\n",
      "Figure 1-3. Pair plot of the Iris dataset, colored by class label\n",
      "From the plots, we can see that the three classes seem to be relatively well separated\n",
      "using the sepal and petal measurements. This means that a machine learning model\n",
      "will likely be able to learn to separate them.\n",
      "Building Your First Model: k-Nearest Neighbors\n",
      "Now we can start building the actual machine learning model. There are many classi‐\n",
      "fication algorithms in scikit-learn that we could use. Here we will use a k-nearest\n",
      "neighbors classifier, which is easy to understand. Building this model only consists of\n",
      "storing the training set. To make a prediction for a new data point, the algorithm\n",
      "finds the point in the training set that is closest to the new point. Then it assigns the\n",
      "label of this training point to the new data point.\n",
      "20 | Chapter 1: Introduction\n",
      "The k in k-nearest neighbors signifies that instead of using only the closest neighbor\n",
      "to the new data point, we can consider any fixed number k of neighbors in the train‐\n",
      "ing (for example, the closest three or five neighbors). Then, we can make a prediction\n",
      "using the majority class among these neighbors. We will go into more detail about\n",
      "this in Chapter 2; for now, we’ll use only a single neighbor.\n",
      "All machine learning models in scikit-learn are implemented in their own classes,\n",
      "which are called Estimator classes. The k-nearest neighbors classification algorithm\n",
      "is implemented in the KNeighborsClassifier class in the neighbors module. Before\n",
      "we can use the model, we need to instantiate the class into an object. This is when we\n",
      "will set any parameters of the model. The most important parameter of KNeighbor\n",
      "sClassifier is the number of neighbors, which we will set to 1:\n",
      "In[25]:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "knn = KNeighborsClassifier(n_neighbors=1)\n",
      "The knn object encapsulates the algorithm that will be used to build the model from\n",
      "the training data, as well the algorithm to make predictions on new data points. It will\n",
      "also hold the information that the algorithm has extracted from the training data. In\n",
      "the case of KNeighborsClassifier, it will just store the training set.\n",
      "To build the model on the training set, we call the fit method of the knn object,\n",
      "which takes as arguments the NumPy array X_train containing the training data and\n",
      "the NumPy array y_train of the corresponding training labels:\n",
      "In[26]:\n",
      "knn.fit(X_train, y_train)\n",
      "Out[26]:\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform')\n",
      "The fit method returns the knn object itself (and modifies it in place), so we get a\n",
      "string representation of our classifier. The representation shows us which parameters\n",
      "were used in creating the model. Nearly all of them are the default values, but you can\n",
      "also find n_neighbors=1, which is the parameter that we passed. Most models in\n",
      "scikit-learn have many parameters, but the majority of them are either speed opti‐\n",
      "mizations or for very special use cases. Y ou don’t have to worry about the other\n",
      "parameters shown in this representation. Printing a scikit-learn model can yield\n",
      "very long strings, but don’t be intimidated by these. We will cover all the important\n",
      "parameters in Chapter 2. In the remainder of this book, we will not show the output\n",
      "of fit because it doesn’t contain any new information.\n",
      "A First Application: Classifying Iris Species | 21\n",
      "Making Predictions\n",
      "We can now make predictions using this model on new data for which we might not\n",
      "know the correct labels. Imagine we found an iris in the wild with a sepal length of\n",
      "5 cm, a sepal width of 2.9 cm, a petal length of 1 cm, and a petal width of 0.2 cm.\n",
      "What species of iris would this be? We can put this data into a NumPy array, again by\n",
      "calculating the shape—that is, the number of samples (1) multiplied by the number of\n",
      "features (4):\n",
      "In[27]:\n",
      "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
      "print(\"X_new.shape: {}\".format(X_new.shape))\n",
      "Out[27]:\n",
      "X_new.shape: (1, 4)\n",
      "Note that we made the measurements of this single flower into a row in a two-\n",
      "dimensional NumPy array, as scikit-learn always expects two-dimensional arrays\n",
      "for the data.\n",
      "To make a prediction, we call the predict method of the knn object:\n",
      "In[28]:\n",
      "prediction = knn.predict(X_new)\n",
      "print(\"Prediction: {}\".format(prediction))\n",
      "print(\"Predicted target name: {}\".format(\n",
      "       iris_dataset['target_names'][prediction]))\n",
      "Out[28]:\n",
      "Prediction: [0]\n",
      "Predicted target name: ['setosa']\n",
      "Our model predicts that this new iris belongs to the class 0, meaning its species is\n",
      "setosa. But how do we know whether we can trust our model? We don’t know the cor‐\n",
      "rect species of this sample, which is the whole point of building the model!\n",
      "Evaluating the Model\n",
      "This is where the test set that we created earlier comes in. This data was not used to\n",
      "build the model, but we do know what the correct species is for each iris in the test\n",
      "set.\n",
      "Therefore, we can make a prediction for each iris in the test data and compare it\n",
      "against its label (the known species). We can measure how well the model works by\n",
      "computing the accuracy, which is the fraction of flowers for which the right species\n",
      "was predicted:\n",
      "22 | Chapter 1: Introduction\n",
      "In[29]:\n",
      "y_pred = knn.predict(X_test)\n",
      "print(\"Test set predictions:\\n {}\".format(y_pred))\n",
      "Out[29]:\n",
      "Test set predictions:\n",
      " [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0 2]\n",
      "In[30]:\n",
      "print(\"Test set score: {:.2f}\".format(np.mean(y_pred == y_test)))\n",
      "Out[30]:\n",
      "Test set score: 0.97\n",
      "We can also use the score method of the knn object, which will compute the test set\n",
      "accuracy for us:\n",
      "In[31]:\n",
      "print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\n",
      "Out[31]:\n",
      "Test set score: 0.97\n",
      "For this model, the test set accuracy is about 0.97, which means we made the right\n",
      "prediction for 97% of the irises in the test set. Under some mathematical assump‐\n",
      "tions, this means that we can expect our model to be correct 97% of the time for new\n",
      "irises. For our hobby botanist application, this high level of accuracy means that our\n",
      "model may be trustworthy enough to use. In later chapters we will discuss how we\n",
      "can improve performance, and what caveats there are in tuning a model.\n",
      "Summary and Outlook\n",
      "Let’s summarize what we learned in this chapter. We started with a brief introduction\n",
      "to machine learning and its applications, then discussed the distinction between\n",
      "supervised and unsupervised learning and gave an overview of the tools we’ll be\n",
      "using in this book. Then, we formulated the task of predicting which species of iris a\n",
      "particular flower belongs to by using physical measurements of the flower. We used a\n",
      "dataset of measurements that was annotated by an expert with the correct species to\n",
      "build our model, making this a supervised learning task. There were three possible\n",
      "species, setosa, versicolor, or virginica, which made the task a three-class classification\n",
      "problem. The possible species are called classes in the classification problem, and the\n",
      "species of a single iris is called its label.\n",
      "The Iris dataset consists of two NumPy arrays: one containing the data, which is\n",
      "referred to as X in scikit-learn, and one containing the correct or desired outputs,\n",
      "Summary and Outlook | 23\n",
      "which is called y. The array X is a two-dimensional array of features, with one row per\n",
      "data point and one column per feature. The array y is a one-dimensional array, which\n",
      "here contains one class label, an integer ranging from 0 to 2, for each of the samples.\n",
      "We split our dataset into a training set, to build our model, and a test set, to evaluate\n",
      "how well our model will generalize to new, previously unseen data.\n",
      "We chose the k-nearest neighbors classification algorithm, which makes predictions\n",
      "for a new data point by considering its closest neighbor(s) in the training set. This is\n",
      "implemented in the KNeighborsClassifier class, which contains the algorithm that\n",
      "builds the model as well as the algorithm that makes a prediction using the model.\n",
      "We instantiated the class, setting parameters. Then we built the model by calling the\n",
      "fit method, passing the training data ( X_train) and training outputs ( y_train) as\n",
      "parameters. We evaluated the model using the score method, which computes the\n",
      "accuracy of the model. We applied the score method to the test set data and the test\n",
      "set labels and found that our model is about 97% accurate, meaning it is correct 97%\n",
      "of the time on the test set.\n",
      "This gave us the confidence to apply the model to new data (in our example, new\n",
      "flower measurements) and trust that the model will be correct about 97% of the time.\n",
      "Here is a summary of the code needed for the whole training and evaluation\n",
      "procedure:\n",
      "In[32]:\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    iris_dataset['data'], iris_dataset['target'], random_state=0)\n",
      "knn = KNeighborsClassifier(n_neighbors=1)\n",
      "knn.fit(X_train, y_train)\n",
      "print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\n",
      "Out[32]:\n",
      "Test set score: 0.97\n",
      "This snippet contains the core code for applying any machine learning algorithm\n",
      "using scikit-learn. The fit, predict, and score methods are the common inter‐\n",
      "face to supervised models in scikit-learn, and with the concepts introduced in this\n",
      "chapter, you can apply these models to many machine learning tasks. In the next\n",
      "chapter, we will go into more depth about the different kinds of supervised models in\n",
      "scikit-learn and how to apply them successfully.\n",
      "24 | Chapter 1: Introduction\n",
      "CHAPTER 2\n",
      "Supervised Learning\n",
      "As we mentioned earlier, supervised machine learning is one of the most commonly\n",
      "used and successful types of machine learning. In this chapter, we will describe super‐\n",
      "vised learning in more detail and explain several popular supervised learning algo‐\n",
      "rithms. We already saw an application of supervised machine learning in Chapter 1:\n",
      "classifying iris flowers into several species using physical measurements of the\n",
      "flowers.\n",
      "Remember that supervised learning is used whenever we want to predict a certain\n",
      "outcome from a given input, and we have examples of input/output pairs. We build a\n",
      "machine learning model from these input/output pairs, which comprise our training\n",
      "set. Our goal is to make accurate predictions for new, never-before-seen data. Super‐\n",
      "vised learning often requires human effort to build the training set, but afterward\n",
      "automates and often speeds up an otherwise laborious or infeasible task.\n",
      "Classification and Regression\n",
      "There are two major types of supervised machine learning problems, called classifica‐\n",
      "tion and regression.\n",
      "In classification, the goal is to predict a class label, which is a choice from a predefined\n",
      "list of possibilities. In Chapter 1 we used the example of classifying irises into one of\n",
      "three possible species. Classification is sometimes separated into binary classification,\n",
      "which is the special case of distinguishing between exactly two classes, and multiclass\n",
      "classification, which is classification between more than two classes. Y ou can think of\n",
      "binary classification as trying to answer a yes/no question. Classifying emails as\n",
      "either spam or not spam is an example of a binary classification problem. In this\n",
      "binary classification task, the yes/no question being asked would be “Is this email\n",
      "spam?”\n",
      "25\n",
      "1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\n",
      "In binary classification we often speak of one class being the posi‐\n",
      "tive class and the other class being the negative class. Here, positive\n",
      "doesn’t represent having benefit or value, but rather what the object\n",
      "of the study is. So, when looking for spam, “positive” could mean\n",
      "the spam class. Which of the two classes is called positive is often a\n",
      "subjective matter, and specific to the domain.\n",
      "The iris example, on the other hand, is an example of a multiclass classification prob‐\n",
      "lem. Another example is predicting what language a website is in from the text on the\n",
      "website. The classes here would be a pre-defined list of possible languages.\n",
      "For regression tasks, the goal is to predict a continuous number, or a floating-point\n",
      "number in programming terms (or real number in mathematical terms). Predicting a\n",
      "person’s annual income from their education, their age, and where they live is an\n",
      "example of a regression task. When predicting income, the predicted value is an\n",
      "amount, and can be any number in a given range. Another example of a regression\n",
      "task is predicting the yield of a corn farm given attributes such as previous yields,\n",
      "weather, and number of employees working on the farm. The yield again can be an\n",
      "arbitrary number.\n",
      "An easy way to distinguish between classification and regression tasks is to ask\n",
      "whether there is some kind of continuity in the output. If there is continuity between\n",
      "possible outcomes, then the problem is a regression problem. Think about predicting\n",
      "annual income. There is a clear continuity in the output. Whether a person makes\n",
      "$40,000 or $40,001 a year does not make a tangible difference, even though these are\n",
      "different amounts of money; if our algorithm predicts $39,999 or $40,001 when it\n",
      "should have predicted $40,000, we don’t mind that much.\n",
      "By contrast, for the task of recognizing the language of a website (which is a classifi‐\n",
      "cation problem), there is no matter of degree. A website is in one language, or it is in\n",
      "another. There is no continuity between languages, and there is no language that is\n",
      "between English and French.1\n",
      "Generalization, Overfitting, and Underfitting\n",
      "In supervised learning, we want to build a model on the training data and then be\n",
      "able to make accurate predictions on new, unseen data that has the same characteris‐\n",
      "tics as the training set that we used. If a model is able to make accurate predictions on\n",
      "unseen data, we say it is able to generalize from the training set to the test set. We\n",
      "want to build a model that is able to generalize as accurately as possible.\n",
      "26 | Chapter 2: Supervised Learning\n",
      "2 In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a\n",
      "boat from us yet, they might have bought one from someone else, or they may still be saving and plan to buy\n",
      "one in the future.\n",
      "Usually we build a model in such a way that it can make accurate predictions on the\n",
      "training set. If the training and test sets have enough in common, we expect the\n",
      "model to also be accurate on the test set. However, there are some cases where this\n",
      "can go wrong. For example, if we allow ourselves to build very complex models, we\n",
      "can always be as accurate as we like on the training set.\n",
      "Let’s take a look at a made-up example to illustrate this point. Say a novice data scien‐\n",
      "tist wants to predict whether a customer will buy a boat, given records of previous\n",
      "boat buyers and customers who we know are not interested in buying a boat. 2 The\n",
      "goal is to send out promotional emails to people who are likely to actually make a\n",
      "purchase, but not bother those customers who won’t be interested.\n",
      "Suppose we have the customer records shown in Table 2-1.\n",
      "Table 2-1. Example data about customers\n",
      "Age Number of \n",
      "cars owned\n",
      "Owns house Number of children Marital status Owns a dog Bought a boat\n",
      "66 1 yes 2 widowed no yes\n",
      "52 2 yes 3 married no yes\n",
      "22 0 no 0 married yes no\n",
      "25 1 no 1 single no no\n",
      "44 0 no 2 divorced yes no\n",
      "39 1 yes 2 married yes no\n",
      "26 1 no 2 single no no\n",
      "40 3 yes 1 married yes no\n",
      "53 2 yes 2 divorced no yes\n",
      "64 2 yes 3 divorced no no\n",
      "58 2 yes 2 married yes yes\n",
      "33 1 no 1 single no no\n",
      "After looking at the data for a while, our novice data scientist comes up with the fol‐\n",
      "lowing rule: “If the customer is older than 45, and has less than 3 children or is not\n",
      "divorced, then they want to buy a boat. ” When asked how well this rule of his does,\n",
      "our data scientist answers, “It’s 100 percent accurate!” And indeed, on the data that is\n",
      "in the table, the rule is perfectly accurate. There are many possible rules we could\n",
      "come up with that would explain perfectly if someone in this dataset wants to buy a\n",
      "boat. No age appears twice in the data, so we could say people who are 66, 52, 53, or\n",
      "Generalization, Overfitting, and Underfitting | 27\n",
      "3 And also provably, with the right math.\n",
      "58 years old want to buy a boat, while all others don’t. While we can make up many\n",
      "rules that work well on this data, remember that we are not interested in making pre‐\n",
      "dictions for this dataset; we already know the answers for these customers. We want\n",
      "to know if new customers are likely to buy a boat. We therefore want to find a rule that\n",
      "will work well for new customers, and achieving 100 percent accuracy on the training\n",
      "set does not help us there. We might not expect that the rule our data scientist came\n",
      "up with will work very well on new customers. It seems too complex, and it is sup‐\n",
      "ported by very little data. For example, the “or is not divorced” part of the rule hinges\n",
      "on a single customer.\n",
      "The only measure of whether an algorithm will perform well on new data is the eval‐\n",
      "uation on the test set. However, intuitively 3 we expect simple models to generalize\n",
      "better to new data. If the rule was “People older than 50 want to buy a boat, ” and this\n",
      "would explain the behavior of all the customers, we would trust it more than the rule\n",
      "involving children and marital status in addition to age. Therefore, we always want to\n",
      "find the simplest model. Building a model that is too complex for the amount of\n",
      "information we have, as our novice data scientist did, is called overfitting. Overfitting\n",
      "occurs when you fit a model too closely to the particularities of the training set and\n",
      "obtain a model that works well on the training set but is not able to generalize to new\n",
      "data. On the other hand, if your model is too simple—say, “Everybody who owns a\n",
      "house buys a boat”—then you might not be able to capture all the aspects of and vari‐\n",
      "ability in the data, and your model will do badly even on the training set. Choosing\n",
      "too simple a model is called underfitting.\n",
      "The more complex we allow our model to be, the better we will be able to predict on\n",
      "the training data. However, if our model becomes too complex, we start focusing too\n",
      "much on each individual data point in our training set, and the model will not gener‐\n",
      "alize well to new data.\n",
      "There is a sweet spot in between that will yield the best generalization performance.\n",
      "This is the model we want to find.\n",
      "The trade-off between overfitting and underfitting is illustrated in Figure 2-1.\n",
      "28 | Chapter 2: Supervised Learning\n",
      "Figure 2-1. Trade-off of model complexity against training and test accuracy\n",
      "Relation of Model Complexity to Dataset Size\n",
      "It’s important to note that model complexity is intimately tied to the variation of\n",
      "inputs contained in your training dataset: the larger variety of data points your data‐\n",
      "set contains, the more complex a model you can use without overfitting. Usually, col‐\n",
      "lecting more data points will yield more variety, so larger datasets allow building\n",
      "more complex models. However, simply duplicating the same data points or collect‐\n",
      "ing very similar data will not help.\n",
      "Going back to the boat selling example, if we saw 10,000 more rows of customer data,\n",
      "and all of them complied with the rule “If the customer is older than 45, and has less\n",
      "than 3 children or is not divorced, then they want to buy a boat, ” we would be much\n",
      "more likely to believe this to be a good rule than when it was developed using only\n",
      "the 12 rows in Table 2-1.\n",
      "Having more data and building appropriately more complex models can often work\n",
      "wonders for supervised learning tasks. In this book, we will focus on working with\n",
      "datasets of fixed sizes. In the real world, you often have the ability to decide how\n",
      "much data to collect, which might be more beneficial than tweaking and tuning your\n",
      "model. Never underestimate the power of more data.\n",
      "Supervised Machine Learning Algorithms\n",
      "We will now review the most popular machine learning algorithms and explain how\n",
      "they learn from data and how they make predictions. We will also discuss how the\n",
      "concept of model complexity plays out for each of these models, and provide an over‐\n",
      "Supervised Machine Learning Algorithms | 29\n",
      "4 Discussing all of them is beyond the scope of the book, and we refer you to the scikit-learn documentation\n",
      "for more details.\n",
      "view of how each algorithm builds a model. We will examine the strengths and weak‐\n",
      "nesses of each algorithm, and what kind of data they can best be applied to. We will\n",
      "also explain the meaning of the most important parameters and options. 4 Many algo‐\n",
      "rithms have a classification and a regression variant, and we will describe both.\n",
      "It is not necessary to read through the descriptions of each algorithm in detail, but\n",
      "understanding the models will give you a better feeling for the different ways\n",
      "machine learning algorithms can work. This chapter can also be used as a reference\n",
      "guide, and you can come back to it when you are unsure about the workings of any of\n",
      "the algorithms.\n",
      "Some Sample Datasets\n",
      "We will use several datasets to illustrate the different algorithms. Some of the datasets\n",
      "will be small and synthetic (meaning made-up), designed to highlight particular\n",
      "aspects of the algorithms. Other datasets will be large, real-world examples.\n",
      "An example of a synthetic two-class classification dataset is the forge dataset, which\n",
      "has two features. The following code creates a scatter plot ( Figure 2-2) visualizing all\n",
      "of the data points in this dataset. The plot has the first feature on the x-axis and the\n",
      "second feature on the y-axis. As is always the case in scatter plots, each data point is\n",
      "represented as one dot. The color and shape of the dot indicates its class:\n",
      "In[2]:\n",
      "# generate dataset\n",
      "X, y = mglearn.datasets.make_forge()\n",
      "# plot dataset\n",
      "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
      "plt.legend([\"Class 0\", \"Class 1\"], loc=4)\n",
      "plt.xlabel(\"First feature\")\n",
      "plt.ylabel(\"Second feature\")\n",
      "print(\"X.shape: {}\".format(X.shape))\n",
      "Out[2]:\n",
      "X.shape: (26, 2)\n",
      "30 | Chapter 2: Supervised Learning\n",
      "Figure 2-2. Scatter plot of the forge dataset\n",
      "As you can see from X.shape, this dataset consists of 26 data points, with 2 features.\n",
      "To illustrate regression algorithms, we will use the synthetic wave dataset. The wave\n",
      "dataset has a single input feature and a continuous target variable (or response) that\n",
      "we want to model. The plot created here ( Figure 2-3) shows the single feature on the\n",
      "x-axis and the regression target (the output) on the y-axis:\n",
      "In[3]:\n",
      "X, y = mglearn.datasets.make_wave(n_samples=40)\n",
      "plt.plot(X, y, 'o')\n",
      "plt.ylim(-3, 3)\n",
      "plt.xlabel(\"Feature\")\n",
      "plt.ylabel(\"Target\")\n",
      "Supervised Machine Learning Algorithms | 31\n",
      "Figure 2-3. Plot of the wave dataset, with the x-axis showing the feature and the y-axis\n",
      "showing the regression target\n",
      "We are using these very simple, low-dimensional datasets because we can easily visu‐\n",
      "alize them—a printed page has two dimensions, so data with more than two features\n",
      "is hard to show. Any intuition derived from datasets with few features (also called\n",
      "low-dimensional datasets) might not hold in datasets with many features ( high-\n",
      "dimensional datasets). As long as you keep that in mind, inspecting algorithms on\n",
      "low-dimensional datasets can be very instructive.\n",
      "We will complement these small synthetic datasets with two real-world datasets that\n",
      "are included in scikit-learn. One is the Wisconsin Breast Cancer dataset ( cancer,\n",
      "for short), which records clinical measurements of breast cancer tumors. Each tumor\n",
      "is labeled as “benign” (for harmless tumors) or “malignant” (for cancerous tumors),\n",
      "and the task is to learn to predict whether a tumor is malignant based on the meas‐\n",
      "urements of the tissue.\n",
      "The data can be loaded using the load_breast_cancer function from scikit-learn:\n",
      "In[4]:\n",
      "from sklearn.datasets import load_breast_cancer\n",
      "cancer = load_breast_cancer()\n",
      "print(\"cancer.keys(): \\n{}\".format(cancer.keys()))\n",
      "32 | Chapter 2: Supervised Learning\n",
      "Out[4]:\n",
      "cancer.keys():\n",
      "dict_keys(['feature_names', 'data', 'DESCR', 'target', 'target_names'])\n",
      "Datasets that are included in scikit-learn are usually stored as\n",
      "Bunch objects, which contain some information about the dataset\n",
      "as well as the actual data. All you need to know about Bunch objects\n",
      "is that they behave like dictionaries, with the added benefit that you\n",
      "can access values using a dot (as in bunch.key instead of\n",
      "bunch['key']).\n",
      "The dataset consists of 569 data points, with 30 features each:\n",
      "In[5]:\n",
      "print(\"Shape of cancer data: {}\".format(cancer.data.shape))\n",
      "Out[5]:\n",
      "Shape of cancer data: (569, 30)\n",
      "Of these 569 data points, 212 are labeled as malignant and 357 as benign:\n",
      "In[6]:\n",
      "print(\"Sample counts per class:\\n{}\".format(\n",
      "      {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))\n",
      "Out[6]:\n",
      "Sample counts per class:\n",
      "{'benign': 357, 'malignant': 212}\n",
      "To get a description of the semantic meaning of each feature, we can have a look at\n",
      "the feature_names attribute:\n",
      "In[7]:\n",
      "print(\"Feature names:\\n{}\".format(cancer.feature_names))\n",
      "Out[7]:\n",
      "Feature names:\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Supervised Machine Learning Algorithms | 33\n",
      "5 This is called the binomial coefficient, which is the number of combinations of k elements that can be selected\n",
      "from a set of n elements. Often this is written as nk and spoken as “n choose k”—in this case, “13 choose 2. ”\n",
      "Y ou can find out more about the data by reading cancer.DESCR if you are interested.\n",
      "We will also be using a real-world regression dataset, the Boston Housing dataset.\n",
      "The task associated with this dataset is to predict the median value of homes in sev‐\n",
      "eral Boston neighborhoods in the 1970s, using information such as crime rate, prox‐\n",
      "imity to the Charles River, highway accessibility, and so on. The dataset contains 506\n",
      "data points, described by 13 features:\n",
      "In[8]:\n",
      "from sklearn.datasets import load_boston\n",
      "boston = load_boston()\n",
      "print(\"Data shape: {}\".format(boston.data.shape))\n",
      "Out[8]:\n",
      "Data shape: (506, 13)\n",
      "Again, you can get more information about the dataset by reading the DESCR attribute\n",
      "of boston. For our purposes here, we will actually expand this dataset by not only\n",
      "considering these 13 measurements as input features, but also looking at all products\n",
      "(also called interactions) between features. In other words, we will not only consider\n",
      "crime rate and highway accessibility as features, but also the product of crime rate\n",
      "and highway accessibility. Including derived feature like these is called feature engi‐\n",
      "neering, which we will discuss in more detail in Chapter 4. This derived dataset can be\n",
      "loaded using the load_extended_boston function:\n",
      "In[9]:\n",
      "X, y = mglearn.datasets.load_extended_boston()\n",
      "print(\"X.shape: {}\".format(X.shape))\n",
      "Out[9]:\n",
      "X.shape: (506, 104)\n",
      "The resulting 104 features are the 13 original features together with the 91 possible\n",
      "combinations of two features within those 13.5\n",
      "We will use these datasets to explain and illustrate the properties of the different\n",
      "machine learning algorithms. But for now, let’s get to the algorithms themselves.\n",
      "First, we will revisit the k-nearest neighbors (k-NN) algorithm that we saw in the pre‐\n",
      "vious chapter.\n",
      "34 | Chapter 2: Supervised Learning\n",
      "k-Nearest Neighbors\n",
      "The k-NN algorithm is arguably the simplest machine learning algorithm. Building\n",
      "the model consists only of storing the training dataset. To make a prediction for a\n",
      "new data point, the algorithm finds the closest data points in the training dataset—its\n",
      "“nearest neighbors. ”\n",
      "k-Neighbors classification\n",
      "In its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐\n",
      "bor, which is the closest training data point to the point we want to make a prediction\n",
      "for. The prediction is then simply the known output for this training point. Figure 2-4\n",
      "illustrates this for the case of classification on the forge dataset:\n",
      "In[10]:\n",
      "mglearn.plots.plot_knn_classification(n_neighbors=1)\n",
      "Figure 2-4. Predictions made by the one-nearest-neighbor model on the forge dataset\n",
      "Here, we added three new data points, shown as stars. For each of them, we marked\n",
      "the closest point in the training set. The prediction of the one-nearest-neighbor algo‐\n",
      "rithm is the label of that point (shown by the color of the cross).\n",
      "Supervised Machine Learning Algorithms | 35\n",
      "Instead of considering only the closest neighbor, we can also consider an arbitrary\n",
      "number, k, of neighbors. This is where the name of the k-nearest neighbors algorithm\n",
      "comes from. When considering more than one neighbor, we use voting to assign a\n",
      "label. This means that for each test point, we count how many neighbors belong to\n",
      "class 0 and how many neighbors belong to class 1. We then assign the class that is\n",
      "more frequent: in other words, the majority class among the k-nearest neighbors. The\n",
      "following example (Figure 2-5) uses the three closest neighbors:\n",
      "In[11]:\n",
      "mglearn.plots.plot_knn_classification(n_neighbors=3)\n",
      "Figure 2-5. Predictions made by the three-nearest-neighbors model on the forge dataset\n",
      "Again, the prediction is shown as the color of the cross. Y ou can see that the predic‐\n",
      "tion for the new data point at the top left is not the same as the prediction when we\n",
      "used only one neighbor.\n",
      "While this illustration is for a binary classification problem, this method can be\n",
      "applied to datasets with any number of classes. For more classes, we count how many\n",
      "neighbors belong to each class and again predict the most common class.\n",
      "Now let’s look at how we can apply the k-nearest neighbors algorithm using scikit-\n",
      "learn. First, we split our data into a training and a test set so we can evaluate general‐\n",
      "ization performance, as discussed in Chapter 1:\n",
      "36 | Chapter 2: Supervised Learning\n",
      "In[12]:\n",
      "from sklearn.model_selection import train_test_split\n",
      "X, y = mglearn.datasets.make_forge()\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
      "Next, we import and instantiate the class. This is when we can set parameters, like the\n",
      "number of neighbors to use. Here, we set it to 3:\n",
      "In[13]:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "clf = KNeighborsClassifier(n_neighbors=3)\n",
      "Now, we fit the classifier using the training set. For KNeighborsClassifier this\n",
      "means storing the dataset, so we can compute neighbors during prediction:\n",
      "In[14]:\n",
      "clf.fit(X_train, y_train)\n",
      "To make predictions on the test data, we call the predict method. For each data point\n",
      "in the test set, this computes its nearest neighbors in the training set and finds the\n",
      "most common class among these:\n",
      "In[15]:\n",
      "print(\"Test set predictions: {}\".format(clf.predict(X_test)))\n",
      "Out[15]:\n",
      "Test set predictions: [1 0 1 0 1 0 0]\n",
      "To evaluate how well our model generalizes, we can call the score method with the\n",
      "test data together with the test labels:\n",
      "In[16]:\n",
      "print(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))\n",
      "Out[16]:\n",
      "Test set accuracy: 0.86\n",
      "We see that our model is about 86% accurate, meaning the model predicted the class\n",
      "correctly for 86% of the samples in the test dataset.\n",
      "Analyzing KNeighborsClassifier\n",
      "For two-dimensional datasets, we can also illustrate the prediction for all possible test\n",
      "points in the xy-plane. We color the plane according to the class that would be\n",
      "assigned to a point in this region. This lets us view the decision boundary, which is the\n",
      "divide between where the algorithm assigns class 0 versus where it assigns class 1.\n",
      "Supervised Machine Learning Algorithms | 37\n",
      "The following code produces the visualizations of the decision boundaries for one,\n",
      "three, and nine neighbors shown in Figure 2-6:\n",
      "In[17]:\n",
      "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
      "for n_neighbors, ax in zip([1, 3, 9], axes):\n",
      "    # the fit method returns the object self, so we can instantiate\n",
      "    # and fit in one line\n",
      "    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\n",
      "    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\n",
      "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
      "    ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n",
      "    ax.set_xlabel(\"feature 0\")\n",
      "    ax.set_ylabel(\"feature 1\")\n",
      "axes[0].legend(loc=3)\n",
      "Figure 2-6. Decision boundaries created by the nearest neighbors model for different val‐\n",
      "ues of n_neighbors\n",
      "As you can see on the left in the figure, using a single neighbor results in a decision\n",
      "boundary that follows the training data closely. Considering more and more neigh‐\n",
      "bors leads to a smoother decision boundary. A smoother boundary corresponds to a\n",
      "simpler model. In other words, using few neighbors corresponds to high model com‐\n",
      "plexity (as shown on the right side of Figure 2-1), and using many neighbors corre‐\n",
      "sponds to low model complexity (as shown on the left side of Figure 2-1 ). If you\n",
      "consider the extreme case where the number of neighbors is the number of all data\n",
      "points in the training set, each test point would have exactly the same neighbors (all\n",
      "training points) and all predictions would be the same: the class that is most frequent\n",
      "in the training set.\n",
      "Let’s investigate whether we can confirm the connection between model complexity\n",
      "and generalization that we discussed earlier. We will do this on the real-world Breast\n",
      "Cancer dataset. We begin by splitting the dataset into a training and a test set. Then\n",
      "38 | Chapter 2: Supervised Learning\n",
      "we evaluate training and test set performance with different numbers of neighbors.\n",
      "The results are shown in Figure 2-7:\n",
      "In[18]:\n",
      "from sklearn.datasets import load_breast_cancer\n",
      "cancer = load_breast_cancer()\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    cancer.data, cancer.target, stratify=cancer.target, random_state=66)\n",
      "training_accuracy = []\n",
      "test_accuracy = []\n",
      "# try n_neighbors from 1 to 10\n",
      "neighbors_settings = range(1, 11)\n",
      "for n_neighbors in neighbors_settings:\n",
      "    # build the model\n",
      "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
      "    clf.fit(X_train, y_train)\n",
      "    # record training set accuracy\n",
      "    training_accuracy.append(clf.score(X_train, y_train))\n",
      "    # record generalization accuracy\n",
      "    test_accuracy.append(clf.score(X_test, y_test))\n",
      "plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
      "plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
      "plt.ylabel(\"Accuracy\")\n",
      "plt.xlabel(\"n_neighbors\")\n",
      "plt.legend()\n",
      "The plot shows the training and test set accuracy on the y-axis against the setting of\n",
      "n_neighbors on the x-axis. While real-world plots are rarely very smooth, we can still\n",
      "recognize some of the characteristics of overfitting and underfitting (note that\n",
      "because considering fewer neighbors corresponds to a more complex model, the plot\n",
      "is horizontally flipped relative to the illustration in Figure 2-1). Considering a single\n",
      "nearest neighbor, the prediction on the training set is perfect. But when more neigh‐\n",
      "bors are considered, the model becomes simpler and the training accuracy drops. The\n",
      "test set accuracy for using a single neighbor is lower than when using more neigh‐\n",
      "bors, indicating that using the single nearest neighbor leads to a model that is too\n",
      "complex. On the other hand, when considering 10 neighbors, the model is too simple\n",
      "and performance is even worse. The best performance is somewhere in the middle,\n",
      "using around six neighbors. Still, it is good to keep the scale of the plot in mind. The\n",
      "worst performance is around 88% accuracy, which might still be acceptable.\n",
      "Supervised Machine Learning Algorithms | 39\n",
      "Figure 2-7. Comparison of training and test accuracy as a function of n_neighbors\n",
      "k-neighbors regression\n",
      "There is also a regression variant of the k-nearest neighbors algorithm. Again, let’s\n",
      "start by using the single nearest neighbor, this time using the wave dataset. We’ve\n",
      "added three test data points as green stars on the x-axis. The prediction using a single\n",
      "neighbor is just the target value of the nearest neighbor. These are shown as blue stars\n",
      "in Figure 2-8:\n",
      "In[19]:\n",
      "mglearn.plots.plot_knn_regression(n_neighbors=1)\n",
      "40 | Chapter 2: Supervised Learning\n",
      "Figure 2-8. Predictions made by one-nearest-neighbor regression on the wave dataset\n",
      "Again, we can use more than the single closest neighbor for regression. When using\n",
      "multiple nearest neighbors, the prediction is the average, or mean, of the relevant\n",
      "neighbors (Figure 2-9):\n",
      "In[20]:\n",
      "mglearn.plots.plot_knn_regression(n_neighbors=3)\n",
      "Supervised Machine Learning Algorithms | 41\n",
      "Figure 2-9. Predictions made by three-nearest-neighbors regression on the wave dataset\n",
      "The k-nearest neighbors algorithm for regression is implemented in the KNeighbors\n",
      "Regressor class in scikit-learn. It’s used similarly to KNeighborsClassifier:\n",
      "In[21]:\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "X, y = mglearn.datasets.make_wave(n_samples=40)\n",
      "# split the wave dataset into a training and a test set\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
      "# instantiate the model and set the number of neighbors to consider to 3\n",
      "reg = KNeighborsRegressor(n_neighbors=3)\n",
      "# fit the model using the training data and training targets\n",
      "reg.fit(X_train, y_train)\n",
      "Now we can make predictions on the test set:\n",
      "In[22]:\n",
      "print(\"Test set predictions:\\n{}\".format(reg.predict(X_test)))\n",
      "42 | Chapter 2: Supervised Learning\n",
      "Out[22]:\n",
      "Test set predictions:\n",
      "[-0.054  0.357  1.137 -1.894 -1.139 -1.631  0.357  0.912 -0.447 -1.139]\n",
      "We can also evaluate the model using the score method, which for regressors returns\n",
      "the R2 score. The R2 score, also known as the coefficient of determination, is a meas‐\n",
      "ure of goodness of a prediction for a regression model, and yields a score between 0\n",
      "and 1. A value of 1 corresponds to a perfect prediction, and a value of 0 corresponds\n",
      "to a constant model that just predicts the mean of the training set responses, y_train:\n",
      "In[23]:\n",
      "print(\"Test set R^2: {:.2f}\".format(reg.score(X_test, y_test)))\n",
      "Out[23]:\n",
      "Test set R^2: 0.83\n",
      "Here, the score is 0.83, which indicates a relatively good model fit.\n",
      "Analyzing KNeighborsRegressor\n",
      "For our one-dimensional dataset, we can see what the predictions look like for all\n",
      "possible feature values (Figure 2-10). To do this, we create a test dataset consisting of\n",
      "many points on the line:\n",
      "In[24]:\n",
      "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
      "# create 1,000 data points, evenly spaced between -3 and 3\n",
      "line = np.linspace(-3, 3, 1000).reshape(-1, 1)\n",
      "for n_neighbors, ax in zip([1, 3, 9], axes):\n",
      "    # make predictions using 1, 3, or 9 neighbors\n",
      "    reg = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
      "    reg.fit(X_train, y_train)\n",
      "    ax.plot(line, reg.predict(line))\n",
      "    ax.plot(X_train, y_train, '^', c=mglearn.cm2(0), markersize=8)\n",
      "    ax.plot(X_test, y_test, 'v', c=mglearn.cm2(1), markersize=8)\n",
      "    ax.set_title(\n",
      "        \"{} neighbor(s)\\n train score: {:.2f} test score: {:.2f}\".format(\n",
      "            n_neighbors, reg.score(X_train, y_train),\n",
      "            reg.score(X_test, y_test)))\n",
      "    ax.set_xlabel(\"Feature\")\n",
      "    ax.set_ylabel(\"Target\")\n",
      "axes[0].legend([\"Model predictions\", \"Training data/target\",\n",
      "                \"Test data/target\"], loc=\"best\")\n",
      "Supervised Machine Learning Algorithms | 43\n",
      "Figure 2-10. Comparing predictions made by nearest neighbors regression for different\n",
      "values of n_neighbors\n",
      "As we can see from the plot, using only a single neighbor, each point in the training\n",
      "set has an obvious influence on the predictions, and the predicted values go through\n",
      "all of the data points. This leads to a very unsteady prediction. Considering more\n",
      "neighbors leads to smoother predictions, but these do not fit the training data as well.\n",
      "Strengths, weaknesses, and parameters\n",
      "In principle, there are two important parameters to the KNeighbors classifier: the\n",
      "number of neighbors and how you measure distance between data points. In practice,\n",
      "using a small number of neighbors like three or five often works well, but you should\n",
      "certainly adjust this parameter. Choosing the right distance measure is somewhat\n",
      "beyond the scope of this book. By default, Euclidean distance is used, which works\n",
      "well in many settings.\n",
      "One of the strengths of k-NN is that the model is very easy to understand, and often\n",
      "gives reasonable performance without a lot of adjustments. Using this algorithm is a\n",
      "good baseline method to try before considering more advanced techniques. Building\n",
      "the nearest neighbors model is usually very fast, but when your training set is very\n",
      "large (either in number of features or in number of samples) prediction can be slow.\n",
      "When using the k-NN algorithm, it’s important to preprocess your data (see Chap‐\n",
      "ter 3 ). This approach often does not perform well on datasets with many features\n",
      "(hundreds or more), and it does particularly badly with datasets where most features\n",
      "are 0 most of the time (so-called sparse datasets).\n",
      "So, while the nearest k-neighbors algorithm is easy to understand, it is not often used\n",
      "in practice, due to prediction being slow and its inability to handle many features.\n",
      "The method we discuss next has neither of these drawbacks.\n",
      "44 | Chapter 2: Supervised Learning\n",
      "Linear Models\n",
      "Linear models are a class of models that are widely used in practice and have been\n",
      "studied extensively in the last few decades, with roots going back over a hundred\n",
      "years. Linear models make a prediction using a linear function of the input features,\n",
      "which we will explain shortly.\n",
      "Linear models for regression\n",
      "For regression, the general prediction formula for a linear model looks as follows:\n",
      "ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\n",
      "Here, x[0] to x[p] denotes the features (in this example, the number of features is p)\n",
      "of a single data point, w and b are parameters of the model that are learned, and ŷ is\n",
      "the prediction the model makes. For a dataset with a single feature, this is:\n",
      "ŷ = w[0] * x[0] + b\n",
      "which you might remember from high school mathematics as the equation for a line.\n",
      "Here, w[0] is the slope and b is the y-axis offset. For more features, w contains the\n",
      "slopes along each feature axis. Alternatively, you can think of the predicted response\n",
      "as being a weighted sum of the input features, with weights (which can be negative)\n",
      "given by the entries of w.\n",
      "Trying to learn the parameters w[0] and b on our one-dimensional wave dataset\n",
      "might lead to the following line (see Figure 2-11):\n",
      "In[25]:\n",
      "mglearn.plots.plot_linear_regression_wave()\n",
      "Out[25]:\n",
      "w[0]: 0.393906  b: -0.031804\n",
      "Supervised Machine Learning Algorithms | 45\n",
      "Figure 2-11. Predictions of a linear model on the wave dataset\n",
      "We added a coordinate cross into the plot to make it easier to understand the line.\n",
      "Looking at w[0] we see that the slope should be around 0.4, which we can confirm\n",
      "visually in the plot. The intercept is where the prediction line should cross the y-axis:\n",
      "this is slightly below zero, which you can also confirm in the image.\n",
      "Linear models for regression can be characterized as regression models for which the\n",
      "prediction is a line for a single feature, a plane when using two features, or a hyper‐\n",
      "plane in higher dimensions (that is, when using more features).\n",
      "If you compare the predictions made by the straight line with those made by the\n",
      "KNeighborsRegressor in Figure 2-10, using a straight line to make predictions seems\n",
      "very restrictive. It looks like all the fine details of the data are lost. In a sense, this is\n",
      "true. It is a strong (and somewhat unrealistic) assumption that our target y is a linear\n",
      "46 | Chapter 2: Supervised Learning\n",
      "6 This is easy to see if you know some linear algebra.\n",
      "combination of the features. But looking at one-dimensional data gives a somewhat\n",
      "skewed perspective. For datasets with many features, linear models can be very pow‐\n",
      "erful. In particular, if you have more features than training data points, any target y\n",
      "can be perfectly modeled (on the training set) as a linear function.6\n",
      "There are many different linear models for regression. The difference between these\n",
      "models lies in how the model parameters w and b are learned from the training data,\n",
      "and how model complexity can be controlled. We will now take a look at the most\n",
      "popular linear models for regression.\n",
      "Linear regression (aka ordinary least squares)\n",
      "Linear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐\n",
      "ear method for regression. Linear regression finds the parameters w and b that mini‐\n",
      "mize the mean squared error between predictions and the true regression targets, y,\n",
      "on the training set. The mean squared error is the sum of the squared differences\n",
      "between the predictions and the true values. Linear regression has no parameters,\n",
      "which is a benefit, but it also has no way to control model complexity.\n",
      "Here is the code that produces the model you can see in Figure 2-11:\n",
      "In[26]:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "X, y = mglearn.datasets.make_wave(n_samples=60)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
      "lr = LinearRegression().fit(X_train, y_train)\n",
      "The “slope” parameters (w), also called weights or coefficients, are stored in the coef_\n",
      "attribute, while the offset or intercept (b) is stored in the intercept_ attribute:\n",
      "In[27]:\n",
      "print(\"lr.coef_: {}\".format(lr.coef_))\n",
      "print(\"lr.intercept_: {}\".format(lr.intercept_))\n",
      "Out[27]:\n",
      "lr.coef_: [ 0.394]\n",
      "lr.intercept_: -0.031804343026759746\n",
      "Supervised Machine Learning Algorithms | 47\n",
      "Y ou might notice the strange-looking trailing underscore at the end\n",
      "of coef_ and intercept_. scikit-learn always stores anything\n",
      "that is derived from the training data in attributes that end with a\n",
      "trailing underscore. That is to separate them from parameters that\n",
      "are set by the user.\n",
      "The intercept_ attribute is always a single float number, while the coef_ attribute is\n",
      "a NumPy array with one entry per input feature. As we only have a single input fea‐\n",
      "ture in the wave dataset, lr.coef_ only has a single entry.\n",
      "Let’s look at the training set and test set performance:\n",
      "In[28]:\n",
      "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
      "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n",
      "Out[28]:\n",
      "Training set score: 0.67\n",
      "Test set score: 0.66\n",
      "An R2 of around 0.66 is not very good, but we can see that the scores on the training\n",
      "and test sets are very close together. This means we are likely underfitting, not over‐\n",
      "fitting. For this one-dimensional dataset, there is little danger of overfitting, as the\n",
      "model is very simple (or restricted). However, with higher-dimensional datasets\n",
      "(meaning datasets with a large number of features), linear models become more pow‐\n",
      "erful, and there is a higher chance of overfitting. Let’s take a look at how LinearRe\n",
      "gression performs on a more complex dataset, like the Boston Housing dataset.\n",
      "Remember that this dataset has 506 samples and 105 derived features. First, we load\n",
      "the dataset and split it into a training and a test set. Then we build the linear regres‐\n",
      "sion model as before:\n",
      "In[29]:\n",
      "X, y = mglearn.datasets.load_extended_boston()\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
      "lr = LinearRegression().fit(X_train, y_train)\n",
      "When comparing training set and test set scores, we find that we predict very accu‐\n",
      "rately on the training set, but the R2 on the test set is much worse:\n",
      "In[30]:\n",
      "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
      "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n",
      "48 | Chapter 2: Supervised Learning\n",
      "7 Mathematically, Ridge penalizes the L2 norm of the coefficients, or the Euclidean length of w.\n",
      "Out[30]:\n",
      "Training set score: 0.95\n",
      "Test set score: 0.61\n",
      "This discrepancy between performance on the training set and the test set is a clear\n",
      "sign of overfitting, and therefore we should try to find a model that allows us to con‐\n",
      "trol complexity. One of the most commonly used alternatives to standard linear\n",
      "regression is ridge regression, which we will look into next.\n",
      "Ridge regression\n",
      "Ridge regression is also a linear model for regression, so the formula it uses to make\n",
      "predictions is the same one used for ordinary least squares. In ridge regression,\n",
      "though, the coefficients (w) are chosen not only so that they predict well on the train‐\n",
      "ing data, but also to fit an additional constraint. We also want the magnitude of coef‐\n",
      "ficients to be as small as possible; in other words, all entries of w should be close to\n",
      "zero. Intuitively, this means each feature should have as little effect on the outcome as\n",
      "possible (which translates to having a small slope), while still predicting well. This\n",
      "constraint is an example of what is called regularization. Regularization means explic‐\n",
      "itly restricting a model to avoid overfitting. The particular kind used by ridge regres‐\n",
      "sion is known as L2 regularization.7\n",
      "Ridge regression is implemented in linear_model.Ridge. Let’s see how well it does\n",
      "on the extended Boston Housing dataset:\n",
      "In[31]:\n",
      "from sklearn.linear_model import Ridge\n",
      "ridge = Ridge().fit(X_train, y_train)\n",
      "print(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\n",
      "print(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\n",
      "Out[31]:\n",
      "Training set score: 0.89\n",
      "Test set score: 0.75\n",
      "As you can see, the training set score of Ridge is lower than for LinearRegression,\n",
      "while the test set score is higher. This is consistent with our expectation. With linear\n",
      "regression, we were overfitting our data. Ridge is a more restricted model, so we are\n",
      "less likely to overfit. A less complex model means worse performance on the training\n",
      "set, but better generalization. As we are only interested in generalization perfor‐\n",
      "mance, we should choose the Ridge model over the LinearRegression model.\n",
      "Supervised Machine Learning Algorithms | 49\n",
      "The Ridge model makes a trade-off between the simplicity of the model (near-zero\n",
      "coefficients) and its performance on the training set. How much importance the\n",
      "model places on simplicity versus training set performance can be specified by the\n",
      "user, using the alpha parameter. In the previous example, we used the default param‐\n",
      "eter alpha=1.0. There is no reason why this will give us the best trade-off, though.\n",
      "The optimum setting of alpha depends on the particular dataset we are using.\n",
      "Increasing alpha forces coefficients to move more toward zero, which decreases\n",
      "training set performance but might help generalization. For example:\n",
      "In[32]:\n",
      "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
      "print(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
      "print(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))\n",
      "Out[32]:\n",
      "Training set score: 0.79\n",
      "Test set score: 0.64\n",
      "Decreasing alpha allows the coefficients to be less restricted, meaning we move right\n",
      "in Figure 2-1. For very small values of alpha, coefficients are barely restricted at all,\n",
      "and we end up with a model that resembles LinearRegression:\n",
      "In[33]:\n",
      "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
      "print(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
      "print(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))\n",
      "Out[33]:\n",
      "Training set score: 0.93\n",
      "Test set score: 0.77\n",
      "Here, alpha=0.1 seems to be working well. We could try decreasing alpha even more\n",
      "to improve generalization. For now, notice how the parameter alpha corresponds to\n",
      "the model complexity as shown in Figure 2-1. We will discuss methods to properly\n",
      "select parameters in Chapter 5.\n",
      "We can also get a more qualitative insight into how the alpha parameter changes the\n",
      "model by inspecting the coef_ attribute of models with different values of alpha. A\n",
      "higher alpha means a more restricted model, so we expect the entries of coef_ to\n",
      "have smaller magnitude for a high value of alpha than for a low value of alpha. This\n",
      "is confirmed in the plot in Figure 2-12:\n",
      "50 | Chapter 2: Supervised Learning\n",
      "In[34]:\n",
      "plt.plot(ridge.coef_, 's', label=\"Ridge alpha=1\")\n",
      "plt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\n",
      "plt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\n",
      "plt.plot(lr.coef_, 'o', label=\"LinearRegression\")\n",
      "plt.xlabel(\"Coefficient index\")\n",
      "plt.ylabel(\"Coefficient magnitude\")\n",
      "plt.hlines(0, 0, len(lr.coef_))\n",
      "plt.ylim(-25, 25)\n",
      "plt.legend()\n",
      "Figure 2-12. Comparing coefficient magnitudes for ridge regression with different values\n",
      "of alpha and linear regression\n",
      "Here, the x-axis enumerates the entries of coef_: x=0 shows the coefficient associated\n",
      "with the first feature, x=1 the coefficient associated with the second feature, and so on\n",
      "up to x=100. The y-axis shows the numeric values of the corresponding values of the\n",
      "coefficients. The main takeaway here is that for alpha=10, the coefficients are mostly\n",
      "between around –3 and 3. The coefficients for the Ridge model with alpha=1 are\n",
      "somewhat larger. The dots corresponding to alpha=0.1 have larger magnitude still,\n",
      "and many of the dots corresponding to linear regression without any regularization\n",
      "(which would be alpha=0) are so large they are outside of the chart.\n",
      "Supervised Machine Learning Algorithms | 51\n",
      "Another way to understand the influence of regularization is to fix a value of alpha\n",
      "but vary the amount of training data available. For Figure 2-13, we subsampled the\n",
      "Boston Housing dataset and evaluated LinearRegression and Ridge(alpha=1) on\n",
      "subsets of increasing size (plots that show model performance as a function of dataset\n",
      "size are called learning curves):\n",
      "In[35]:\n",
      "mglearn.plots.plot_ridge_n_samples()\n",
      "Figure 2-13. Learning curves for ridge regression and linear regression on the Boston\n",
      "Housing dataset\n",
      "As one would expect, the training score is higher than the test score for all dataset\n",
      "sizes, for both ridge and linear regression. Because ridge is regularized, the training\n",
      "score of ridge is lower than the training score for linear regression across the board.\n",
      "However, the test score for ridge is better, particularly for small subsets of the data.\n",
      "For less than 400 data points, linear regression is not able to learn anything. As more\n",
      "and more data becomes available to the model, both models improve, and linear\n",
      "regression catches up with ridge in the end. The lesson here is that with enough train‐\n",
      "ing data, regularization becomes less important, and given enough data, ridge and\n",
      "52 | Chapter 2: Supervised Learning\n",
      "8 The lasso penalizes the L1 norm of the coefficient vector—or in other words, the sum of the absolute values of\n",
      "the coefficients.\n",
      "linear regression will have the same performance (the fact that this happens here\n",
      "when using the full dataset is just by chance). Another interesting aspect of\n",
      "Figure 2-13 is the decrease in training performance for linear regression. If more data\n",
      "is added, it becomes harder for a model to overfit, or memorize the data.\n",
      "Lasso\n",
      "An alternative to Ridge for regularizing linear regression is Lasso. As with ridge\n",
      "regression, using the lasso also restricts coefficients to be close to zero, but in a\n",
      "slightly different way, called L1 regularization.8 The consequence of L1 regularization\n",
      "is that when using the lasso, some coefficients are exactly zero. This means some fea‐\n",
      "tures are entirely ignored by the model. This can be seen as a form of automatic fea‐\n",
      "ture selection. Having some coefficients be exactly zero often makes a model easier to\n",
      "interpret, and can reveal the most important features of your model.\n",
      "Let’s apply the lasso to the extended Boston Housing dataset:\n",
      "In[36]:\n",
      "from sklearn.linear_model import Lasso\n",
      "lasso = Lasso().fit(X_train, y_train)\n",
      "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
      "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
      "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))\n",
      "Out[36]:\n",
      "Training set score: 0.29\n",
      "Test set score: 0.21\n",
      "Number of features used: 4\n",
      "As you can see, Lasso does quite badly, both on the training and the test set. This\n",
      "indicates that we are underfitting, and we find that it used only 4 of the 105 features.\n",
      "Similarly to Ridge, the Lasso also has a regularization parameter, alpha, that controls\n",
      "how strongly coefficients are pushed toward zero. In the previous example, we used\n",
      "the default of alpha=1.0. To reduce underfitting, let’s try decreasing alpha. When we\n",
      "do this, we also need to increase the default setting of max_iter (the maximum num‐\n",
      "ber of iterations to run):\n",
      "Supervised Machine Learning Algorithms | 53\n",
      "In[37]:\n",
      "# we increase the default setting of \"max_iter\",\n",
      "# otherwise the model would warn us that we should increase max_iter.\n",
      "lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\n",
      "print(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\n",
      "print(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\n",
      "print(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))\n",
      "Out[37]:\n",
      "Training set score: 0.90\n",
      "Test set score: 0.77\n",
      "Number of features used: 33\n",
      "A lower alpha allowed us to fit a more complex model, which worked better on the\n",
      "training and test data. The performance is slightly better than using Ridge, and we are\n",
      "using only 33 of the 105 features. This makes this model potentially easier to under‐\n",
      "stand.\n",
      "If we set alpha too low, however, we again remove the effect of regularization and end\n",
      "up overfitting, with a result similar to LinearRegression:\n",
      "In[38]:\n",
      "lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)\n",
      "print(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\n",
      "print(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\n",
      "print(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))\n",
      "Out[38]:\n",
      "Training set score: 0.95\n",
      "Test set score: 0.64\n",
      "Number of features used: 94\n",
      "Again, we can plot the coefficients of the different models, similarly to Figure 2-12.\n",
      "The result is shown in Figure 2-14:\n",
      "In[39]:\n",
      "plt.plot(lasso.coef_, 's', label=\"Lasso alpha=1\")\n",
      "plt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")\n",
      "plt.plot(lasso00001.coef_, 'v', label=\"Lasso alpha=0.0001\")\n",
      "plt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\")\n",
      "plt.legend(ncol=2, loc=(0, 1.05))\n",
      "plt.ylim(-25, 25)\n",
      "plt.xlabel(\"Coefficient index\")\n",
      "plt.ylabel(\"Coefficient magnitude\")\n",
      "54 | Chapter 2: Supervised Learning\n",
      "Figure 2-14. Comparing coefficient magnitudes for lasso regression with different values\n",
      "of alpha and ridge regression\n",
      "For alpha=1, we not only see that most of the coefficients are zero (which we already\n",
      "knew), but that the remaining coefficients are also small in magnitude. Decreasing\n",
      "alpha to 0.01, we obtain the solution shown as the green dots, which causes most\n",
      "features to be exactly zero. Using alpha=0.00001, we get a model that is quite unregu‐\n",
      "larized, with most coefficients nonzero and of large magnitude. For comparison, the\n",
      "best Ridge solution is shown in teal. The Ridge model with alpha=0.1 has similar\n",
      "predictive performance as the lasso model with alpha=0.01, but using Ridge, all coef‐\n",
      "ficients are nonzero.\n",
      "In practice, ridge regression is usually the first choice between these two models.\n",
      "However, if you have a large amount of features and expect only a few of them to be\n",
      "important, Lasso might be a better choice. Similarly, if you would like to have a\n",
      "model that is easy to interpret, Lasso will provide a model that is easier to under‐\n",
      "stand, as it will select only a subset of the input features. scikit-learn also provides\n",
      "the ElasticNet class, which combines the penalties of Lasso and Ridge. In practice,\n",
      "this combination works best, though at the price of having two parameters to adjust:\n",
      "one for the L1 regularization, and one for the L2 regularization.\n",
      "Supervised Machine Learning Algorithms | 55\n",
      "Linear models for classification\n",
      "Linear models are also extensively used for classification. Let’s look at binary classifi‐\n",
      "cation first. In this case, a prediction is made using the following formula:\n",
      "ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\n",
      "The formula looks very similar to the one for linear regression, but instead of just\n",
      "returning the weighted sum of the features, we threshold the predicted value at zero.\n",
      "If the function is smaller than zero, we predict the class –1; if it is larger than zero, we\n",
      "predict the class +1. This prediction rule is common to all linear models for classifica‐\n",
      "tion. Again, there are many different ways to find the coefficients ( w) and the inter‐\n",
      "cept (b).\n",
      "For linear models for regression, the output, ŷ, is a linear function of the features: a\n",
      "line, plane, or hyperplane (in higher dimensions). For linear models for classification,\n",
      "the decision boundary is a linear function of the input. In other words, a (binary) lin‐\n",
      "ear classifier is a classifier that separates two classes using a line, a plane, or a hyper‐\n",
      "plane. We will see examples of that in this section.\n",
      "There are many algorithms for learning linear models. These algorithms all differ in\n",
      "the following two ways:\n",
      "• The way in which they measure how well a particular combination of coefficients\n",
      "and intercept fits the training data\n",
      "• If and what kind of regularization they use\n",
      "Different algorithms choose different ways to measure what “fitting the training set\n",
      "well” means. For technical mathematical reasons, it is not possible to adjust w and b\n",
      "to minimize the number of misclassifications the algorithms produce, as one might\n",
      "hope. For our purposes, and many applications, the different choices for item 1 in the\n",
      "preceding list (called loss functions) are of little significance.\n",
      "The two most common linear classification algorithms are logistic regression, imple‐\n",
      "mented in linear_model.LogisticRegression, and linear support vector machines\n",
      "(linear SVMs), implemented in svm.LinearSVC (SVC stands for support vector classi‐\n",
      "fier). Despite its name, LogisticRegression is a classification algorithm and not a\n",
      "regression algorithm, and it should not be confused with LinearRegression.\n",
      "We can apply the LogisticRegression and LinearSVC models to the forge dataset,\n",
      "and visualize the decision boundary as found by the linear models (Figure 2-15):\n",
      "56 | Chapter 2: Supervised Learning\n",
      "In[40]:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import LinearSVC\n",
      "X, y = mglearn.datasets.make_forge()\n",
      "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
      "for model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n",
      "    clf = model.fit(X, y)\n",
      "    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n",
      "                                    ax=ax, alpha=.7)\n",
      "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
      "    ax.set_title(\"{}\".format(clf.__class__.__name__))\n",
      "    ax.set_xlabel(\"Feature 0\")\n",
      "    ax.set_ylabel(\"Feature 1\")\n",
      "axes[0].legend()\n",
      "Figure 2-15. Decision boundaries of a linear SVM and logistic regression on the forge\n",
      "dataset with the default parameters\n",
      "In this figure, we have the first feature of the forge dataset on the x-axis and the sec‐\n",
      "ond feature on the y-axis, as before. We display the decision boundaries found by\n",
      "LinearSVC and LogisticRegression respectively as straight lines, separating the area\n",
      "classified as class 1 on the top from the area classified as class 0 on the bottom. In\n",
      "other words, any new data point that lies above the black line will be classified as class\n",
      "1 by the respective classifier, while any point that lies below the black line will be clas‐\n",
      "sified as class 0.\n",
      "The two models come up with similar decision boundaries. Note that both misclas‐\n",
      "sify two of the points. By default, both models apply an L2 regularization, in the same\n",
      "way that Ridge does for regression.\n",
      "For LogisticRegression and LinearSVC the trade-off parameter that determines the\n",
      "strength of the regularization is called C, and higher values of C correspond to less\n",
      "Supervised Machine Learning Algorithms | 57\n",
      "regularization. In other words, when you use a high value for the parameter C, Logis\n",
      "ticRegression and LinearSVC try to fit the training set as best as possible, while with\n",
      "low values of the parameter C, the models put more emphasis on finding a coefficient\n",
      "vector (w) that is close to zero.\n",
      "There is another interesting aspect of how the parameter C acts. Using low values of C\n",
      "will cause the algorithms to try to adjust to the “majority” of data points, while using\n",
      "a higher value of C stresses the importance that each individual data point be classi‐\n",
      "fied correctly. Here is an illustration using LinearSVC (Figure 2-16):\n",
      "In[41]:\n",
      "mglearn.plots.plot_linear_svc_regularization()\n",
      "Figure 2-16. Decision boundaries of a linear SVM on the forge dataset for different\n",
      "values of C\n",
      "On the lefthand side, we have a very small C corresponding to a lot of regularization.\n",
      "Most of the points in class 0 are at the top, and most of the points in class 1 are at the\n",
      "bottom. The strongly regularized model chooses a relatively horizontal line, misclas‐\n",
      "sifying two points. In the center plot, C is slightly higher, and the model focuses more\n",
      "on the two misclassified samples, tilting the decision boundary. Finally, on the right‐\n",
      "hand side, the very high value of C in the model tilts the decision boundary a lot, now\n",
      "correctly classifying all points in class 0. One of the points in class 1 is still misclassi‐\n",
      "fied, as it is not possible to correctly classify all points in this dataset using a straight\n",
      "line. The model illustrated on the righthand side tries hard to correctly classify all\n",
      "points, but might not capture the overall layout of the classes well. In other words,\n",
      "this model is likely overfitting.\n",
      "Similarly to the case of regression, linear models for classification might seem very\n",
      "restrictive in low-dimensional spaces, only allowing for decision boundaries that are\n",
      "straight lines or planes. Again, in high dimensions, linear models for classification\n",
      "58 | Chapter 2: Supervised Learning\n",
      "become very powerful, and guarding against overfitting becomes increasingly impor‐\n",
      "tant when considering more features.\n",
      "Let’s analyze LinearLogistic in more detail on the Breast Cancer dataset:\n",
      "In[42]:\n",
      "from sklearn.datasets import load_breast_cancer\n",
      "cancer = load_breast_cancer()\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
      "logreg = LogisticRegression().fit(X_train, y_train)\n",
      "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
      "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
      "Out[42]:\n",
      "Training set score: 0.953\n",
      "Test set score: 0.958\n",
      "The default value of C=1 provides quite good performance, with 95% accuracy on\n",
      "both the training and the test set. But as training and test set performance are very\n",
      "close, it is likely that we are underfitting. Let’s try to increase C to fit a more flexible\n",
      "model:\n",
      "In[43]:\n",
      "logreg100 = LogisticRegression(C=100).fit(X_train, y_train)\n",
      "print(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\n",
      "print(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))\n",
      "Out[43]:\n",
      "Training set score: 0.972\n",
      "Test set score: 0.965\n",
      "Using C=100 results in higher training set accuracy, and also a slightly increased test\n",
      "set accuracy, confirming our intuition that a more complex model should perform\n",
      "better.\n",
      "We can also investigate what happens if we use an even more regularized model than\n",
      "the default of C=1, by setting C=0.01:\n",
      "In[44]:\n",
      "logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)\n",
      "print(\"Training set score: {:.3f}\".format(logreg001.score(X_train, y_train)))\n",
      "print(\"Test set score: {:.3f}\".format(logreg001.score(X_test, y_test)))\n",
      "Out[44]:\n",
      "Training set score: 0.934\n",
      "Test set score: 0.930\n",
      "Supervised Machine Learning Algorithms | 59\n",
      "As expected, when moving more to the left along the scale shown in Figure 2-1 from\n",
      "an already underfit model, both training and test set accuracy decrease relative to the\n",
      "default parameters.\n",
      "Finally, let’s look at the coefficients learned by the models with the three different set‐\n",
      "tings of the regularization parameter C (Figure 2-17):\n",
      "In[45]:\n",
      "plt.plot(logreg.coef_.T, 'o', label=\"C=1\")\n",
      "plt.plot(logreg100.coef_.T, '^', label=\"C=100\")\n",
      "plt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\n",
      "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
      "plt.hlines(0, 0, cancer.data.shape[1])\n",
      "plt.ylim(-5, 5)\n",
      "plt.xlabel(\"Coefficient index\")\n",
      "plt.ylabel(\"Coefficient magnitude\")\n",
      "plt.legend()\n",
      "As LogisticRegression applies an L2 regularization by default,\n",
      "the result looks similar to that produced by Ridge in Figure 2-12.\n",
      "Stronger regularization pushes coefficients more and more toward\n",
      "zero, though coefficients never become exactly zero. Inspecting the\n",
      "plot more closely, we can also see an interesting effect in the third\n",
      "coefficient, for “mean perimeter. ” For C=100 and C=1, the coefficient\n",
      "is negative, while for C=0.001, the coefficient is positive, with a\n",
      "magnitude that is even larger than for C=1. Interpreting a model\n",
      "like this, one might think the coefficient tells us which class a fea‐\n",
      "ture is associated with. For example, one might think that a high\n",
      "“texture error” feature is related to a sample being “malignant. ”\n",
      "However, the change of sign in the coefficient for “mean perimeter”\n",
      "means that depending on which model we look at, a high “mean\n",
      "perimeter” could be taken as being either indicative of “benign” or\n",
      "indicative of “malignant. ” This illustrates that interpretations of\n",
      "coefficients of linear models should always be taken with a grain of\n",
      "salt.\n",
      "60 | Chapter 2: Supervised Learning\n",
      "Figure 2-17. Coefficients learned by logistic regression on the Breast Cancer dataset for\n",
      "different values of C\n",
      "Supervised Machine Learning Algorithms | 61\n",
      "If we desire a more interpretable model, using L1 regularization might help, as it lim‐\n",
      "its the model to using only a few features. Here is the coefficient plot and classifica‐\n",
      "tion accuracies for L1 regularization (Figure 2-18):\n",
      "In[46]:\n",
      "for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n",
      "    lr_l1 = LogisticRegression(C=C, penalty=\"l1\").fit(X_train, y_train)\n",
      "    print(\"Training accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n",
      "          C, lr_l1.score(X_train, y_train)))\n",
      "    print(\"Test accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n",
      "          C, lr_l1.score(X_test, y_test)))\n",
      "    plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\n",
      "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
      "plt.hlines(0, 0, cancer.data.shape[1])\n",
      "plt.xlabel(\"Coefficient index\")\n",
      "plt.ylabel(\"Coefficient magnitude\")\n",
      "plt.ylim(-5, 5)\n",
      "plt.legend(loc=3)\n",
      "Out[46]:\n",
      "Training accuracy of l1 logreg with C=0.001: 0.91\n",
      "Test accuracy of l1 logreg with C=0.001: 0.92\n",
      "Training accuracy of l1 logreg with C=1.000: 0.96\n",
      "Test accuracy of l1 logreg with C=1.000: 0.96\n",
      "Training accuracy of l1 logreg with C=100.000: 0.99\n",
      "Test accuracy of l1 logreg with C=100.000: 0.98\n",
      "As you can see, there are many parallels between linear models for binary classifica‐\n",
      "tion and linear models for regression. As in regression, the main difference between\n",
      "the models is the penalty parameter, which influences the regularization and\n",
      "whether the model will use all available features or select only a subset.\n",
      "62 | Chapter 2: Supervised Learning\n",
      "Figure 2-18. Coefficients learned by logistic regression with L1 penalty on the Breast\n",
      "Cancer dataset for different values of C\n",
      "Linear models for multiclass classification\n",
      "Many linear classification models are for binary classification only, and don’t extend\n",
      "naturally to the multiclass case (with the exception of logistic regression). A common\n",
      "technique to extend a binary classification algorithm to a multiclass classification\n",
      "algorithm is the one-vs.-rest approach. In the one-vs.-rest approach, a binary model is\n",
      "learned for each class that tries to separate that class from all of the other classes,\n",
      "resulting in as many binary models as there are classes. To make a prediction, all\n",
      "binary classifiers are run on a test point. The classifier that has the highest score on its\n",
      "single class “wins, ” and this class label is returned as the prediction.\n",
      "Supervised Machine Learning Algorithms | 63\n",
      "Having one binary classifier per class results in having one vector of coefficients ( w)\n",
      "and one intercept (b) for each class. The class for which the result of the classification\n",
      "confidence formula given here is highest is the assigned class label:\n",
      "w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\n",
      "The mathematics behind multiclass logistic regression differ somewhat from the one-\n",
      "vs.-rest approach, but they also result in one coefficient vector and intercept per class,\n",
      "and the same method of making a prediction is applied.\n",
      "Let’s apply the one-vs.-rest method to a simple three-class classification dataset. We\n",
      "use a two-dimensional dataset, where each class is given by data sampled from a\n",
      "Gaussian distribution (see Figure 2-19):\n",
      "In[47]:\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y = make_blobs(random_state=42)\n",
      "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "plt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])\n",
      "Figure 2-19. Two-dimensional toy dataset containing three classes\n",
      "64 | Chapter 2: Supervised Learning\n",
      "Now, we train a LinearSVC classifier on the dataset:\n",
      "In[48]:\n",
      "linear_svm = LinearSVC().fit(X, y)\n",
      "print(\"Coefficient shape: \", linear_svm.coef_.shape)\n",
      "print(\"Intercept shape: \", linear_svm.intercept_.shape)\n",
      "Out[48]:\n",
      "Coefficient shape:  (3, 2)\n",
      "Intercept shape:  (3,)\n",
      "We see that the shape of the coef_ is (3, 2), meaning that each row of coef_ con‐\n",
      "tains the coefficient vector for one of the three classes and each column holds the\n",
      "coefficient value for a specific feature (there are two in this dataset). The intercept_\n",
      "is now a one-dimensional array, storing the intercepts for each class.\n",
      "Let’s visualize the lines given by the three binary classifiers (Figure 2-20):\n",
      "In[49]:\n",
      "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
      "line = np.linspace(-15, 15)\n",
      "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n",
      "                                  ['b', 'r', 'g']):\n",
      "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
      "plt.ylim(-10, 15)\n",
      "plt.xlim(-10, 8)\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n",
      "            'Line class 2'], loc=(1.01, 0.3))\n",
      "Y ou can see that all the points belonging to class 0 in the training data are above the\n",
      "line corresponding to class 0, which means they are on the “class 0” side of this binary\n",
      "classifier. The points in class 0 are above the line corresponding to class 2, which\n",
      "means they are classified as “rest” by the binary classifier for class 2. The points\n",
      "belonging to class 0 are to the left of the line corresponding to class 1, which means\n",
      "the binary classifier for class 1 also classifies them as “rest. ” Therefore, any point in\n",
      "this area will be classified as class 0 by the final classifier (the result of the classifica‐\n",
      "tion confidence formula for classifier 0 is greater than zero, while it is smaller than\n",
      "zero for the other two classes).\n",
      "But what about the triangle in the middle of the plot? All three binary classifiers clas‐\n",
      "sify points there as “rest. ” Which class would a point there be assigned to? The answer\n",
      "is the one with the highest value for the classification formula: the class of the closest\n",
      "line.\n",
      "Supervised Machine Learning Algorithms | 65\n",
      "Figure 2-20. Decision boundaries learned by the three one-vs.-rest classifiers\n",
      "The following example ( Figure 2-21) shows the predictions for all regions of the 2D\n",
      "space:\n",
      "In[50]:\n",
      "mglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\n",
      "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
      "line = np.linspace(-15, 15)\n",
      "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n",
      "                                  ['b', 'r', 'g']):\n",
      "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
      "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n",
      "            'Line class 2'], loc=(1.01, 0.3))\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "66 | Chapter 2: Supervised Learning\n",
      "Figure 2-21. Multiclass decision boundaries derived from the three one-vs.-rest classifiers\n",
      "Strengths, weaknesses, and parameters\n",
      "The main parameter of linear models is the regularization parameter, called alpha in\n",
      "the regression models and C in LinearSVC and LogisticRegression. Large values for\n",
      "alpha or small values for C mean simple models. In particular for the regression mod‐\n",
      "els, tuning these parameters is quite important. Usually C and alpha are searched for\n",
      "on a logarithmic scale. The other decision you have to make is whether you want to\n",
      "use L1 regularization or L2 regularization. If you assume that only a few of your fea‐\n",
      "tures are actually important, you should use L1. Otherwise, you should default to L2.\n",
      "L1 can also be useful if interpretability of the model is important. As L1 will use only\n",
      "a few features, it is easier to explain which features are important to the model, and\n",
      "what the effects of these features are.\n",
      "Linear models are very fast to train, and also fast to predict. They scale to very large\n",
      "datasets and work well with sparse data. If your data consists of hundreds of thou‐\n",
      "sands or millions of samples, you might want to investigate using the solver='sag'\n",
      "option in LogisticRegression and Ridge, which can be faster than the default on\n",
      "large datasets. Other options are the SGDClassifier class and the SGDRegressor\n",
      "class, which implement even more scalable versions of the linear models described\n",
      "here.\n",
      "Another strength of linear models is that they make it relatively easy to understand\n",
      "how a prediction is made, using the formulas we saw earlier for regression and classi‐\n",
      "fication. Unfortunately, it is often not entirely clear why coefficients are the way they\n",
      "are. This is particularly true if your dataset has highly correlated features; in these\n",
      "cases, the coefficients might be hard to interpret.\n",
      "Supervised Machine Learning Algorithms | 67\n",
      "Linear models often perform well when the number of features is large compared to\n",
      "the number of samples. They are also often used on very large datasets, simply\n",
      "because it’s not feasible to train other models. However, in lower-dimensional spaces,\n",
      "other models might yield better generalization performance. We will look at some\n",
      "examples in which linear models fail in “Kernelized Support Vector Machines” on\n",
      "page 92.\n",
      "Method Chaining\n",
      "The fit method of all scikit-learn models returns self. This allows you to write\n",
      "code like the following, which we’ve already used extensively in this chapter:\n",
      "In[51]:\n",
      "# instantiate model and fit it in one line\n",
      "logreg = LogisticRegression().fit(X_train, y_train)\n",
      "Here, we used the return value of fit (which is self) to assign the trained model to\n",
      "the variable logreg. This concatenation of method calls (here __init__ and then fit)\n",
      "is known as method chaining. Another common application of method chaining in\n",
      "scikit-learn is to fit and predict in one line:\n",
      "In[52]:\n",
      "logreg = LogisticRegression()\n",
      "y_pred = logreg.fit(X_train, y_train).predict(X_test)\n",
      "Finally, you can even do model instantiation, fitting, and predicting in one line:\n",
      "In[53]:\n",
      "y_pred = LogisticRegression().fit(X_train, y_train).predict(X_test)\n",
      "This very short variant is not ideal, though. A lot is happening in a single line, which\n",
      "might make the code hard to read. Additionally, the fitted logistic regression model\n",
      "isn’t stored in any variable, so we can’t inspect it or use it to predict on any other data.\n",
      "Naive Bayes Classifiers\n",
      "Naive Bayes classifiers are a family of classifiers that are quite similar to the linear\n",
      "models discussed in the previous section. However, they tend to be even faster in\n",
      "training. The price paid for this efficiency is that naive Bayes models often provide\n",
      "generalization performance that is slightly worse than that of linear classifiers like\n",
      "LogisticRegression and LinearSVC.\n",
      "The reason that naive Bayes models are so efficient is that they learn parameters by\n",
      "looking at each feature individually and collect simple per-class statistics from each\n",
      "feature. There are three kinds of naive Bayes classifiers implemented in scikit-\n",
      "68 | Chapter 2: Supervised Learning\n",
      "learn: GaussianNB, BernoulliNB, and MultinomialNB. GaussianNB can be applied to\n",
      "any continuous data, while BernoulliNB assumes binary data and MultinomialNB\n",
      "assumes count data (that is, that each feature represents an integer count of some‐\n",
      "thing, like how often a word appears in a sentence). BernoulliNB and MultinomialNB\n",
      "are mostly used in text data classification.\n",
      "The BernoulliNB classifier counts how often every feature of each class is not zero.\n",
      "This is most easily understood with an example:\n",
      "In[54]:\n",
      "X = np.array([[0, 1, 0, 1],\n",
      "              [1, 0, 1, 1],\n",
      "              [0, 0, 0, 1],\n",
      "              [1, 0, 1, 0]])\n",
      "y = np.array([0, 1, 0, 1])\n",
      "Here, we have four data points, with four binary features each. There are two classes,\n",
      "0 and 1. For class 0 (the first and third data points), the first feature is zero two times\n",
      "and nonzero zero times, the second feature is zero one time and nonzero one time,\n",
      "and so on. These same counts are then calculated for the data points in the second\n",
      "class. Counting the nonzero entries per class in essence looks like this:\n",
      "In[55]:\n",
      "counts = {}\n",
      "for label in np.unique(y):\n",
      "    # iterate over each class\n",
      "    # count (sum) entries of 1 per feature\n",
      "    counts[label] = X[y == label].sum(axis=0)\n",
      "print(\"Feature counts:\\n{}\".format(counts))\n",
      "Out[55]:\n",
      "Feature counts:\n",
      "{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\n",
      "The other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\n",
      "ferent in what kinds of statistics they compute. MultinomialNB takes into account the\n",
      "average value of each feature for each class, while GaussianNB stores the average value\n",
      "as well as the standard deviation of each feature for each class.\n",
      "To make a prediction, a data point is compared to the statistics for each of the classes,\n",
      "and the best matching class is predicted. Interestingly, for both MultinomialNB and\n",
      "BernoulliNB, this leads to a prediction formula that is of the same form as in the lin‐\n",
      "ear models (see “Linear models for classification” on page 56). Unfortunately, coef_\n",
      "for the naive Bayes models has a somewhat different meaning than in the linear mod‐\n",
      "els, in that coef_ is not the same as w.\n",
      "Supervised Machine Learning Algorithms | 69\n",
      "Strengths, weaknesses, and parameters\n",
      "MultinomialNB and BernoulliNB have a single parameter, alpha, which controls\n",
      "model complexity. The way alpha works is that the algorithm adds to the data alpha\n",
      "many virtual data points that have positive values for all the features. This results in a\n",
      "“smoothing” of the statistics. A large alpha means more smoothing, resulting in less\n",
      "complex models. The algorithm’s performance is relatively robust to the setting of\n",
      "alpha, meaning that setting alpha is not critical for good performance. However,\n",
      "tuning it usually improves accuracy somewhat.\n",
      "GaussianNB is mostly used on very high-dimensional data, while the other two var‐\n",
      "iants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\n",
      "usually performs better than BinaryNB, particularly on datasets with a relatively large\n",
      "number of nonzero features (i.e., large documents).\n",
      "The naive Bayes models share many of the strengths and weaknesses of the linear\n",
      "models. They are very fast to train and to predict, and the training procedure is easy\n",
      "to understand. The models work very well with high-dimensional sparse data and are\n",
      "relatively robust to the parameters. Naive Bayes models are great baseline models and\n",
      "are often used on very large datasets, where training even a linear model might take\n",
      "too long.\n",
      "Decision Trees\n",
      "Decision trees are widely used models for classification and regression tasks. Essen‐\n",
      "tially, they learn a hierarchy of if/else questions, leading to a decision.\n",
      "These questions are similar to the questions you might ask in a game of 20 Questions.\n",
      "Imagine you want to distinguish between the following four animals: bears, hawks,\n",
      "penguins, and dolphins. Y our goal is to get to the right answer by asking as few if/else\n",
      "questions as possible. Y ou might start off by asking whether the animal has feathers, a\n",
      "question that narrows down your possible animals to just two. If the answer is “yes, ”\n",
      "you can ask another question that could help you distinguish between hawks and\n",
      "penguins. For example, you could ask whether the animal can fly. If the animal\n",
      "doesn’t have feathers, your possible animal choices are dolphins and bears, and you\n",
      "will need to ask a question to distinguish between these two animals—for example,\n",
      "asking whether the animal has fins.\n",
      "This series of questions can be expressed as a decision tree, as shown in Figure 2-22.\n",
      "In[56]:\n",
      "mglearn.plots.plot_animal_tree()\n",
      "70 | Chapter 2: Supervised Learning\n",
      "Figure 2-22. A decision tree to distinguish among several animals\n",
      "In this illustration, each node in the tree either represents a question or a terminal\n",
      "node (also called a leaf) that contains the answer. The edges connect the answers to a\n",
      "question with the next question you would ask.\n",
      "In machine learning parlance, we built a model to distinguish between four classes of\n",
      "animals (hawks, penguins, dolphins, and bears) using the three features “has feath‐\n",
      "ers, ” “can fly, ” and “has fins. ” Instead of building these models by hand, we can learn\n",
      "them from data using supervised learning.\n",
      "Building decision trees\n",
      "Let’s go through the process of building a decision tree for the 2D classification data‐\n",
      "set shown in Figure 2-23 . The dataset consists of two half-moon shapes, with each\n",
      "class consisting of 75 data points. We will refer to this dataset as two_moons.\n",
      "Learning a decision tree means learning the sequence of if/else questions that gets us\n",
      "to the true answer most quickly. In the machine learning setting, these questions are\n",
      "called tests (not to be confused with the test set, which is the data we use to test to see\n",
      "how generalizable our model is). Usually data does not come in the form of binary\n",
      "yes/no features as in the animal example, but is instead represented as continuous\n",
      "features such as in the 2D dataset shown in Figure 2-23. The tests that are used on\n",
      "continuous data are of the form “Is feature i larger than value a?”\n",
      "Supervised Machine Learning Algorithms | 71\n",
      "Figure 2-23. Two-moons dataset on which the decision tree will be built\n",
      "To build a tree, the algorithm searches over all possible tests and finds the one that is\n",
      "most informative about the target variable. Figure 2-24  shows the first test that is\n",
      "picked. Splitting the dataset vertically at x[1]=0.0596 yields the most information; it\n",
      "best separates the points in class 1 from the points in class 2. The top node, also called\n",
      "the root, represents the whole dataset, consisting of 75 points belonging to class 0 and\n",
      "75 points belonging to class 1. The split is done by testing whether x[1] <= 0.0596,\n",
      "indicated by a black line. If the test is true, a point is assigned to the left node, which\n",
      "contains 2 points belonging to class 0 and 32 points belonging to class 1. Otherwise\n",
      "the point is assigned to the right node, which contains 48 points belonging to class 0\n",
      "and 18 points belonging to class 1. These two nodes correspond to the top and bot‐\n",
      "tom regions shown in Figure 2-24. Even though the first split did a good job of sepa‐\n",
      "rating the two classes, the bottom region still contains points belonging to class 0, and\n",
      "the top region still contains points belonging to class 1. We can build a more accurate\n",
      "model by repeating the process of looking for the best test in both regions.\n",
      "Figure 2-25 shows that the most informative next split for the left and the right region\n",
      "is based on x[0].\n",
      "72 | Chapter 2: Supervised Learning\n",
      "Figure 2-24. Decision boundary of tree with depth 1 (left) and corresponding tree (right)\n",
      "Figure 2-25. Decision boundary of tree with depth 2 (left) and corresponding decision\n",
      "tree (right)\n",
      "This recursive process yields a binary tree of decisions, with each node containing a\n",
      "test. Alternatively, you can think of each test as splitting the part of the data that is\n",
      "currently being considered along one axis. This yields a view of the algorithm as\n",
      "building a hierarchical partition. As each test concerns only a single feature, the\n",
      "regions in the resulting partition always have axis-parallel boundaries.\n",
      "The recursive partitioning of the data is repeated until each region in the partition\n",
      "(each leaf in the decision tree) only contains a single target value (a single class or a\n",
      "single regression value). A leaf of the tree that contains data points that all share the\n",
      "same target value is called pure. The final partitioning for this dataset is shown in\n",
      "Figure 2-26.\n",
      "Supervised Machine Learning Algorithms | 73\n",
      "Figure 2-26. Decision boundary of tree with depth 9 (left) and part of the corresponding\n",
      "tree (right); the full tree is quite large and hard to visualize\n",
      "A prediction on a new data point is made by checking which region of the partition\n",
      "of the feature space the point lies in, and then predicting the majority target (or the\n",
      "single target in the case of pure leaves) in that region. The region can be found by\n",
      "traversing the tree from the root and going left or right, depending on whether the\n",
      "test is fulfilled or not.\n",
      "It is also possible to use trees for regression tasks, using exactly the same technique.\n",
      "To make a prediction, we traverse the tree based on the tests in each node and find\n",
      "the leaf the new data point falls into. The output for this data point is the mean target\n",
      "of the training points in this leaf.\n",
      "Controlling complexity of decision trees\n",
      "Typically, building a tree as described here and continuing until all leaves are pure\n",
      "leads to models that are very complex and highly overfit to the training data. The\n",
      "presence of pure leaves mean that a tree is 100% accurate on the training set; each\n",
      "data point in the training set is in a leaf that has the correct majority class. The over‐\n",
      "fitting can be seen on the left of Figure 2-26. Y ou can see the regions determined to\n",
      "belong to class 1 in the middle of all the points belonging to class 0. On the other\n",
      "hand, there is a small strip predicted as class 0 around the point belonging to class 0\n",
      "to the very right. This is not how one would imagine the decision boundary to look,\n",
      "and the decision boundary focuses a lot on single outlier points that are far away\n",
      "from the other points in that class.\n",
      "There are two common strategies to prevent overfitting: stopping the creation of the\n",
      "tree early (also called pre-pruning), or building the tree but then removing or collaps‐\n",
      "ing nodes that contain little information (also called post-pruning or just pruning).\n",
      "Possible criteria for pre-pruning include limiting the maximum depth of the tree,\n",
      "limiting the maximum number of leaves, or requiring a minimum number of points\n",
      "in a node to keep splitting it.\n",
      "74 | Chapter 2: Supervised Learning\n",
      "Decision trees in scikit-learn are implemented in the DecisionTreeRegressor and\n",
      "DecisionTreeClassifier classes. scikit-learn only implements pre-pruning, not\n",
      "post-pruning.\n",
      "Let’s look at the effect of pre-pruning in more detail on the Breast Cancer dataset. As\n",
      "always, we import the dataset and split it into a training and a test part. Then we build\n",
      "a model using the default setting of fully developing the tree (growing the tree until\n",
      "all leaves are pure). We fix the random_state in the tree, which is used for tie-\n",
      "breaking internally:\n",
      "In[58]:\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "cancer = load_breast_cancer()\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
      "tree = DecisionTreeClassifier(random_state=0)\n",
      "tree.fit(X_train, y_train)\n",
      "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
      "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n",
      "Out[58]:\n",
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 0.937\n",
      "As expected, the accuracy on the training set is 100%—because the leaves are pure,\n",
      "the tree was grown deep enough that it could perfectly memorize all the labels on the\n",
      "training data. The test set accuracy is slightly worse than for the linear models we\n",
      "looked at previously, which had around 95% accuracy.\n",
      "If we don’t restrict the depth of a decision tree, the tree can become arbitrarily deep\n",
      "and complex. Unpruned trees are therefore prone to overfitting and not generalizing\n",
      "well to new data. Now let’s apply pre-pruning to the tree, which will stop developing\n",
      "the tree before we perfectly fit to the training data. One option is to stop building the\n",
      "tree after a certain depth has been reached. Here we set max_depth=4, meaning only\n",
      "four consecutive questions can be asked (cf. Figures 2-24 and 2-26). Limiting the\n",
      "depth of the tree decreases overfitting. This leads to a lower accuracy on the training\n",
      "set, but an improvement on the test set:\n",
      "In[59]:\n",
      "tree = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
      "tree.fit(X_train, y_train)\n",
      "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
      "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n",
      "Supervised Machine Learning Algorithms | 75\n",
      "Out[59]:\n",
      "Accuracy on training set: 0.988\n",
      "Accuracy on test set: 0.951\n",
      "Analyzing decision trees\n",
      "We can visualize the tree using the export_graphviz function from the tree module.\n",
      "This writes a file in the .dot file format, which is a text file format for storing graphs.\n",
      "We set an option to color the nodes to reflect the majority class in each node and pass\n",
      "the class and features names so the tree can be properly labeled:\n",
      "In[61]:\n",
      "from sklearn.tree import export_graphviz\n",
      "export_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"],\n",
      "                feature_names=cancer.feature_names, impurity=False, filled=True)\n",
      "We can read this file and visualize it, as seen in Figure 2-27, using the graphviz mod‐\n",
      "ule (or you can use any program that can read .dot files):\n",
      "In[61]:\n",
      "import graphviz\n",
      "with open(\"tree.dot\") as f:\n",
      "    dot_graph = f.read()\n",
      "graphviz.Source(dot_graph)\n",
      "Figure 2-27. Visualization of the decision tree built on the Breast Cancer dataset\n",
      "76 | Chapter 2: Supervised Learning\n",
      "The visualization of the tree provides a great in-depth view of how the algorithm\n",
      "makes predictions, and is a good example of a machine learning algorithm that is\n",
      "easily explained to nonexperts. However, even with a tree of depth four, as seen here,\n",
      "the tree can become a bit overwhelming. Deeper trees (a depth of 10 is not uncom‐\n",
      "mon) are even harder to grasp. One method of inspecting the tree that may be helpful\n",
      "is to find out which path most of the data actually takes. The n_samples shown in\n",
      "each node in Figure 2-27 gives the number of samples in that node, while value pro‐\n",
      "vides the number of samples per class. Following the branches to the right, we see\n",
      "that worst radius <= 16.795  creates a node that contains only 8 benign but 134\n",
      "malignant samples. The rest of this side of the tree then uses some finer distinctions\n",
      "to split off these 8 remaining benign samples. Of the 142 samples that went to the\n",
      "right in the initial split, nearly all of them (132) end up in the leaf to the very right.\n",
      "Taking a left at the root, for worst radius > 16.795  we end up with 25 malignant\n",
      "and 259 benign samples. Nearly all of the benign samples end up in the second leaf\n",
      "from the right, with most of the other leaves containing very few samples.\n",
      "Feature importance in trees\n",
      "Instead of looking at the whole tree, which can be taxing, there are some useful prop‐\n",
      "erties that we can derive to summarize the workings of the tree. The most commonly\n",
      "used summary is feature importance, which rates how important each feature is for\n",
      "the decision a tree makes. It is a number between 0 and 1 for each feature, where 0\n",
      "means “not used at all” and 1 means “perfectly predicts the target. ” The feature\n",
      "importances always sum to 1:\n",
      "In[62]:\n",
      "print(\"Feature importances:\\n{}\".format(tree.feature_importances_))\n",
      "Out[62]:\n",
      "Feature importances:\n",
      "[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.01\n",
      "  0.048  0.     0.     0.002  0.     0.     0.     0.     0.     0.727  0.046\n",
      "  0.     0.     0.014  0.     0.018  0.122  0.012  0.   ]\n",
      "We can visualize the feature importances in a way that is similar to the way we visual‐\n",
      "ize the coefficients in the linear model (Figure 2-28):\n",
      "In[63]:\n",
      "def plot_feature_importances_cancer(model):\n",
      "    n_features = cancer.data.shape[1]\n",
      "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
      "    plt.yticks(np.arange(n_features), cancer.feature_names)\n",
      "    plt.xlabel(\"Feature importance\")\n",
      "    plt.ylabel(\"Feature\")\n",
      "plot_feature_importances_cancer(tree)\n",
      "Supervised Machine Learning Algorithms | 77\n",
      "Figure 2-28. Feature importances computed from a decision tree learned on the Breast\n",
      "Cancer dataset\n",
      "Here we see that the feature used in the top split (“worst radius”) is by far the most\n",
      "important feature. This confirms our observation in analyzing the tree that the first\n",
      "level already separates the two classes fairly well.\n",
      "However, if a feature has a low feature_importance, it doesn’t mean that this feature\n",
      "is uninformative. It only means that the feature was not picked by the tree, likely\n",
      "because another feature encodes the same information.\n",
      "In contrast to the coefficients in linear models, feature importances are always posi‐\n",
      "tive, and don’t encode which class a feature is indicative of. The feature importances\n",
      "tell us that “worst radius” is important, but not whether a high radius is indicative of a\n",
      "sample being benign or malignant. In fact, there might not be such a simple relation‐\n",
      "ship between features and class, as you can see in the following example (Figures 2-29\n",
      "and 2-30):\n",
      "In[64]:\n",
      "tree = mglearn.plots.plot_tree_not_monotone()\n",
      "display(tree)\n",
      "Out[64]:\n",
      "Feature importances: [ 0.  1.]\n",
      "78 | Chapter 2: Supervised Learning\n",
      "Figure 2-29. A two-dimensional dataset in which the feature on the y-axis has a nonmo‐\n",
      "notonous relationship with the class label, and the decision boundaries found by a deci‐\n",
      "sion tree\n",
      "Figure 2-30. Decision tree learned on the data shown in Figure 2-29\n",
      "The plot shows a dataset with two features and two classes. Here, all the information\n",
      "is contained in X[1], and X[0] is not used at all. But the relation between X[1] and\n",
      "Supervised Machine Learning Algorithms | 79\n",
      "the output class is not monotonous, meaning we cannot say “a high value of X[0]\n",
      "means class 0, and a low value means class 1” (or vice versa).\n",
      "While we focused our discussion here on decision trees for classification, all that was\n",
      "said is similarly true for decision trees for regression, as implemented in Decision\n",
      "TreeRegressor. The usage and analysis of regression trees is very similar to that of\n",
      "classification trees. There is one particular property of using tree-based models for\n",
      "regression that we want to point out, though. The DecisionTreeRegressor (and all\n",
      "other tree-based regression models) is not able to extrapolate, or make predictions\n",
      "outside of the range of the training data.\n",
      "Let’s look into this in more detail, using a dataset of historical computer memory\n",
      "(RAM) prices. Figure 2-31 shows the dataset, with the date on the x-axis and the price\n",
      "of one megabyte of RAM in that year on the y-axis:\n",
      "In[65]:\n",
      "import pandas as pd\n",
      "ram_prices = pd.read_csv(\"data/ram_price.csv\")\n",
      "plt.semilogy(ram_prices.date, ram_prices.price)\n",
      "plt.xlabel(\"Year\")\n",
      "plt.ylabel(\"Price in $/Mbyte\")\n",
      "Figure 2-31. Historical development of the price of RAM, plotted on a log scale\n",
      "80 | Chapter 2: Supervised Learning\n",
      "Note the logarithmic scale of the y-axis. When plotting logarithmically, the relation\n",
      "seems to be quite linear and so should be relatively easy to predict, apart from some\n",
      "bumps.\n",
      "We will make a forecast for the years after 2000 using the historical data up to that\n",
      "point, with the date as our only feature. We will compare two simple models: a\n",
      "DecisionTreeRegressor and LinearRegression. We rescale the prices using a loga‐\n",
      "rithm, so that the relationship is relatively linear. This doesn’t make a difference for\n",
      "the DecisionTreeRegressor, but it makes a big difference for LinearRegression (we\n",
      "will discuss this in more depth in Chapter 4). After training the models and making\n",
      "predictions, we apply the exponential map to undo the logarithm transform. We\n",
      "make predictions on the whole dataset for visualization purposes here, but for a\n",
      "quantitative evaluation we would only consider the test dataset:\n",
      "In[66]:\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "# use historical data to forecast prices after the year 2000\n",
      "data_train = ram_prices[ram_prices.date < 2000]\n",
      "data_test = ram_prices[ram_prices.date >= 2000]\n",
      "# predict prices based on date\n",
      "X_train = data_train.date[:, np.newaxis]\n",
      "# we use a log-transform to get a simpler relationship of data to target\n",
      "y_train = np.log(data_train.price)\n",
      "tree = DecisionTreeRegressor().fit(X_train, y_train)\n",
      "linear_reg = LinearRegression().fit(X_train, y_train)\n",
      "# predict on all data\n",
      "X_all = ram_prices.date[:, np.newaxis]\n",
      "pred_tree = tree.predict(X_all)\n",
      "pred_lr = linear_reg.predict(X_all)\n",
      "# undo log-transform\n",
      "price_tree = np.exp(pred_tree)\n",
      "price_lr = np.exp(pred_lr)\n",
      "Figure 2-32, created here, compares the predictions of the decision tree and the linear\n",
      "regression model with the ground truth:\n",
      "In[67]:\n",
      "plt.semilogy(data_train.date, data_train.price, label=\"Training data\")\n",
      "plt.semilogy(data_test.date, data_test.price, label=\"Test data\")\n",
      "plt.semilogy(ram_prices.date, price_tree, label=\"Tree prediction\")\n",
      "plt.semilogy(ram_prices.date, price_lr, label=\"Linear prediction\")\n",
      "plt.legend()\n",
      "Supervised Machine Learning Algorithms | 81\n",
      "9 It is actually possible to make very good forecasts with tree-based models (for example, when trying to predict\n",
      "whether a price will go up or down). The point of this example was not to show that trees are a bad model for\n",
      "time series, but to illustrate a particular property of how trees make predictions.\n",
      "Figure 2-32. Comparison of predictions made by a linear model and predictions made\n",
      "by a regression tree on the RAM price data\n",
      "The difference between the models is quite striking. The linear model approximates\n",
      "the data with a line, as we knew it would. This line provides quite a good forecast for\n",
      "the test data (the years after 2000), while glossing over some of the finer variations in\n",
      "both the training and the test data. The tree model, on the other hand, makes perfect\n",
      "predictions on the training data; we did not restrict the complexity of the tree, so it\n",
      "learned the whole dataset by heart. However, once we leave the data range for which\n",
      "the model has data, the model simply keeps predicting the last known point. The tree\n",
      "has no ability to generate “new” responses, outside of what was seen in the training\n",
      "data. This shortcoming applies to all models based on trees.9\n",
      "Strengths, weaknesses, and parameters\n",
      "As discussed earlier, the parameters that control model complexity in decision trees\n",
      "are the pre-pruning parameters that stop the building of the tree before it is fully\n",
      "developed. Usually, picking one of the pre-pruning strategies—setting either\n",
      "82 | Chapter 2: Supervised Learning\n",
      "max_depth, max_leaf_nodes, or min_samples_leaf—is sufficient to prevent overfit‐\n",
      "ting.\n",
      "Decision trees have two advantages over many of the algorithms we’ve discussed so\n",
      "far: the resulting model can easily be visualized and understood by nonexperts (at\n",
      "least for smaller trees), and the algorithms are completely invariant to scaling of the\n",
      "data. As each feature is processed separately, and the possible splits of the data don’t\n",
      "depend on scaling, no preprocessing like normalization or standardization of features\n",
      "is needed for decision tree algorithms. In particular, decision trees work well when\n",
      "you have features that are on completely different scales, or a mix of binary and con‐\n",
      "tinuous features.\n",
      "The main downside of decision trees is that even with the use of pre-pruning, they\n",
      "tend to overfit and provide poor generalization performance. Therefore, in most\n",
      "applications, the ensemble methods we discuss next are usually used in place of a sin‐\n",
      "gle decision tree.\n",
      "Ensembles of Decision Trees\n",
      "Ensembles are methods that combine multiple machine learning models to create\n",
      "more powerful models. There are many models in the machine learning literature\n",
      "that belong to this category, but there are two ensemble models that have proven to\n",
      "be effective on a wide range of datasets for classification and regression, both of\n",
      "which use decision trees as their building blocks: random forests and gradient boos‐\n",
      "ted decision trees.\n",
      "Random forests\n",
      "As we just observed, a main drawback of decision trees is that they tend to overfit the\n",
      "training data. Random forests are one way to address this problem. A random forest\n",
      "is essentially a collection of decision trees, where each tree is slightly different from\n",
      "the others. The idea behind random forests is that each tree might do a relatively\n",
      "good job of predicting, but will likely overfit on part of the data. If we build many\n",
      "trees, all of which work well and overfit in different ways, we can reduce the amount\n",
      "of overfitting by averaging their results. This reduction in overfitting, while retaining\n",
      "the predictive power of the trees, can be shown using rigorous mathematics.\n",
      "To implement this strategy, we need to build many decision trees. Each tree should do\n",
      "an acceptable job of predicting the target, and should also be different from the other\n",
      "trees. Random forests get their name from injecting randomness into the tree build‐\n",
      "ing to ensure each tree is different. There are two ways in which the trees in a random\n",
      "forest are randomized: by selecting the data points used to build a tree and by select‐\n",
      "ing the features in each split test. Let’s go into this process in more detail.\n",
      "Supervised Machine Learning Algorithms | 83\n",
      "Building random forests.    To build a random forest model, you need to decide on the\n",
      "number of trees to build (the n_estimators parameter of RandomForestRegressor or\n",
      "RandomForestClassifier). Let’s say we want to build 10 trees. These trees will be\n",
      "built completely independently from each other, and the algorithm will make differ‐\n",
      "ent random choices for each tree to make sure the trees are distinct. To build a tree,\n",
      "we first take what is called a bootstrap sample of our data. That is, from our n_samples\n",
      "data points, we repeatedly draw an example randomly with replacement (meaning the\n",
      "same sample can be picked multiple times), n_samples times. This will create a data‐\n",
      "set that is as big as the original dataset, but some data points will be missing from it\n",
      "(approximately one third), and some will be repeated.\n",
      "To illustrate, let’s say we want to create a bootstrap sample of the list ['a', 'b',\n",
      "'c', 'd']. A possible bootstrap sample would be ['b', 'd', 'd', 'c']. Another\n",
      "possible sample would be ['d', 'a', 'd', 'a'].\n",
      "Next, a decision tree is built based on this newly created dataset. However, the algo‐\n",
      "rithm we described for the decision tree is slightly modified. Instead of looking for\n",
      "the best test for each node, in each node the algorithm randomly selects a subset of\n",
      "the features, and it looks for the best possible test involving one of these features. The\n",
      "number of features that are selected is controlled by the max_features parameter.\n",
      "This selection of a subset of features is repeated separately in each node, so that each\n",
      "node in a tree can make a decision using a different subset of the features.\n",
      "The bootstrap sampling leads to each decision tree in the random forest being built\n",
      "on a slightly different dataset. Because of the selection of features in each node, each\n",
      "split in each tree operates on a different subset of features. Together, these two mech‐\n",
      "anisms ensure that all the trees in the random forest are different.\n",
      "A critical parameter in this process is max_features. If we set max_features to n_fea\n",
      "tures, that means that each split can look at all features in the dataset, and no ran‐\n",
      "domness will be injected in the feature selection (the randomness due to the\n",
      "bootstrapping remains, though). If we set max_features to 1, that means that the\n",
      "splits have no choice at all on which feature to test, and can only search over different\n",
      "thresholds for the feature that was selected randomly. Therefore, a high max_fea\n",
      "tures means that the trees in the random forest will be quite similar, and they will be\n",
      "able to fit the data easily, using the most distinctive features. A low max_features\n",
      "means that the trees in the random forest will be quite different, and that each tree\n",
      "might need to be very deep in order to fit the data well.\n",
      "To make a prediction using the random forest, the algorithm first makes a prediction\n",
      "for every tree in the forest. For regression, we can average these results to get our final\n",
      "prediction. For classification, a “soft voting” strategy is used. This means each algo‐\n",
      "rithm makes a “soft” prediction, providing a probability for each possible output\n",
      "84 | Chapter 2: Supervised Learning\n",
      "label. The probabilities predicted by all the trees are averaged, and the class with the\n",
      "highest probability is predicted.\n",
      "Analyzing random forests.    Let’s apply a random forest consisting of five trees to the\n",
      "two_moons dataset we studied earlier:\n",
      "In[68]:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.datasets import make_moons\n",
      "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
      "                                                    random_state=42)\n",
      "forest = RandomForestClassifier(n_estimators=5, random_state=2)\n",
      "forest.fit(X_train, y_train)\n",
      "The trees that are built as part of the random forest are stored in the estimator_\n",
      "attribute. Let’s visualize the decision boundaries learned by each tree, together with\n",
      "their aggregate prediction as made by the forest (Figure 2-33):\n",
      "In[69]:\n",
      "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
      "for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\n",
      "    ax.set_title(\"Tree {}\".format(i))\n",
      "    mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)\n",
      "mglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1],\n",
      "                                alpha=.4)\n",
      "axes[-1, -1].set_title(\"Random Forest\")\n",
      "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
      "Y ou can clearly see that the decision boundaries learned by the five trees are quite dif‐\n",
      "ferent. Each of them makes some mistakes, as some of the training points that are\n",
      "plotted here were not actually included in the training sets of the trees, due to the\n",
      "bootstrap sampling.\n",
      "The random forest overfits less than any of the trees individually, and provides a\n",
      "much more intuitive decision boundary. In any real application, we would use many\n",
      "more trees (often hundreds or thousands), leading to even smoother boundaries.\n",
      "Supervised Machine Learning Algorithms | 85\n",
      "Figure 2-33. Decision boundaries found by five randomized decision trees and the deci‐\n",
      "sion boundary obtained by averaging their predicted probabilities\n",
      "As another example, let’s apply a random forest consisting of 100 trees on the Breast\n",
      "Cancer dataset:\n",
      "In[70]:\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    cancer.data, cancer.target, random_state=0)\n",
      "forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
      "forest.fit(X_train, y_train)\n",
      "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\n",
      "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))\n",
      "Out[70]:\n",
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 0.972\n",
      "The random forest gives us an accuracy of 97%, better than the linear models or a\n",
      "single decision tree, without tuning any parameters. We could adjust the max_fea\n",
      "tures setting, or apply pre-pruning as we did for the single decision tree. However,\n",
      "often the default parameters of the random forest already work quite well.\n",
      "Similarly to the decision tree, the random forest provides feature importances, which\n",
      "are computed by aggregating the feature importances over the trees in the forest. Typ‐\n",
      "ically, the feature importances provided by the random forest are more reliable than\n",
      "the ones provided by a single tree. Take a look at Figure 2-34.\n",
      "86 | Chapter 2: Supervised Learning\n",
      "In[71]:\n",
      "plot_feature_importances_cancer(forest)\n",
      "Figure 2-34. Feature importances computed from a random forest that was fit to the\n",
      "Breast Cancer dataset\n",
      "As you can see, the random forest gives nonzero importance to many more features\n",
      "than the single tree. Similarly to the single decision tree, the random forest also gives\n",
      "a lot of importance to the “worst radius” feature, but it actually chooses “worst perim‐\n",
      "eter” to be the most informative feature overall. The randomness in building the ran‐\n",
      "dom forest forces the algorithm to consider many possible explanations, the result\n",
      "being that the random forest captures a much broader picture of the data than a sin‐\n",
      "gle tree.\n",
      "Strengths, weaknesses, and parameters.    Random forests for regression and classifica‐\n",
      "tion are currently among the most widely used machine learning methods. They are\n",
      "very powerful, often work well without heavy tuning of the parameters, and don’t\n",
      "require scaling of the data.\n",
      "Essentially, random forests share all of the benefits of decision trees, while making up\n",
      "for some of their deficiencies. One reason to still use decision trees is if you need a\n",
      "compact representation of the decision-making process. It is basically impossible to\n",
      "interpret tens or hundreds of trees in detail, and trees in random forests tend to be\n",
      "deeper than decision trees (because of the use of feature subsets). Therefore, if you\n",
      "need to summarize the prediction making in a visual way to nonexperts, a single\n",
      "decision tree might be a better choice. While building random forests on large data‐\n",
      "sets might be somewhat time consuming, it can be parallelized across multiple CPU\n",
      "Supervised Machine Learning Algorithms | 87\n",
      "cores within a computer easily. If you are using a multi-core processor (as nearly all\n",
      "modern computers do), you can use the n_jobs parameter to adjust the number of\n",
      "cores to use. Using more CPU cores will result in linear speed-ups (using two cores,\n",
      "the training of the random forest will be twice as fast), but specifying n_jobs larger\n",
      "than the number of cores will not help. Y ou can set n_jobs=-1 to use all the cores in\n",
      "your computer.\n",
      "Y ou should keep in mind that random forests, by their nature, are random, and set‐\n",
      "ting different random states (or not setting the random_state at all) can drastically\n",
      "change the model that is built. The more trees there are in the forest, the more robust\n",
      "it will be against the choice of random state. If you want to have reproducible results,\n",
      "it is important to fix the random_state.\n",
      "Random forests don’t tend to perform well on very high dimensional, sparse data,\n",
      "such as text data. For this kind of data, linear models might be more appropriate.\n",
      "Random forests usually work well even on very large datasets, and training can easily\n",
      "be parallelized over many CPU cores within a powerful computer. However, random\n",
      "forests require more memory and are slower to train and to predict than linear mod‐\n",
      "els. If time and memory are important in an application, it might make sense to use a\n",
      "linear model instead.\n",
      "The important parameters to adjust are n_estimators, max_features, and possibly\n",
      "pre-pruning options like max_depth. For n_estimators, larger is always better. Aver‐\n",
      "aging more trees will yield a more robust ensemble by reducing overfitting. However,\n",
      "there are diminishing returns, and more trees need more memory and more time to\n",
      "train. A common rule of thumb is to build “as many as you have time/memory for. ”\n",
      "As described earlier, max_features determines how random each tree is, and a\n",
      "smaller max_features reduces overfitting. In general, it’s a good rule of thumb to use\n",
      "the default values: max_features=sqrt(n_features) for classification and max_fea\n",
      "tures=log2(n_features) for regression. Adding max_features or max_leaf_nodes\n",
      "might sometimes improve performance. It can also drastically reduce space and time\n",
      "requirements for training and prediction.\n",
      "Gradient boosted regression trees (gradient boosting machines)\n",
      "The gradient boosted regression tree is another ensemble method that combines mul‐\n",
      "tiple decision trees to create a more powerful model. Despite the “regression” in the\n",
      "name, these models can be used for regression and classification. In contrast to the\n",
      "random forest approach, gradient boosting works by building trees in a serial man‐\n",
      "ner, where each tree tries to correct the mistakes of the previous one. By default, there\n",
      "is no randomization in gradient boosted regression trees; instead, strong pre-pruning\n",
      "is used. Gradient boosted trees often use very shallow trees, of depth one to five,\n",
      "which makes the model smaller in terms of memory and makes predictions faster.\n",
      "88 | Chapter 2: Supervised Learning\n",
      "The main idea behind gradient boosting is to combine many simple models (in this\n",
      "context known as weak learners), like shallow trees. Each tree can only provide good\n",
      "predictions on part of the data, and so more and more trees are added to iteratively\n",
      "improve performance.\n",
      "Gradient boosted trees are frequently the winning entries in machine learning com‐\n",
      "petitions, and are widely used in industry. They are generally a bit more sensitive to\n",
      "parameter settings than random forests, but can provide better accuracy if the param‐\n",
      "eters are set correctly.\n",
      "Apart from the pre-pruning and the number of trees in the ensemble, another impor‐\n",
      "tant parameter of gradient boosting is the learning_rate, which controls how\n",
      "strongly each tree tries to correct the mistakes of the previous trees. A higher learning\n",
      "rate means each tree can make stronger corrections, allowing for more complex mod‐\n",
      "els. Adding more trees to the ensemble, which can be accomplished by increasing\n",
      "n_estimators, also increases the model complexity, as the model has more chances\n",
      "to correct mistakes on the training set.\n",
      "Here is an example of using GradientBoostingClassifier on the Breast Cancer\n",
      "dataset. By default, 100 trees of maximum depth 3 and a learning rate of 0.1 are used:\n",
      "In[72]:\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    cancer.data, cancer.target, random_state=0)\n",
      "gbrt = GradientBoostingClassifier(random_state=0)\n",
      "gbrt.fit(X_train, y_train)\n",
      "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
      "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\n",
      "Out[72]:\n",
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 0.958\n",
      "As the training set accuracy is 100%, we are likely to be overfitting. To reduce overfit‐\n",
      "ting, we could either apply stronger pre-pruning by limiting the maximum depth or\n",
      "lower the learning rate:\n",
      "Supervised Machine Learning Algorithms | 89\n",
      "In[73]:\n",
      "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
      "gbrt.fit(X_train, y_train)\n",
      "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
      "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\n",
      "Out[73]:\n",
      "Accuracy on training set: 0.991\n",
      "Accuracy on test set: 0.972\n",
      "In[74]:\n",
      "gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\n",
      "gbrt.fit(X_train, y_train)\n",
      "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
      "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\n",
      "Out[74]:\n",
      "Accuracy on training set: 0.988\n",
      "Accuracy on test set: 0.965\n",
      "Both methods of decreasing the model complexity reduced the training set accuracy,\n",
      "as expected. In this case, lowering the maximum depth of the trees provided a signifi‐\n",
      "cant improvement of the model, while lowering the learning rate only increased the\n",
      "generalization performance slightly.\n",
      "As for the other decision tree–based models, we can again visualize the feature\n",
      "importances to get more insight into our model (Figure 2-35). As we used 100 trees, it\n",
      "is impractical to inspect them all, even if they are all of depth 1:\n",
      "In[75]:\n",
      "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
      "gbrt.fit(X_train, y_train)\n",
      "plot_feature_importances_cancer(gbrt)\n",
      "90 | Chapter 2: Supervised Learning\n",
      "Figure 2-35. Feature importances computed from a gradient boosting classifier that was\n",
      "fit to the Breast Cancer dataset\n",
      "We can see that the feature importances of the gradient boosted trees are somewhat\n",
      "similar to the feature importances of the random forests, though the gradient boost‐\n",
      "ing completely ignored some of the features.\n",
      "As both gradient boosting and random forests perform well on similar kinds of data,\n",
      "a common approach is to first try random forests, which work quite robustly. If ran‐\n",
      "dom forests work well but prediction time is at a premium, or it is important to\n",
      "squeeze out the last percentage of accuracy from the machine learning model, mov‐\n",
      "ing to gradient boosting often helps.\n",
      "If you want to apply gradient boosting to a large-scale problem, it might be worth\n",
      "looking into the xgboost package and its Python interface, which at the time of writ‐\n",
      "ing is faster (and sometimes easier to tune) than the scikit-learn implementation of\n",
      "gradient boosting on many datasets.\n",
      "Strengths, weaknesses, and parameters.    Gradient boosted decision trees are among the\n",
      "most powerful and widely used models for supervised learning. Their main drawback\n",
      "is that they require careful tuning of the parameters and may take a long time to\n",
      "train. Similarly to other tree-based models, the algorithm works well without scaling\n",
      "and on a mixture of binary and continuous features. As with other tree-based models,\n",
      "it also often does not work well on high-dimensional sparse data.\n",
      "The main parameters of gradient boosted tree models are the number of trees, n_esti\n",
      "mators, and the learning_rate, which controls the degree to which each tree is\n",
      "allowed to correct the mistakes of the previous trees. These two parameters are highly\n",
      "Supervised Machine Learning Algorithms | 91\n",
      "interconnected, as a lower learning_rate means that more trees are needed to build\n",
      "a model of similar complexity. In contrast to random forests, where a higher n_esti\n",
      "mators value is always better, increasing n_estimators in gradient boosting leads to a\n",
      "more complex model, which may lead to overfitting. A common practice is to fit\n",
      "n_estimators depending on the time and memory budget, and then search over dif‐\n",
      "ferent learning_rates.\n",
      "Another important parameter is max_depth (or alternatively max_leaf_nodes), to\n",
      "reduce the complexity of each tree. Usually max_depth is set very low for gradient\n",
      "boosted models, often not deeper than five splits.\n",
      "Kernelized Support Vector Machines\n",
      "The next type of supervised model we will discuss is kernelized support vector\n",
      "machines. We explored the use of linear support vector machines for classification in\n",
      "“Linear models for classification” on page 56. Kernelized support vector machines\n",
      "(often just referred to as SVMs) are an extension that allows for more complex mod‐\n",
      "els that are not defined simply by hyperplanes in the input space. While there are sup‐\n",
      "port vector machines for classification and regression, we will restrict ourselves to the\n",
      "classification case, as implemented in SVC. Similar concepts apply to support vector\n",
      "regression, as implemented in SVR.\n",
      "The math behind kernelized support vector machines is a bit involved, and is beyond\n",
      "the scope of this book. Y ou can find the details in Chapter 1 of Hastie, Tibshirani, and\n",
      "Friedman’s The Elements of Statistical Learning. However, we will try to give you some\n",
      "sense of the idea behind the method.\n",
      "Linear models and nonlinear features\n",
      "As you saw in Figure 2-15 , linear models can be quite limiting in low-dimensional\n",
      "spaces, as lines and hyperplanes have limited flexibility. One way to make a linear\n",
      "model more flexible is by adding more features—for example, by adding interactions\n",
      "or polynomials of the input features.\n",
      "Let’s look at the synthetic dataset we used in “Feature importance in trees” on page 77\n",
      "(see Figure 2-29):\n",
      "In[76]:\n",
      "X, y = make_blobs(centers=4, random_state=8)\n",
      "y = y % 2\n",
      "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "92 | Chapter 2: Supervised Learning\n",
      "10 We picked this particular feature to add for illustration purposes. The choice is not particularly important.\n",
      "Figure 2-36. Two-class classification dataset in which classes are not linearly separable\n",
      "A linear model for classification can only separate points using a line, and will not be\n",
      "able to do a very good job on this dataset (see Figure 2-37):\n",
      "In[77]:\n",
      "from sklearn.svm import LinearSVC\n",
      "linear_svm = LinearSVC().fit(X, y)\n",
      "mglearn.plots.plot_2d_separator(linear_svm, X)\n",
      "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "Now let’s expand the set of input features, say by also adding feature1 ** 2 , the\n",
      "square of the second feature, as a new feature. Instead of representing each data point\n",
      "as a two-dimensional point, (feature0, feature1), we now represent it as a three-\n",
      "dimensional point, (feature0, feature1, feature1 ** 2).10 This new representa‐\n",
      "tion is illustrated in Figure 2-38 in a three-dimensional scatter plot:\n",
      "Supervised Machine Learning Algorithms | 93\n",
      "Figure 2-37. Decision boundary found by a linear SVM\n",
      "In[78]:\n",
      "# add the squared first feature\n",
      "X_new = np.hstack([X, X[:, 1:] ** 2])\n",
      "from mpl_toolkits.mplot3d import Axes3D, axes3d\n",
      "figure = plt.figure()\n",
      "# visualize in 3D\n",
      "ax = Axes3D(figure, elev=-152, azim=-26)\n",
      "# plot first all the points with y == 0, then all with y == 1\n",
      "mask = y == 0\n",
      "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
      "           cmap=mglearn.cm2, s=60)\n",
      "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
      "           cmap=mglearn.cm2, s=60)\n",
      "ax.set_xlabel(\"feature0\")\n",
      "ax.set_ylabel(\"feature1\")\n",
      "ax.set_zlabel(\"feature1 ** 2\")\n",
      "94 | Chapter 2: Supervised Learning\n",
      "Figure 2-38. Expansion of the dataset shown in Figure 2-37, created by adding a third\n",
      "feature derived from feature1\n",
      "In the new representation of the data, it is now indeed possible to separate the two\n",
      "classes using a linear model, a plane in three dimensions. We can confirm this by fit‐\n",
      "ting a linear model to the augmented data (see Figure 2-39):\n",
      "In[79]:\n",
      "linear_svm_3d = LinearSVC().fit(X_new, y)\n",
      "coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n",
      "# show linear decision boundary\n",
      "figure = plt.figure()\n",
      "ax = Axes3D(figure, elev=-152, azim=-26)\n",
      "xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\n",
      "yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\n",
      "XX, YY = np.meshgrid(xx, yy)\n",
      "ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\n",
      "ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\n",
      "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
      "           cmap=mglearn.cm2, s=60)\n",
      "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
      "           cmap=mglearn.cm2, s=60)\n",
      "ax.set_xlabel(\"feature0\")\n",
      "ax.set_ylabel(\"feature1\")\n",
      "ax.set_zlabel(\"feature0 ** 2\")\n",
      "Supervised Machine Learning Algorithms | 95\n",
      "Figure 2-39. Decision boundary found by a linear SVM on the expanded three-\n",
      "dimensional dataset\n",
      "As a function of the original features, the linear SVM model is not actually linear any‐\n",
      "more. It is not a line, but more of an ellipse, as you can see from the plot created here\n",
      "(Figure 2-40):\n",
      "In[80]:\n",
      "ZZ = YY ** 2\n",
      "dec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\n",
      "plt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\n",
      "             cmap=mglearn.cm2, alpha=0.5)\n",
      "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "96 | Chapter 2: Supervised Learning\n",
      "Figure 2-40. The decision boundary from Figure 2-39 as a function of the original two\n",
      "features\n",
      "The kernel trick\n",
      "The lesson here is that adding nonlinear features to the representation of our data can\n",
      "make linear models much more powerful. However, often we don’t know which fea‐\n",
      "tures to add, and adding many features (like all possible interactions in a 100-\n",
      "dimensional feature space) might make computation very expensive. Luckily, there is\n",
      "a clever mathematical trick that allows us to learn a classifier in a higher-dimensional\n",
      "space without actually computing the new, possibly very large representation. This is\n",
      "known as the kernel trick, and it works by directly computing the distance (more pre‐\n",
      "cisely, the scalar products) of the data points for the expanded feature representation,\n",
      "without ever actually computing the expansion.\n",
      "There are two ways to map your data into a higher-dimensional space that are com‐\n",
      "monly used with support vector machines: the polynomial kernel, which computes all\n",
      "possible polynomials up to a certain degree of the original features (like feature1 **\n",
      "2 * feature2 ** 5 ); and the radial basis function (RBF) kernel, also known as the\n",
      "Gaussian kernel. The Gaussian kernel is a bit harder to explain, as it corresponds to\n",
      "an infinite-dimensional feature space. One way to explain the Gaussian kernel is that\n",
      "Supervised Machine Learning Algorithms | 97\n",
      "11 This follows from the Taylor expansion of the exponential map.\n",
      "it considers all possible polynomials of all degrees, but the importance of the features\n",
      "decreases for higher degrees.11\n",
      "In practice, the mathematical details behind the kernel SVM are not that important,\n",
      "though, and how an SVM with an RBF kernel makes a decision can be summarized\n",
      "quite easily—we’ll do so in the next section.\n",
      "Understanding SVMs\n",
      "During training, the SVM learns how important each of the training data points is to\n",
      "represent the decision boundary between the two classes. Typically only a subset of\n",
      "the training points matter for defining the decision boundary: the ones that lie on the\n",
      "border between the classes. These are called support vectors and give the support vec‐\n",
      "tor machine its name.\n",
      "To make a prediction for a new point, the distance to each of the support vectors is\n",
      "measured. A classification decision is made based on the distances to the support vec‐\n",
      "tor, and the importance of the support vectors that was learned during training\n",
      "(stored in the dual_coef_ attribute of SVC).\n",
      "The distance between data points is measured by the Gaussian kernel:\n",
      "krbf(x1, x2) = exp (ɣǁx1 - x2ǁ2)\n",
      "Here, x1 and x2 are data points, ǁ x1 - x2 ǁ denotes Euclidean distance, and ɣ (gamma)\n",
      "is a parameter that controls the width of the Gaussian kernel.\n",
      "Figure 2-41  shows the result of training a support vector machine on a two-\n",
      "dimensional two-class dataset. The decision boundary is shown in black, and the sup‐\n",
      "port vectors are larger points with the wide outline. The following code creates this\n",
      "plot by training an SVM on the forge dataset:\n",
      "In[81]:\n",
      "from sklearn.svm import SVC\n",
      "X, y = mglearn.tools.make_handcrafted_dataset()\n",
      "svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)\n",
      "mglearn.plots.plot_2d_separator(svm, X, eps=.5)\n",
      "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
      "# plot support vectors\n",
      "sv = svm.support_vectors_\n",
      "# class labels of support vectors are given by the sign of the dual coefficients\n",
      "sv_labels = svm.dual_coef_.ravel() > 0\n",
      "mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "98 | Chapter 2: Supervised Learning\n",
      "Figure 2-41. Decision boundary and support vectors found by an SVM with RBF kernel\n",
      "In this case, the SVM yields a very smooth and nonlinear (not a straight line) bound‐\n",
      "ary. We adjusted two parameters here: the C parameter and the gamma parameter,\n",
      "which we will now discuss in detail.\n",
      "Tuning SVM parameters\n",
      "The gamma parameter is the one shown in the formula given in the previous section,\n",
      "which controls the width of the Gaussian kernel. It determines the scale of what it\n",
      "means for points to be close together. The C parameter is a regularization parameter,\n",
      "similar to that used in the linear models. It limits the importance of each point (or\n",
      "more precisely, their dual_coef_).\n",
      "Let’s have a look at what happens when we vary these parameters (Figure 2-42):\n",
      "In[82]:\n",
      "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
      "for ax, C in zip(axes, [-1, 0, 3]):\n",
      "    for a, gamma in zip(ax, range(-1, 2)):\n",
      "        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\n",
      "axes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\n",
      "                  ncol=4, loc=(.9, 1.2))\n",
      "Supervised Machine Learning Algorithms | 99\n",
      "Figure 2-42. Decision boundaries and support vectors for different settings of the param‐\n",
      "eters C and gamma\n",
      "Going from left to right, we increase the value of the parameter gamma from 0.1 to 10.\n",
      "A small gamma means a large radius for the Gaussian kernel, which means that many\n",
      "points are considered close by. This is reflected in very smooth decision boundaries\n",
      "on the left, and boundaries that focus more on single points further to the right. A\n",
      "low value of gamma means that the decision boundary will vary slowly, which yields a\n",
      "model of low complexity, while a high value of gamma yields a more complex model.\n",
      "Going from top to bottom, we increase the C parameter from 0.1 to 1000. As with the\n",
      "linear models, a small C means a very restricted model, where each data point can\n",
      "only have very limited influence. Y ou can see that at the top left the decision bound‐\n",
      "ary looks nearly linear, with the misclassified points barely having any influence on\n",
      "the line. Increasing C, as shown on the bottom right, allows these points to have a\n",
      "stronger influence on the model and makes the decision boundary bend to correctly\n",
      "classify them.\n",
      "100 | Chapter 2: Supervised Learning\n",
      "Let’s apply the RBF kernel SVM to the Breast Cancer dataset. By default, C=1 and\n",
      "gamma=1/n_features:\n",
      "In[83]:\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    cancer.data, cancer.target, random_state=0)\n",
      "svc = SVC()\n",
      "svc.fit(X_train, y_train)\n",
      "print(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\n",
      "print(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))\n",
      "Out[83]:\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.63\n",
      "The model overfits quite substantially, with a perfect score on the training set and\n",
      "only 63% accuracy on the test set. While SVMs often perform quite well, they are\n",
      "very sensitive to the settings of the parameters and to the scaling of the data. In par‐\n",
      "ticular, they require all the features to vary on a similar scale. Let’s look at the mini‐\n",
      "mum and maximum values for each feature, plotted in log-space (Figure 2-43):\n",
      "In[84]:\n",
      "plt.plot(X_train.min(axis=0), 'o', label=\"min\")\n",
      "plt.plot(X_train.max(axis=0), '^', label=\"max\")\n",
      "plt.legend(loc=4)\n",
      "plt.xlabel(\"Feature index\")\n",
      "plt.ylabel(\"Feature magnitude\")\n",
      "plt.yscale(\"log\")\n",
      "From this plot we can determine that features in the Breast Cancer dataset are of\n",
      "completely different orders of magnitude. This can be somewhat of a problem for\n",
      "other models (like linear models), but it has devastating effects for the kernel SVM.\n",
      "Let’s examine some ways to deal with this issue.\n",
      "Supervised Machine Learning Algorithms | 101\n",
      "Figure 2-43. Feature ranges for the Breast Cancer dataset (note that the y axis has a log‐\n",
      "arithmic scale)\n",
      "Preprocessing data for SVMs\n",
      "One way to resolve this problem is by rescaling each feature so that they are all\n",
      "approximately on the same scale. A common rescaling method for kernel SVMs is to\n",
      "scale the data such that all features are between 0 and 1. We will see how to do this\n",
      "using the MinMaxScaler preprocessing method in Chapter 3, where we’ll give more\n",
      "details. For now, let’s do this “by hand”:\n",
      "In[85]:\n",
      "# compute the minimum value per feature on the training set\n",
      "min_on_training = X_train.min(axis=0)\n",
      "# compute the range of each feature (max - min) on the training set\n",
      "range_on_training = (X_train - min_on_training).max(axis=0)\n",
      "# subtract the min, and divide by range\n",
      "# afterward, min=0 and max=1 for each feature\n",
      "X_train_scaled = (X_train - min_on_training) / range_on_training\n",
      "print(\"Minimum for each feature\\n{}\".format(X_train_scaled.min(axis=0)))\n",
      "print(\"Maximum for each feature\\n {}\".format(X_train_scaled.max(axis=0)))\n",
      "102 | Chapter 2: Supervised Learning\n",
      "Out[85]:\n",
      "Minimum for each feature\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Maximum for each feature\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "In[86]:\n",
      "# use THE SAME transformation on the test set,\n",
      "# using min and range of the training set (see Chapter 3 for details)\n",
      "X_test_scaled = (X_test - min_on_training) / range_on_training\n",
      "In[87]:\n",
      "svc = SVC()\n",
      "svc.fit(X_train_scaled, y_train)\n",
      "print(\"Accuracy on training set: {:.3f}\".format(\n",
      "    svc.score(X_train_scaled, y_train)))\n",
      "print(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\n",
      "Out[87]:\n",
      "Accuracy on training set: 0.948\n",
      "Accuracy on test set: 0.951\n",
      "Scaling the data made a huge difference! Now we are actually in an underfitting\n",
      "regime, where training and test set performance are quite similar but less close to\n",
      "100% accuracy. From here, we can try increasing either C or gamma to fit a more com‐\n",
      "plex model. For example:\n",
      "In[88]:\n",
      "svc = SVC(C=1000)\n",
      "svc.fit(X_train_scaled, y_train)\n",
      "print(\"Accuracy on training set: {:.3f}\".format(\n",
      "    svc.score(X_train_scaled, y_train)))\n",
      "print(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\n",
      "Out[88]:\n",
      "Accuracy on training set: 0.988\n",
      "Accuracy on test set: 0.972\n",
      "Here, increasing C allows us to improve the model significantly, resulting in 97.2%\n",
      "accuracy.\n",
      "Supervised Machine Learning Algorithms | 103\n",
      "Strengths, weaknesses, and parameters\n",
      "Kernelized support vector machines are powerful models and perform well on a vari‐\n",
      "ety of datasets. SVMs allow for complex decision boundaries, even if the data has only\n",
      "a few features. They work well on low-dimensional and high-dimensional data (i.e.,\n",
      "few and many features), but don’t scale very well with the number of samples. Run‐\n",
      "ning an SVM on data with up to 10,000 samples might work well, but working with\n",
      "datasets of size 100,000 or more can become challenging in terms of runtime and\n",
      "memory usage.\n",
      "Another downside of SVMs is that they require careful preprocessing of the data and\n",
      "tuning of the parameters. This is why, these days, most people instead use tree-based\n",
      "models such as random forests or gradient boosting (which require little or no pre‐\n",
      "processing) in many applications. Furthermore, SVM models are hard to inspect; it\n",
      "can be difficult to understand why a particular prediction was made, and it might be\n",
      "tricky to explain the model to a nonexpert.\n",
      "Still, it might be worth trying SVMs, particularly if all of your features represent\n",
      "measurements in similar units (e.g., all are pixel intensities) and they are on similar\n",
      "scales.\n",
      "The important parameters in kernel SVMs are the regularization parameter C, the\n",
      "choice of the kernel, and the kernel-specific parameters. Although we primarily\n",
      "focused on the RBF kernel, other choices are available in scikit-learn. The RBF\n",
      "kernel has only one parameter, gamma, which is the inverse of the width of the Gaus‐\n",
      "sian kernel. gamma and C both control the complexity of the model, with large values\n",
      "in either resulting in a more complex model. Therefore, good settings for the two\n",
      "parameters are usually strongly correlated, and C and gamma should be adjusted\n",
      "together.\n",
      "Neural Networks (Deep Learning)\n",
      "A family of algorithms known as neural networks has recently seen a revival under\n",
      "the name “deep learning. ” While deep learning shows great promise in many machine\n",
      "learning applications, deep learning algorithms are often tailored very carefully to a\n",
      "specific use case. Here, we will only discuss some relatively simple methods, namely\n",
      "multilayer perceptrons for classification and regression, that can serve as a starting\n",
      "point for more involved deep learning methods. Multilayer perceptrons (MLPs) are\n",
      "also known as (vanilla) feed-forward neural networks, or sometimes just neural\n",
      "networks.\n",
      "The neural network model\n",
      "MLPs can be viewed as generalizations of linear models that perform multiple stages\n",
      "of processing to come to a decision.\n",
      "104 | Chapter 2: Supervised Learning\n",
      "Remember that the prediction by a linear regressor is given as:\n",
      "ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\n",
      "In plain English, ŷ is a weighted sum of the input features x[0] to x[p], weighted by\n",
      "the learned coefficients w[0] to w[p]. We could visualize this graphically as shown in\n",
      "Figure 2-44:\n",
      "In[89]:\n",
      "display(mglearn.plots.plot_logistic_regression_graph())\n",
      "Figure 2-44. Visualization of logistic regression, where input features and predictions are\n",
      "shown as nodes, and the coefficients are connections between the nodes\n",
      "Here, each node on the left represents an input feature, the connecting lines represent\n",
      "the learned coefficients, and the node on the right represents the output, which is a\n",
      "weighted sum of the inputs.\n",
      "In an MLP this process of computing weighted sums is repeated multiple times, first\n",
      "computing hidden units that represent an intermediate processing step, which are\n",
      "again combined using weighted sums to yield the final result (Figure 2-45):\n",
      "In[90]:\n",
      "display(mglearn.plots.plot_single_hidden_layer_graph())\n",
      "Supervised Machine Learning Algorithms | 105\n",
      "Figure 2-45. Illustration of a multilayer perceptron with a single hidden layer\n",
      "This model has a lot more coefficients (also called weights) to learn: there is one\n",
      "between every input and every hidden unit (which make up the hidden layer), and\n",
      "one between every unit in the hidden layer and the output.\n",
      "Computing a series of weighted sums is mathematically the same as computing just\n",
      "one weighted sum, so to make this model truly more powerful than a linear model,\n",
      "we need one extra trick. After computing a weighted sum for each hidden unit, a\n",
      "nonlinear function is applied to the result—usually the rectifying nonlinearity (also\n",
      "known as rectified linear unit or relu) or the tangens hyperbolicus (tanh). The result of\n",
      "this function is then used in the weighted sum that computes the output, ŷ. The two\n",
      "functions are visualized in Figure 2-46. The relu cuts off values below zero, while tanh\n",
      "saturates to –1 for low input values and +1 for high input values. Either nonlinear\n",
      "function allows the neural network to learn much more complicated functions than a\n",
      "linear model could:\n",
      "In[91]:\n",
      "line = np.linspace(-3, 3, 100)\n",
      "plt.plot(line, np.tanh(line), label=\"tanh\")\n",
      "plt.plot(line, np.maximum(line, 0), label=\"relu\")\n",
      "plt.legend(loc=\"best\")\n",
      "plt.xlabel(\"x\")\n",
      "plt.ylabel(\"relu(x), tanh(x)\")\n",
      "106 | Chapter 2: Supervised Learning\n",
      "Figure 2-46. The hyperbolic tangent activation function and the rectified linear activa‐\n",
      "tion function\n",
      "For the small neural network pictured in Figure 2-45, the full formula for computing\n",
      "ŷ in the case of regression would be (when using a tanh nonlinearity):\n",
      "h[0] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\n",
      "h[1] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\n",
      "h[2] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\n",
      "ŷ = v[0] * h[0] + v[1] * h[1] + v[2] * h[2]\n",
      "Here, w are the weights between the input x and the hidden layer h, and v are the\n",
      "weights between the hidden layer h and the output ŷ. The weights v and w are learned\n",
      "from data, x are the input features, ŷ is the computed output, and h are intermediate\n",
      "computations. An important parameter that needs to be set by the user is the number\n",
      "of nodes in the hidden layer. This can be as small as 10 for very small or simple data‐\n",
      "sets and as big as 10,000 for very complex data. It is also possible to add additional\n",
      "hidden layers, as shown in Figure 2-47:\n",
      "Supervised Machine Learning Algorithms | 107\n",
      "In[92]:\n",
      "mglearn.plots.plot_two_hidden_layer_graph()\n",
      "Figure 2-47. A multilayer perceptron with two hidden layers\n",
      "Having large neural networks made up of many of these layers of computation is\n",
      "what inspired the term “deep learning. ”\n",
      "Tuning neural networks\n",
      "Let’s look into the workings of the MLP by applying the MLPClassifier to the\n",
      "two_moons dataset we used earlier in this chapter. The results are shown in\n",
      "Figure 2-48:\n",
      "In[93]:\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.datasets import make_moons\n",
      "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
      "                                                    random_state=42)\n",
      "mlp = MLPClassifier(algorithm='l-bfgs', random_state=0).fit(X_train, y_train)\n",
      "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
      "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "108 | Chapter 2: Supervised Learning\n",
      "Figure 2-48. Decision boundary learned by a neural network with 100 hidden units on\n",
      "the two_moons dataset\n",
      "As you can see, the neural network learned a very nonlinear but relatively smooth\n",
      "decision boundary. We used algorithm='l-bfgs', which we will discuss later.\n",
      "By default, the MLP uses 100 hidden nodes, which is quite a lot for this small dataset.\n",
      "We can reduce the number (which reduces the complexity of the model) and still get\n",
      "a good result (Figure 2-49):\n",
      "In[94]:\n",
      "mlp = MLPClassifier(algorithm='l-bfgs', random_state=0, hidden_layer_sizes=[10])\n",
      "mlp.fit(X_train, y_train)\n",
      "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
      "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "Supervised Machine Learning Algorithms | 109\n",
      "Figure 2-49. Decision boundary learned by a neural network with 10 hidden units on\n",
      "the two_moons dataset\n",
      "With only 10 hidden units, the decision boundary looks somewhat more ragged. The\n",
      "default nonlinearity is relu, shown in Figure 2-46 . With a single hidden layer, this\n",
      "means the decision function will be made up of 10 straight line segments. If we want\n",
      "a smoother decision boundary, we could add more hidden units (as in Figure 2-49),\n",
      "add a second hidden layer (Figure 2-50), or use the tanh nonlinearity (Figure 2-51):\n",
      "In[95]:\n",
      "# using two hidden layers, with 10 units each\n",
      "mlp = MLPClassifier(algorithm='l-bfgs', random_state=0,\n",
      "                    hidden_layer_sizes=[10, 10])\n",
      "mlp.fit(X_train, y_train)\n",
      "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
      "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "110 | Chapter 2: Supervised Learning\n",
      "In[96]:\n",
      "# using two hidden layers, with 10 units each, now with tanh nonlinearity\n",
      "mlp = MLPClassifier(algorithm='l-bfgs', activation='tanh',\n",
      "                    random_state=0, hidden_layer_sizes=[10, 10])\n",
      "mlp.fit(X_train, y_train)\n",
      "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
      "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "Figure 2-50. Decision boundary learned using 2 hidden layers with 10 hidden units\n",
      "each, with rect activation function\n",
      "Supervised Machine Learning Algorithms | 111\n",
      "Figure 2-51. Decision boundary learned using 2 hidden layers with 10 hidden units\n",
      "each, with tanh activation function\n",
      "Finally, we can also control the complexity of a neural network by using an l2 penalty\n",
      "to shrink the weights toward zero, as we did in ridge regression and the linear classifi‐\n",
      "ers. The parameter for this in the MLPClassifier is alpha (as in the linear regression\n",
      "models), and it’s set to a very low value (little regularization) by default. Figure 2-52\n",
      "shows the effect of different values of alpha on the two_moons dataset, using two hid‐\n",
      "den layers of 10 or 100 units each:\n",
      "In[97]:\n",
      "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
      "for axx, n_hidden_nodes in zip(axes, [10, 100]):\n",
      "    for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 1]):\n",
      "        mlp = MLPClassifier(algorithm='l-bfgs', random_state=0,\n",
      "                            hidden_layer_sizes=[n_hidden_nodes, n_hidden_nodes],\n",
      "                            alpha=alpha)\n",
      "        mlp.fit(X_train, y_train)\n",
      "        mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\n",
      "        mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\n",
      "        ax.set_title(\"n_hidden=[{}, {}]\\nalpha={:.4f}\".format(\n",
      "                      n_hidden_nodes, n_hidden_nodes, alpha))\n",
      "112 | Chapter 2: Supervised Learning\n",
      "Figure 2-52. Decision functions for different numbers of hidden units and different set‐\n",
      "tings of the alpha parameter\n",
      "As you probably have realized by now, there are many ways to control the complexity\n",
      "of a neural network: the number of hidden layers, the number of units in each hidden\n",
      "layer, and the regularization ( alpha). There are actually even more, which we won’t\n",
      "go into here.\n",
      "An important property of neural networks is that their weights are set randomly\n",
      "before learning is started, and this random initialization affects the model that is\n",
      "learned. That means that even when using exactly the same parameters, we can\n",
      "obtain very different models when using different random seeds. If the networks are\n",
      "large, and their complexity is chosen properly, this should not affect accuracy too\n",
      "much, but it is worth keeping in mind (particularly for smaller networks).\n",
      "Figure 2-53 shows plots of several models, all learned with the same settings of the\n",
      "parameters:\n",
      "In[98]:\n",
      "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
      "for i, ax in enumerate(axes.ravel()):\n",
      "    mlp = MLPClassifier(algorithm='l-bfgs', random_state=i,\n",
      "                        hidden_layer_sizes=[100, 100])\n",
      "    mlp.fit(X_train, y_train)\n",
      "    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\n",
      "    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\n",
      "Supervised Machine Learning Algorithms | 113\n",
      "Figure 2-53. Decision functions learned with the same parameters but different random\n",
      "initializations\n",
      "To get a better understanding of neural networks on real-world data, let’s apply the\n",
      "MLPClassifier to the Breast Cancer dataset. We start with the default parameters:\n",
      "In[99]:\n",
      "print(\"Cancer data per-feature maxima:\\n{}\".format(cancer.data.max(axis=0)))\n",
      "Out[99]:\n",
      "Cancer data per-feature maxima:\n",
      "[   28.110    39.280   188.500  2501.000     0.163     0.345     0.427\n",
      "     0.201     0.304     0.097     2.873     4.885    21.980   542.200\n",
      "     0.031     0.135     0.396     0.053     0.079     0.030    36.040\n",
      "    49.540   251.200  4254.000     0.223     1.058     1.252     0.291\n",
      "     0.664     0.207]\n",
      "In[100]:\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    cancer.data, cancer.target, random_state=0)\n",
      "mlp = MLPClassifier(random_state=42)\n",
      "mlp.fit(X_train, y_train)\n",
      "print(\"Accuracy on training set: {:.2f}\".format(mlp.score(X_train, y_train)))\n",
      "print(\"Accuracy on test set: {:.2f}\".format(mlp.score(X_test, y_test)))\n",
      "Out[100]:\n",
      "Accuracy on training set: 0.92\n",
      "Accuracy on test set: 0.90\n",
      "The accuracy of the MLP is quite good, but not as good as the other models. As in the\n",
      "earlier SVC example, this is likely due to scaling of the data. Neural networks also\n",
      "expect all input features to vary in a similar way, and ideally to have a mean of 0, and\n",
      "114 | Chapter 2: Supervised Learning\n",
      "a variance of 1. We must rescale our data so that it fulfills these requirements. Again,\n",
      "we will do this by hand here, but we’ll introduce the StandardScaler to do this auto‐\n",
      "matically in Chapter 3:\n",
      "In[101]:\n",
      "# compute the mean value per feature on the training set\n",
      "mean_on_train = X_train.mean(axis=0)\n",
      "# compute the standard deviation of each feature on the training set\n",
      "std_on_train = X_train.std(axis=0)\n",
      "# subtract the mean, and scale by inverse standard deviation\n",
      "# afterward, mean=0 and std=1\n",
      "X_train_scaled = (X_train - mean_on_train) / std_on_train\n",
      "# use THE SAME transformation (using training mean and std) on the test set\n",
      "X_test_scaled = (X_test - mean_on_train) / std_on_train\n",
      "mlp = MLPClassifier(random_state=0)\n",
      "mlp.fit(X_train_scaled, y_train)\n",
      "print(\"Accuracy on training set: {:.3f}\".format(\n",
      "    mlp.score(X_train_scaled, y_train)))\n",
      "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n",
      "Out[101]:\n",
      "Accuracy on training set: 0.991\n",
      "Accuracy on test set: 0.965\n",
      "ConvergenceWarning:\n",
      "    Stochastic Optimizer: Maximum iterations reached and the optimization\n",
      "    hasn't converged yet.\n",
      "The results are much better after scaling, and already quite competitive. We got a\n",
      "warning from the model, though, that tells us that the maximum number of iterations\n",
      "has been reached. This is part of the adam algorithm for learning the model, and tells\n",
      "us that we should increase the number of iterations:\n",
      "In[102]:\n",
      "mlp = MLPClassifier(max_iter=1000, random_state=0)\n",
      "mlp.fit(X_train_scaled, y_train)\n",
      "print(\"Accuracy on training set: {:.3f}\".format(\n",
      "    mlp.score(X_train_scaled, y_train)))\n",
      "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n",
      "Out[102]:\n",
      "Accuracy on training set: 0.995\n",
      "Accuracy on test set: 0.965\n",
      "Supervised Machine Learning Algorithms | 115\n",
      "12 Y ou might have noticed at this point that many of the well-performing models achieved exactly the same\n",
      "accuracy of 0.972. This means that all of the models make exactly the same number of mistakes, which is four.\n",
      "If you compare the actual predictions, you can even see that they make exactly the same mistakes! This might\n",
      "be a consequence of the dataset being very small, or it may be because these points are really different from\n",
      "the rest.\n",
      "Increasing the number of iterations only increased the training set performance, not\n",
      "the generalization performance. Still, the model is performing quite well. As there is\n",
      "some gap between the training and the test performance, we might try to decrease the\n",
      "model’s complexity to get better generalization performance. Here, we choose to\n",
      "increase the alpha parameter (quite aggressively, from 0.0001 to 1) to add stronger\n",
      "regularization of the weights:\n",
      "In[103]:\n",
      "mlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)\n",
      "mlp.fit(X_train_scaled, y_train)\n",
      "print(\"Accuracy on training set: {:.3f}\".format(\n",
      "    mlp.score(X_train_scaled, y_train)))\n",
      "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n",
      "Out[103]:\n",
      "Accuracy on training set: 0.988\n",
      "Accuracy on test set: 0.972\n",
      "This leads to a performance on par with the best models so far.12\n",
      "While it is possible to analyze what a neural network has learned, this is usually much\n",
      "trickier than analyzing a linear model or a tree-based model. One way to introspect\n",
      "what was learned is to look at the weights in the model. Y ou can see an example of\n",
      "this in the scikit-learn example gallery . For the Breast Cancer dataset, this might\n",
      "be a bit hard to understand. The following plot ( Figure 2-54) shows the weights that\n",
      "were learned connecting the input to the first hidden layer. The rows in this plot cor‐\n",
      "respond to the 30 input features, while the columns correspond to the 100 hidden\n",
      "units. Light colors represent large positive values, while dark colors represent nega‐\n",
      "tive values:\n",
      "In[104]:\n",
      "plt.figure(figsize=(20, 5))\n",
      "plt.imshow(mlp.coefs_[0], interpolation='none', cmap='viridis')\n",
      "plt.yticks(range(30), cancer.feature_names)\n",
      "plt.xlabel(\"Columns in weight matrix\")\n",
      "plt.ylabel(\"Input feature\")\n",
      "plt.colorbar()\n",
      "116 | Chapter 2: Supervised Learning\n",
      "Figure 2-54. Heat map of the first layer weights in a neural network learned on the\n",
      "Breast Cancer dataset\n",
      "One possible inference we can make is that features that have very small weights for\n",
      "all of the hidden units are “less important” to the model. We can see that “mean\n",
      "smoothness” and “mean compactness, ” in addition to the features found between\n",
      "“smoothness error” and “fractal dimension error, ” have relatively low weights com‐\n",
      "pared to other features. This could mean that these are less important features or pos‐\n",
      "sibly that we didn’t represent them in a way that the neural network could use.\n",
      "We could also visualize the weights connecting the hidden layer to the output layer,\n",
      "but those are even harder to interpret.\n",
      "While the MLPClassifier and MLPRegressor provide easy-to-use interfaces for the\n",
      "most common neural network architectures, they only capture a small subset of what\n",
      "is possible with neural networks. If you are interested in working with more flexible\n",
      "or larger models, we encourage you to look beyond scikit-learn into the fantastic\n",
      "deep learning libraries that are out there. For Python users, the most well-established\n",
      "are keras, lasagna, and tensor-flow. lasagna builds on the theano library, while\n",
      "keras can use either tensor-flow or theano. These libraries provide a much more\n",
      "flexible interface to build neural networks and track the rapid progress in deep learn‐\n",
      "ing research. All of the popular deep learning libraries also allow the use of high-\n",
      "performance graphics processing units (GPUs), which scikit-learn does not\n",
      "support. Using GPUs allows us to accelerate computations by factors of 10x to 100x,\n",
      "and they are essential for applying deep learning methods to large-scale datasets.\n",
      "Strengths, weaknesses, and parameters\n",
      "Neural networks have reemerged as state-of-the-art models in many applications of\n",
      "machine learning. One of their main advantages is that they are able to capture infor‐\n",
      "mation contained in large amounts of data and build incredibly complex models.\n",
      "Given enough computation time, data, and careful tuning of the parameters, neural\n",
      "networks often beat other machine learning algorithms (for classification and regres‐\n",
      "sion tasks).\n",
      "Supervised Machine Learning Algorithms | 117\n",
      "This brings us to the downsides. Neural networks—particularly the large and power‐\n",
      "ful ones—often take a long time to train. They also require careful preprocessing of\n",
      "the data, as we saw here. Similarly to SVMs, they work best with “homogeneous”\n",
      "data, where all the features have similar meanings. For data that has very different\n",
      "kinds of features, tree-based models might work better. Tuning neural network\n",
      "parameters is also an art unto itself. In our experiments, we barely scratched the sur‐\n",
      "face of possible ways to adjust neural network models and how to train them.\n",
      "Estimating complexity in neural networks.    The most important parameters are the num‐\n",
      "ber of layers and the number of hidden units per layer. Y ou should start with one or\n",
      "two hidden layers, and possibly expand from there. The number of nodes per hidden\n",
      "layer is often similar to the number of input features, but rarely higher than in the low\n",
      "to mid-thousands.\n",
      "A helpful measure when thinking about the model complexity of a neural network is\n",
      "the number of weights or coefficients that are learned. If you have a binary classifica‐\n",
      "tion dataset with 100 features, and you have 100 hidden units, then there are 100 *\n",
      "100 = 10,000 weights between the input and the first hidden layer. There are also\n",
      "100 * 1 = 100 weights between the hidden layer and the output layer, for a total of\n",
      "around 10,100 weights. If you add a second hidden layer with 100 hidden units, there\n",
      "will be another 100 * 100 = 10,000 weights from the first hidden layer to the second\n",
      "hidden layer, resulting in a total of 20,100 weights. If instead you use one layer with\n",
      "1,000 hidden units, you are learning 100 * 1,000 = 100,000 weights from the input to\n",
      "the hidden layer and 1,000 x 1 weights from the hidden layer to the output layer, for a\n",
      "total of 101,000. If you add a second hidden layer you add 1,000 * 1,000 = 1,000,000\n",
      "weights, for a whopping total of 1,101,000—50 times larger than the model with two\n",
      "hidden layers of size 100.\n",
      "A common way to adjust parameters in a neural network is to first create a network\n",
      "that is large enough to overfit, making sure that the task can actually be learned by\n",
      "the network. Then, once you know the training data can be learned, either shrink the\n",
      "network or increase alpha to add regularization, which will improve generalization\n",
      "performance.\n",
      "In our experiments, we focused mostly on the definition of the model: the number of\n",
      "layers and nodes per layer, the regularization, and the nonlinearity. These define the\n",
      "model we want to learn. There is also the question of how to learn the model, or the\n",
      "algorithm that is used for learning the parameters, which is set using the algorithm\n",
      "parameter. There are two easy-to-use choices for algorithm. The default is 'adam',\n",
      "which works well in most situations but is quite sensitive to the scaling of the data (so\n",
      "it is important to always scale your data to 0 mean and unit variance). The other one\n",
      "is 'l-bfgs', which is quite robust but might take a long time on larger models or\n",
      "larger datasets. There is also the more advanced 'sgd' option, which is what many\n",
      "deep learning researchers use. The 'sgd' option comes with many additional param‐\n",
      "118 | Chapter 2: Supervised Learning\n",
      "eters that need to be tuned for best results. Y ou can find all of these parameters and\n",
      "their definitions in the user guide. When starting to work with MLPs, we recommend\n",
      "sticking to 'adam' and 'l-bfgs'.\n",
      "fit Resets a Model\n",
      "An important property of scikit-learn models is that calling fit\n",
      "will always reset everything a model previously learned. So if you\n",
      "build a model on one dataset, and then call fit again on a different\n",
      "dataset, the model will “forget” everything it learned from the first\n",
      "dataset. Y ou can call fit as often as you like on a model, and the\n",
      "outcome will be the same as calling fit on a “new” model.\n",
      "Uncertainty Estimates from Classifiers\n",
      "Another useful part of the scikit-learn interface that we haven’t talked about yet is\n",
      "the ability of classifiers to provide uncertainty estimates of predictions. Often, you are\n",
      "not only interested in which class a classifier predicts for a certain test point, but also\n",
      "how certain it is that this is the right class. In practice, different kinds of mistakes lead\n",
      "to very different outcomes in real-world applications. Imagine a medical application\n",
      "testing for cancer. Making a false positive prediction might lead to a patient undergo‐\n",
      "ing additional tests, while a false negative prediction might lead to a serious disease\n",
      "not being treated. We will go into this topic in more detail in Chapter 6.\n",
      "There are two different functions in scikit-learn that can be used to obtain uncer‐\n",
      "tainty estimates from classifiers: decision_function and predict_proba. Most (but\n",
      "not all) classifiers have at least one of them, and many classifiers have both. Let’s look\n",
      "at what these two functions do on a synthetic two-dimensional dataset, when build‐\n",
      "ing a GradientBoostingClassifier classifier, which has both a decision_function\n",
      "and a predict_proba method:\n",
      "In[105]:\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.datasets import make_blobs, make_circles\n",
      "X, y = make_circles(noise=0.25, factor=0.5, random_state=1)\n",
      "# we rename the classes \"blue\" and \"red\" for illustration purposes\n",
      "y_named = np.array([\"blue\", \"red\"])[y]\n",
      "# we can call train_test_split with arbitrarily many arrays;\n",
      "# all will be split in a consistent manner\n",
      "X_train, X_test, y_train_named, y_test_named, y_train, y_test = \\\n",
      "    train_test_split(X, y_named, y, random_state=0)\n",
      "# build the gradient boosting model\n",
      "gbrt = GradientBoostingClassifier(random_state=0)\n",
      "gbrt.fit(X_train, y_train_named)\n",
      "Uncertainty Estimates from Classifiers | 119\n",
      "The Decision Function\n",
      "In the binary classification case, the return value of decision_function is of shape\n",
      "(n_samples,), and it returns one floating-point number for each sample:\n",
      "In[106]:\n",
      "print(\"X_test.shape: {}\".format(X_test.shape))\n",
      "print(\"Decision function shape: {}\".format(\n",
      "    gbrt.decision_function(X_test).shape))\n",
      "Out[106]:\n",
      "X_test.shape: (25, 2)\n",
      "Decision function shape: (25,)\n",
      "This value encodes how strongly the model believes a data point to belong to the\n",
      "“positive” class, in this case class 1. Positive values indicate a preference for the posi‐\n",
      "tive class, and negative values indicate a preference for the “negative” (other) class:\n",
      "In[107]:\n",
      "# show the first few entries of decision_function\n",
      "print(\"Decision function:\\n{}\".format(gbrt.decision_function(X_test)[:6]))\n",
      "Out[107]:\n",
      "Decision function:\n",
      "[ 4.136 -1.683 -3.951 -3.626  4.29   3.662]\n",
      "We can recover the prediction by looking only at the sign of the decision function:\n",
      "In[108]:\n",
      "print(\"Thresholded decision function:\\n{}\".format(\n",
      "    gbrt.decision_function(X_test) > 0))\n",
      "print(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\n",
      "Out[108]:\n",
      "Thresholded decision function:\n",
      "[ True False False False  True  True False  True  True  True False  True\n",
      "  True False  True False False False  True  True  True  True  True False\n",
      "  False]\n",
      "Predictions:\n",
      "['red' 'blue' 'blue' 'blue' 'red' 'red' 'blue' 'red' 'red' 'red' 'blue'\n",
      " 'red' 'red' 'blue' 'red' 'blue' 'blue' 'blue' 'red' 'red' 'red' 'red'\n",
      " 'red' 'blue' 'blue']\n",
      "For binary classification, the “negative” class is always the first entry of the classes_\n",
      "attribute, and the “positive” class is the second entry of classes_. So if you want to\n",
      "fully recover the output of predict, you need to make use of the classes_ attribute:\n",
      "120 | Chapter 2: Supervised Learning\n",
      "In[109]:\n",
      "# make the boolean True/False into 0 and 1\n",
      "greater_zero = (gbrt.decision_function(X_test) > 0).astype(int)\n",
      "# use 0 and 1 as indices into classes_\n",
      "pred = gbrt.classes_[greater_zero]\n",
      "# pred is the same as the output of gbrt.predict\n",
      "print(\"pred is equal to predictions: {}\".format(\n",
      "    np.all(pred == gbrt.predict(X_test))))\n",
      "Out[109]:\n",
      "pred is equal to predictions: True\n",
      "The range of decision_function can be arbitrary, and depends on the data and the\n",
      "model parameters:\n",
      "In[110]:\n",
      "decision_function = gbrt.decision_function(X_test)\n",
      "print(\"Decision function minimum: {:.2f} maximum: {:.2f}\".format(\n",
      "    np.min(decision_function), np.max(decision_function)))\n",
      "Out[110]:\n",
      "Decision function minimum: -7.69 maximum: 4.29\n",
      "This arbitrary scaling makes the output of decision_function often hard to\n",
      "interpret.\n",
      "In the following example we plot the decision_function for all points in the 2D\n",
      "plane using a color coding, next to a visualization of the decision boundary, as we saw\n",
      "earlier. We show training points as circles and test data as triangles (Figure 2-55):\n",
      "In[111]:\n",
      "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
      "mglearn.tools.plot_2d_separator(gbrt, X, ax=axes[0], alpha=.4,\n",
      "                                fill=True, cm=mglearn.cm2)\n",
      "scores_image = mglearn.tools.plot_2d_scores(gbrt, X, ax=axes[1],\n",
      "                                            alpha=.4, cm=mglearn.ReBl)\n",
      "for ax in axes:\n",
      "    # plot training and test points\n",
      "    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\n",
      "                             markers='^', ax=ax)\n",
      "    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\n",
      "                             markers='o', ax=ax)\n",
      "    ax.set_xlabel(\"Feature 0\")\n",
      "    ax.set_ylabel(\"Feature 1\")\n",
      "cbar = plt.colorbar(scores_image, ax=axes.tolist())\n",
      "axes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\n",
      "                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\n",
      "Uncertainty Estimates from Classifiers | 121\n",
      "Figure 2-55. Decision boundary (left) and decision function (right) for a gradient boost‐\n",
      "ing model on a two-dimensional toy dataset\n",
      "Encoding not only the predicted outcome but also how certain the classifier is pro‐\n",
      "vides additional information. However, in this visualization, it is hard to make out the\n",
      "boundary between the two classes.\n",
      "Predicting Probabilities\n",
      "The output of predict_proba is a probability for each class, and is often more easily\n",
      "understood than the output of decision_function. It is always of shape (n_samples,\n",
      "2) for binary classification:\n",
      "In[112]:\n",
      "print(\"Shape of probabilities: {}\".format(gbrt.predict_proba(X_test).shape))\n",
      "Out[112]:\n",
      "Shape of probabilities: (25, 2)\n",
      "The first entry in each row is the estimated probability of the first class, and the sec‐\n",
      "ond entry is the estimated probability of the second class. Because it is a probability,\n",
      "the output of predict_proba is always between 0 and 1, and the sum of the entries\n",
      "for both classes is always 1:\n",
      "In[113]:\n",
      "# show the first few entries of predict_proba\n",
      "print(\"Predicted probabilities:\\n{}\".format(\n",
      "    gbrt.predict_proba(X_test[:6])))\n",
      "122 | Chapter 2: Supervised Learning\n",
      "13 Because the probabilities are floating-point numbers, it is unlikely that they will both be exactly 0.500. How‐\n",
      "ever, if that happens, the prediction is made at random.\n",
      "Out[113]:\n",
      "Predicted probabilities:\n",
      "[[ 0.016  0.984]\n",
      " [ 0.843  0.157]\n",
      " [ 0.981  0.019]\n",
      " [ 0.974  0.026]\n",
      " [ 0.014  0.986]\n",
      " [ 0.025  0.975]]\n",
      "Because the probabilities for the two classes sum to 1, exactly one of the classes will\n",
      "be above 50% certainty. That class is the one that is predicted.13\n",
      "Y ou can see in the previous output that the classifier is relatively certain for most\n",
      "points. How well the uncertainty actually reflects uncertainty in the data depends on\n",
      "the model and the parameters. A model that is more overfitted tends to make more\n",
      "certain predictions, even if they might be wrong. A model with less complexity usu‐\n",
      "ally has more uncertainty in its predictions. A model is called calibrated if the\n",
      "reported uncertainty actually matches how correct it is—in a calibrated model, a pre‐\n",
      "diction made with 70% certainty would be correct 70% of the time.\n",
      "In the following example ( Figure 2-56) we again show the decision boundary on the\n",
      "dataset, next to the class probabilities for the class 1:\n",
      "In[114]:\n",
      "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
      "mglearn.tools.plot_2d_separator(\n",
      "    gbrt, X, ax=axes[0], alpha=.4, fill=True, cm=mglearn.cm2)\n",
      "scores_image = mglearn.tools.plot_2d_scores(\n",
      "    gbrt, X, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function='predict_proba')\n",
      "for ax in axes:\n",
      "    # plot training and test points\n",
      "    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\n",
      "                             markers='^', ax=ax)\n",
      "    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\n",
      "                             markers='o', ax=ax)\n",
      "    ax.set_xlabel(\"Feature 0\")\n",
      "    ax.set_ylabel(\"Feature 1\")\n",
      "cbar = plt.colorbar(scores_image, ax=axes.tolist())\n",
      "axes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\n",
      "                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\n",
      "Uncertainty Estimates from Classifiers | 123\n",
      "Figure 2-56. Decision boundary (left) and predicted probabilities for the gradient boost‐\n",
      "ing model shown in Figure 2-55\n",
      "The boundaries in this plot are much more well-defined, and the small areas of\n",
      "uncertainty are clearly visible.\n",
      "The scikit-learn website has a great comparison of many models and what their\n",
      "uncertainty estimates look like. We’ve reproduced this in Figure 2-57, and we encour‐\n",
      "age you to go though the example there.\n",
      "Figure 2-57. Comparison of several classifiers in scikit-learn on synthetic datasets (image\n",
      "courtesy http://scikit-learn.org)\n",
      "Uncertainty in Multiclass Classification\n",
      "So far, we’ve only talked about uncertainty estimates in binary classification. But the\n",
      "decision_function and predict_proba methods also work in the multiclass setting.\n",
      "Let’s apply them on the Iris dataset, which is a three-class classification dataset:\n",
      "124 | Chapter 2: Supervised Learning\n",
      "In[115]:\n",
      "from sklearn.datasets import load_iris\n",
      "iris = load_iris()\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    iris.data, iris.target, random_state=42)\n",
      "gbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)\n",
      "gbrt.fit(X_train, y_train)\n",
      "In[116]:\n",
      "print(\"Decision function shape: {}\".format(gbrt.decision_function(X_test).shape))\n",
      "# plot the first few entries of the decision function\n",
      "print(\"Decision function:\\n{}\".format(gbrt.decision_function(X_test)[:6, :]))\n",
      "Out[116]:\n",
      "Decision function shape: (38, 3)\n",
      "Decision function:\n",
      "[[-0.529  1.466 -0.504]\n",
      " [ 1.512 -0.496 -0.503]\n",
      " [-0.524 -0.468  1.52 ]\n",
      " [-0.529  1.466 -0.504]\n",
      " [-0.531  1.282  0.215]\n",
      " [ 1.512 -0.496 -0.503]]\n",
      "In the multiclass case, the decision_function has the shape (n_samples,\n",
      "n_classes) and each column provides a “certainty score” for each class, where a large\n",
      "score means that a class is more likely and a small score means the class is less likely.\n",
      "Y ou can recover the predictions from these scores by finding the maximum entry for\n",
      "each data point:\n",
      "In[117]:\n",
      "print(\"Argmax of decision function:\\n{}\".format(\n",
      "      np.argmax(gbrt.decision_function(X_test), axis=1)))\n",
      "print(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\n",
      "Out[117]:\n",
      "Argmax of decision function:\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\n",
      "Predictions:\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\n",
      "The output of predict_proba has the same shape, (n_samples, n_classes). Again,\n",
      "the probabilities for the possible classes for each data point sum to 1:\n",
      "Uncertainty Estimates from Classifiers | 125\n",
      "In[118]:\n",
      "# show the first few entries of predict_proba\n",
      "print(\"Predicted probabilities:\\n{}\".format(gbrt.predict_proba(X_test)[:6]))\n",
      "# show that sums across rows are one\n",
      "print(\"Sums: {}\".format(gbrt.predict_proba(X_test)[:6].sum(axis=1)))\n",
      "Out[118]:\n",
      "Predicted probabilities:\n",
      "[[ 0.107  0.784  0.109]\n",
      " [ 0.789  0.106  0.105]\n",
      " [ 0.102  0.108  0.789]\n",
      " [ 0.107  0.784  0.109]\n",
      " [ 0.108  0.663  0.228]\n",
      " [ 0.789  0.106  0.105]]\n",
      "Sums: [ 1.  1.  1.  1.  1.  1.]\n",
      "We can again recover the predictions by computing the argmax of predict_proba:\n",
      "In[119]:\n",
      "print(\"Argmax of predicted probabilities:\\n{}\".format(\n",
      "    np.argmax(gbrt.predict_proba(X_test), axis=1)))\n",
      "print(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\n",
      "Out[119]:\n",
      "Argmax of predicted probabilities:\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\n",
      "Predictions:\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\n",
      "To summarize, predict_proba and decision_function always have shape (n_sam\n",
      "ples, n_classes)—apart from decision_function in the special binary case. In the\n",
      "binary case, decision_function only has one column, corresponding to the “posi‐\n",
      "tive” class classes_[1]. This is mostly for historical reasons.\n",
      "Y ou can recover the prediction when there are n_classes many columns by comput‐\n",
      "ing the argmax across columns. Be careful, though, if your classes are strings, or you\n",
      "use integers but they are not consecutive and starting from 0. If you want to compare\n",
      "results obtained with predict to results obtained via decision_function or pre\n",
      "dict_proba, make sure to use the classes_ attribute of the classifier to get the actual\n",
      "class names:\n",
      "126 | Chapter 2: Supervised Learning\n",
      "In[120]:\n",
      "logreg = LogisticRegression()\n",
      "# represent each target by its class name in the iris dataset\n",
      "named_target = iris.target_names[y_train]\n",
      "logreg.fit(X_train, named_target)\n",
      "print(\"unique classes in training data: {}\".format(logreg.classes_))\n",
      "print(\"predictions: {}\".format(logreg.predict(X_test)[:10]))\n",
      "argmax_dec_func = np.argmax(logreg.decision_function(X_test), axis=1)\n",
      "print(\"argmax of decision function: {}\".format(argmax_dec_func[:10]))\n",
      "print(\"argmax combined with classes_: {}\".format(\n",
      "        logreg.classes_[argmax_dec_func][:10]))\n",
      "Out[120]:\n",
      "unique classes in training data: ['setosa' 'versicolor' 'virginica']\n",
      "predictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor'\n",
      " 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\n",
      "argmax of decision function: [1 0 2 1 1 0 1 2 1 1]\n",
      "argmax combined with classes_: ['versicolor' 'setosa' 'virginica' 'versicolor'\n",
      " 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\n",
      "Summary and Outlook\n",
      "We started this chapter with a discussion of model complexity, then discussed gener‐\n",
      "alization, or learning a model that is able to perform well on new, previously unseen\n",
      "data. This led us to the concepts of underfitting, which describes a model that cannot\n",
      "capture the variations present in the training data, and overfitting, which describes a\n",
      "model that focuses too much on the training data and is not able to generalize to new\n",
      "data very well.\n",
      "We then discussed a wide array of machine learning models for classification and\n",
      "regression, what their advantages and disadvantages are, and how to control model\n",
      "complexity for each of them. We saw that for many of the algorithms, setting the right\n",
      "parameters is important for good performance. Some of the algorithms are also sensi‐\n",
      "tive to how we represent the input data, and in particular to how the features are\n",
      "scaled. Therefore, blindly applying an algorithm to a dataset without understanding\n",
      "the assumptions the model makes and the meanings of the parameter settings will\n",
      "rarely lead to an accurate model.\n",
      "This chapter contains a lot of information about the algorithms, and it is not neces‐\n",
      "sary for you to remember all of these details for the following chapters. However,\n",
      "some knowledge of the models described here—and which to use in a specific situa‐\n",
      "tion—is important for successfully applying machine learning in practice. Here is a\n",
      "quick summary of when to use each model:\n",
      "Summary and Outlook | 127\n",
      "Nearest neighbors\n",
      "For small datasets, good as a baseline, easy to explain.\n",
      "Linear models\n",
      "Go-to as a first algorithm to try, good for very large datasets, good for very high-\n",
      "dimensional data.\n",
      "Naive Bayes\n",
      "Only for classification. Even faster than linear models, good for very large data‐\n",
      "sets and high-dimensional data. Often less accurate than linear models.\n",
      "Decision trees\n",
      "Very fast, don’t need scaling of the data, can be visualized and easily explained.\n",
      "Random forests\n",
      "Nearly always perform better than a single decision tree, very robust and power‐\n",
      "ful. Don’t need scaling of data. Not good for very high-dimensional sparse data.\n",
      "Gradient boosted decision trees\n",
      "Often slightly more accurate than random forests. Slower to train but faster to\n",
      "predict than random forests, and smaller in memory. Need more parameter tun‐\n",
      "ing than random forests.\n",
      "Support vector machines\n",
      "Powerful for medium-sized datasets of features with similar meaning. Require\n",
      "scaling of data, sensitive to parameters.\n",
      "Neural networks\n",
      "Can build very complex models, particularly for large datasets. Sensitive to scal‐\n",
      "ing of the data and to the choice of parameters. Large models need a long time to\n",
      "train.\n",
      "When working with a new dataset, it is in general a good idea to start with a simple\n",
      "model, such as a linear model or a naive Bayes or nearest neighbors classifier, and see\n",
      "how far you can get. After understanding more about the data, you can consider\n",
      "moving to an algorithm that can build more complex models, such as random forests,\n",
      "gradient boosted decision trees, SVMs, or neural networks.\n",
      "Y ou should now be in a position where you have some idea of how to apply, tune, and\n",
      "analyze the models we discussed here. In this chapter, we focused on the binary clas‐\n",
      "sification case, as this is usually easiest to understand. Most of the algorithms presen‐\n",
      "ted have classification and regression variants, however, and all of the classification\n",
      "algorithms support both binary and multiclass classification. Try applying any of\n",
      "these algorithms to the built-in datasets in scikit-learn, like the boston_housing or\n",
      "diabetes datasets for regression, or the digits dataset for multiclass classification.\n",
      "Playing around with the algorithms on different datasets will give you a better feel for\n",
      "128 | Chapter 2: Supervised Learning\n",
      "how long they need to train, how easy it is to analyze the models, and how sensitive\n",
      "they are to the representation of the data.\n",
      "While we analyzed the consequences of different parameter settings for the algo‐\n",
      "rithms we investigated, building a model that actually generalizes well to new data in\n",
      "production is a bit trickier than that. We will see how to properly adjust parameters\n",
      "and how to find good parameters automatically in Chapter 6.\n",
      "First, though, we will dive in more detail into unsupervised learning and preprocess‐\n",
      "ing in the next chapter.\n",
      "Summary and Outlook | 129\n",
      "\n",
      "CHAPTER 3\n",
      "Unsupervised Learning and Preprocessing\n",
      "The second family of machine learning algorithms that we will discuss is unsuper‐\n",
      "vised learning algorithms. Unsupervised learning subsumes all kinds of machine\n",
      "learning where there is no known output, no teacher to instruct the learning algo‐\n",
      "rithm. In unsupervised learning, the learning algorithm is just shown the input data\n",
      "and asked to extract knowledge from this data.\n",
      "Types of Unsupervised Learning\n",
      "We will look into two kinds of unsupervised learning in this chapter: transformations\n",
      "of the dataset and clustering.\n",
      "Unsupervised transformations of a dataset are algorithms that create a new representa‐\n",
      "tion of the data which might be easier for humans or other machine learning algo‐\n",
      "rithms to understand compared to the original representation of the data. A common\n",
      "application of unsupervised transformations is dimensionality reduction, which takes\n",
      "a high-dimensional representation of the data, consisting of many features, and finds\n",
      "a new way to represent this data that summarizes the essential characteristics with\n",
      "fewer features. A common application for dimensionality reduction is reduction to\n",
      "two dimensions for visualization purposes.\n",
      "Another application for unsupervised transformations is finding the parts or compo‐\n",
      "nents that “make up” the data. An example of this is topic extraction on collections of\n",
      "text documents. Here, the task is to find the unknown topics that are talked about in\n",
      "each document, and to learn what topics appear in each document. This can be useful\n",
      "for tracking the discussion of themes like elections, gun control, or pop stars on social\n",
      "media.\n",
      "Clustering algorithms, on the other hand, partition data into distinct groups of similar\n",
      "items. Consider the example of uploading photos to a social media site. To allow you\n",
      "131\n",
      "to organize your pictures, the site might want to group together pictures that show\n",
      "the same person. However, the site doesn’t know which pictures show whom, and it\n",
      "doesn’t know how many different people appear in your photo collection. A sensible\n",
      "approach would be to extract all the faces and divide them into groups of faces that\n",
      "look similar. Hopefully, these correspond to the same person, and the images can be\n",
      "grouped together for you.\n",
      "Challenges in Unsupervised Learning\n",
      "A major challenge in unsupervised learning is evaluating whether the algorithm\n",
      "learned something useful. Unsupervised learning algorithms are usually applied to\n",
      "data that does not contain any label information, so we don’t know what the right\n",
      "output should be. Therefore, it is very hard to say whether a model “did well. ” For\n",
      "example, our hypothetical clustering algorithm could have grouped together all the\n",
      "pictures that show faces in profile and all the full-face pictures. This would certainly\n",
      "be a possible way to divide a collection of pictures of people’s faces, but it’s not the one\n",
      "we were looking for. However, there is no way for us to “tell” the algorithm what we\n",
      "are looking for, and often the only way to evaluate the result of an unsupervised algo‐\n",
      "rithm is to inspect it manually.\n",
      "As a consequence, unsupervised algorithms are used often in an exploratory setting,\n",
      "when a data scientist wants to understand the data better, rather than as part of a\n",
      "larger automatic system. Another common application for unsupervised algorithms\n",
      "is as a preprocessing step for supervised algorithms. Learning a new representation of\n",
      "the data can sometimes improve the accuracy of supervised algorithms, or can lead to\n",
      "reduced memory and time consumption.\n",
      "Before we start with “real” unsupervised algorithms, we will briefly discuss some sim‐\n",
      "ple preprocessing methods that often come in handy. Even though preprocessing and\n",
      "scaling are often used in tandem with supervised learning algorithms, scaling meth‐\n",
      "ods don’t make use of the supervised information, making them unsupervised.\n",
      "Preprocessing and Scaling\n",
      "In the previous chapter we saw that some algorithms, like neural networks and SVMs,\n",
      "are very sensitive to the scaling of the data. Therefore, a common practice is to adjust\n",
      "the features so that the data representation is more suitable for these algorithms.\n",
      "Often, this is a simple per-feature rescaling and shift of the data. The following code\n",
      "(Figure 3-1) shows a simple example:\n",
      "In[2]:\n",
      "mglearn.plots.plot_scaling()\n",
      "132 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of\n",
      "the numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\n",
      "smaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\n",
      "Figure 3-1. Different ways to rescale and preprocess a dataset\n",
      "Different Kinds of Preprocessing\n",
      "The first plot in Figure 3-1 shows a synthetic two-class classification dataset with two\n",
      "features. The first feature (the x-axis value) is between 10 and 15. The second feature\n",
      "(the y-axis value) is between around 1 and 9.\n",
      "The following four plots show four different ways to transform the data that yield\n",
      "more standard ranges. The StandardScaler in scikit-learn ensures that for each\n",
      "feature the mean is 0 and the variance is 1, bringing all features to the same magni‐\n",
      "tude. However, this scaling does not ensure any particular minimum and maximum\n",
      "values for the features. The RobustScaler works similarly to the StandardScaler in\n",
      "that it ensures statistical properties for each feature that guarantee that they are on the\n",
      "same scale. However, the RobustScaler uses the median and quartiles, 1 instead of\n",
      "mean and variance. This makes the RobustScaler ignore data points that are very\n",
      "different from the rest (like measurement errors). These odd data points are also\n",
      "called outliers, and can lead to trouble for other scaling techniques.\n",
      "The MinMaxScaler, on the other hand, shifts the data such that all features are exactly\n",
      "between 0 and 1. For the two-dimensional dataset this means all of the data is con‐\n",
      "Preprocessing and Scaling | 133\n",
      "tained within the rectangle created by the x-axis between 0 and 1 and the y-axis\n",
      "between 0 and 1.\n",
      "Finally, the Normalizer does a very different kind of rescaling. It scales each data\n",
      "point such that the feature vector has a Euclidean length of 1. In other words, it\n",
      "projects a data point on the circle (or sphere, in the case of higher dimensions) with a\n",
      "radius of 1. This means every data point is scaled by a different number (by the\n",
      "inverse of its length). This normalization is often used when only the direction (or\n",
      "angle) of the data matters, not the length of the feature vector.\n",
      "Applying Data Transformations\n",
      "Now that we’ve seen what the different kinds of transformations do, let’s apply them\n",
      "using scikit-learn. We will use the cancer dataset that we saw in Chapter 2. Pre‐\n",
      "processing methods like the scalers are usually applied before applying a supervised\n",
      "machine learning algorithm. As an example, say we want to apply the kernel SVM\n",
      "(SVC) to the cancer dataset, and use MinMaxScaler for preprocessing the data. We\n",
      "start by loading our dataset and splitting it into a training set and a test set (we need\n",
      "separate training and test sets to evaluate the supervised model we will build after the\n",
      "preprocessing):\n",
      "In[3]:\n",
      "from sklearn.datasets import load_breast_cancer\n",
      "from sklearn.model_selection import train_test_split\n",
      "cancer = load_breast_cancer()\n",
      "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
      "                                                    random_state=1)\n",
      "print(X_train.shape)\n",
      "print(X_test.shape)\n",
      "Out[3]:\n",
      "(426, 30)\n",
      "(143, 30)\n",
      "As a reminder, the dataset contains 569 data points, each represented by 30 measure‐\n",
      "ments. We split the dataset into 426 samples for the training set and 143 samples for\n",
      "the test set.\n",
      "As with the supervised models we built earlier, we first import the class that imple‐\n",
      "ments the preprocessing, and then instantiate it:\n",
      "In[4]:\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler()\n",
      "134 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "We then fit the scaler using the fit method, applied to the training data. For the Min\n",
      "MaxScaler, the fit method computes the minimum and maximum value of each fea‐\n",
      "ture on the training set. In contrast to the classifiers and regressors of Chapter 2, the\n",
      "scaler is only provided with the data (X_train) when fit is called, and y_train is not\n",
      "used:\n",
      "In[5]:\n",
      "scaler.fit(X_train)\n",
      "Out[5]:\n",
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "To apply the transformation that we just learned—that is, to actually scale the training\n",
      "data—we use the transform method of the scaler. The transform method is used in\n",
      "scikit-learn whenever a model returns a new representation of the data:\n",
      "In[6]:\n",
      "# transform data\n",
      "X_train_scaled = scaler.transform(X_train)\n",
      "# print dataset properties before and after scaling\n",
      "print(\"transformed shape: {}\".format(X_train_scaled.shape))\n",
      "print(\"per-feature minimum before scaling:\\n {}\".format(X_train.min(axis=0)))\n",
      "print(\"per-feature maximum before scaling:\\n {}\".format(X_train.max(axis=0)))\n",
      "print(\"per-feature minimum after scaling:\\n {}\".format(\n",
      "    X_train_scaled.min(axis=0)))\n",
      "print(\"per-feature maximum after scaling:\\n {}\".format(\n",
      "    X_train_scaled.max(axis=0)))\n",
      "Out[6]:\n",
      "transformed shape: (426, 30)\n",
      "per-feature minimum before scaling:\n",
      " [   6.98    9.71   43.79  143.50    0.05    0.02    0.      0.      0.11\n",
      "     0.05    0.12    0.36    0.76    6.80    0.      0.      0.      0.\n",
      "     0.01    0.      7.93   12.02   50.41  185.20    0.07    0.03    0.\n",
      "     0.      0.16    0.06]\n",
      "per-feature maximum before scaling:\n",
      " [   28.11    39.28   188.5   2501.0     0.16     0.29     0.43     0.2\n",
      "     0.300    0.100    2.87     4.88    21.98   542.20     0.03     0.14\n",
      "     0.400    0.050    0.06     0.03    36.04    49.54   251.20  4254.00\n",
      "     0.220    0.940    1.17     0.29     0.58     0.15]\n",
      "per-feature minimum after scaling:\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "per-feature maximum after scaling:\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Preprocessing and Scaling | 135\n",
      "The transformed data has the same shape as the original data—the features are simply\n",
      "shifted and scaled. Y ou can see that all of the features are now between 0 and 1, as\n",
      "desired.\n",
      "To apply the SVM to the scaled data, we also need to transform the test set. This is\n",
      "again done by calling the transform method, this time on X_test:\n",
      "In[7]:\n",
      "# transform test data\n",
      "X_test_scaled = scaler.transform(X_test)\n",
      "# print test data properties after scaling\n",
      "print(\"per-feature minimum after scaling:\\n{}\".format(X_test_scaled.min(axis=0)))\n",
      "print(\"per-feature maximum after scaling:\\n{}\".format(X_test_scaled.max(axis=0)))\n",
      "Out[7]:\n",
      "per-feature minimum after scaling:\n",
      "[ 0.034  0.023  0.031  0.011  0.141  0.044  0.     0.     0.154 -0.006\n",
      " -0.001  0.006  0.004  0.001  0.039  0.011  0.     0.    -0.032  0.007\n",
      "  0.027  0.058  0.02   0.009  0.109  0.026  0.     0.    -0.    -0.002]\n",
      "per-feature maximum after scaling:\n",
      "[ 0.958  0.815  0.956  0.894  0.811  1.22   0.88   0.933  0.932  1.037\n",
      "  0.427  0.498  0.441  0.284  0.487  0.739  0.767  0.629  1.337  0.391\n",
      "  0.896  0.793  0.849  0.745  0.915  1.132  1.07   0.924  1.205  1.631]\n",
      "Maybe somewhat surprisingly, you can see that for the test set, after scaling, the mini‐\n",
      "mum and maximum are not 0 and 1. Some of the features are even outside the 0–1\n",
      "range! The explanation is that the MinMaxScaler (and all the other scalers) always\n",
      "applies exactly the same transformation to the training and the test set. This means\n",
      "the transform method always subtracts the training set minimum and divides by the\n",
      "training set range, which might be different from the minimum and range for the test\n",
      "set.\n",
      "Scaling Training and Test Data the Same Way\n",
      "It is important to apply exactly the same transformation to the training set and the\n",
      "test set for the supervised model to work on the test set. The following example\n",
      "(Figure 3-2) illustrates what would happen if we were to use the minimum and range\n",
      "of the test set instead:\n",
      "In[8]:\n",
      "from sklearn.datasets import make_blobs\n",
      "# make synthetic data\n",
      "X, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\n",
      "# split it into training and test sets\n",
      "X_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n",
      "# plot the training and test sets\n",
      "fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
      "136 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "axes[0].scatter(X_train[:, 0], X_train[:, 1],\n",
      "                c=mglearn.cm2(0), label=\"Training set\", s=60)\n",
      "axes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^',\n",
      "                c=mglearn.cm2(1), label=\"Test set\", s=60)\n",
      "axes[0].legend(loc='upper left')\n",
      "axes[0].set_title(\"Original Data\")\n",
      "# scale the data using MinMaxScaler\n",
      "scaler = MinMaxScaler()\n",
      "scaler.fit(X_train)\n",
      "X_train_scaled = scaler.transform(X_train)\n",
      "X_test_scaled = scaler.transform(X_test)\n",
      "# visualize the properly scaled data\n",
      "axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
      "                c=mglearn.cm2(0), label=\"Training set\", s=60)\n",
      "axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',\n",
      "                c=mglearn.cm2(1), label=\"Test set\", s=60)\n",
      "axes[1].set_title(\"Scaled Data\")\n",
      "# rescale the test set separately\n",
      "# so test set min is 0 and test set max is 1\n",
      "# DO NOT DO THIS! For illustration purposes only.\n",
      "test_scaler = MinMaxScaler()\n",
      "test_scaler.fit(X_test)\n",
      "X_test_scaled_badly = test_scaler.transform(X_test)\n",
      "# visualize wrongly scaled data\n",
      "axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
      "                c=mglearn.cm2(0), label=\"training set\", s=60)\n",
      "axes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],\n",
      "                marker='^', c=mglearn.cm2(1), label=\"test set\", s=60)\n",
      "axes[2].set_title(\"Improperly Scaled Data\")\n",
      "for ax in axes:\n",
      "    ax.set_xlabel(\"Feature 0\")\n",
      "    ax.set_ylabel(\"Feature 1\")\n",
      "Figure 3-2. Effect of scaling training and test data shown on the left together (center) and\n",
      "separately (right)\n",
      "Preprocessing and Scaling | 137\n",
      "The first panel is an unscaled two-dimensional dataset, with the training set shown as\n",
      "circles and the test set shown as triangles. The second panel is the same data, but\n",
      "scaled using the MinMaxScaler. Here, we called fit on the training set, and then\n",
      "called transform on the training and test sets. Y ou can see that the dataset in the sec‐\n",
      "ond panel looks identical to the first; only the ticks on the axes have changed. Now all\n",
      "the features are between 0 and 1. Y ou can also see that the minimum and maximum\n",
      "feature values for the test data (the triangles) are not 0 and 1.\n",
      "The third panel shows what would happen if we scaled the training set and test set\n",
      "separately. In this case, the minimum and maximum feature values for both the train‐\n",
      "ing and the test set are 0 and 1. But now the dataset looks different. The test points\n",
      "moved incongruously to the training set, as they were scaled differently. We changed\n",
      "the arrangement of the data in an arbitrary way. Clearly this is not what we want to\n",
      "do.\n",
      "As another way to think about this, imagine your test set is a single point. There is no\n",
      "way to scale a single point correctly, to fulfill the minimum and maximum require‐\n",
      "ments of the MinMaxScaler. But the size of your test set should not change your\n",
      "processing.\n",
      "Shortcuts and Efficient Alternatives\n",
      "Often, you want to fit a model on some dataset, and then transform it. This is a very\n",
      "common task, which can often be computed more efficiently than by simply calling\n",
      "fit and then transform. For this use case, all models that have a transform method\n",
      "also have a fit_transform method. Here is an example using StandardScaler:\n",
      "In[9]:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "# calling fit and transform in sequence (using method chaining)\n",
      "X_scaled = scaler.fit(X).transform(X)\n",
      "# same result, but more efficient computation\n",
      "X_scaled_d = scaler.fit_transform(X)\n",
      "While fit_transform is not necessarily more efficient for all models, it is still good\n",
      "practice to use this method when trying to transform the training set.\n",
      "The Effect of Preprocessing on Supervised Learning\n",
      "Now let’s go back to the cancer dataset and see the effect of using the MinMaxScaler\n",
      "on learning the SVC (this is a different way of doing the same scaling we did in Chap‐\n",
      "ter 2). First, let’s fit the SVC on the original data again for comparison:\n",
      "138 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "In[10]:\n",
      "from sklearn.svm import SVC\n",
      "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
      "                                                    random_state=0)\n",
      "svm = SVC(C=100)\n",
      "svm.fit(X_train, y_train)\n",
      "print(\"Test set accuracy: {:.2f}\".format(svm.score(X_test, y_test)))\n",
      "Out[10]:\n",
      "Test set accuracy: 0.63\n",
      "Now, let’s scale the data using MinMaxScaler before fitting the SVC:\n",
      "In[11]:\n",
      "# preprocessing using 0-1 scaling\n",
      "scaler = MinMaxScaler()\n",
      "scaler.fit(X_train)\n",
      "X_train_scaled = scaler.transform(X_train)\n",
      "X_test_scaled = scaler.transform(X_test)\n",
      "# learning an SVM on the scaled training data\n",
      "svm.fit(X_train_scaled, y_train)\n",
      "# scoring on the scaled test set\n",
      "print(\"Scaled test set accuracy: {:.2f}\".format(\n",
      "    svm.score(X_test_scaled, y_test)))\n",
      "Out[11]:\n",
      "Scaled test set accuracy: 0.97\n",
      "As we saw before, the effect of scaling the data is quite significant. Even though scal‐\n",
      "ing the data doesn’t involve any complicated math, it is good practice to use the scal‐\n",
      "ing mechanisms provided by scikit-learn instead of reimplementing them yourself,\n",
      "as it’s easy to make mistakes even in these simple computations.\n",
      "Y ou can also easily replace one preprocessing algorithm with another by changing the\n",
      "class you use, as all of the preprocessing classes have the same interface, consisting of\n",
      "the fit and transform methods:\n",
      "In[12]:\n",
      "# preprocessing using zero mean and unit variance scaling\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(X_train)\n",
      "X_train_scaled = scaler.transform(X_train)\n",
      "X_test_scaled = scaler.transform(X_test)\n",
      "Preprocessing and Scaling | 139\n",
      "# learning an SVM on the scaled training data\n",
      "svm.fit(X_train_scaled, y_train)\n",
      "# scoring on the scaled test set\n",
      "print(\"SVM test accuracy: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\n",
      "Out[12]:\n",
      "SVM test accuracy: 0.96\n",
      "Now that we’ve seen how simple data transformations for preprocessing work, let’s\n",
      "move on to more interesting transformations using unsupervised learning.\n",
      "Dimensionality Reduction, Feature Extraction, and\n",
      "Manifold Learning\n",
      "As we discussed earlier, transforming data using unsupervised learning can have\n",
      "many motivations. The most common motivations are visualization, compressing the\n",
      "data, and finding a representation that is more informative for further processing.\n",
      "One of the simplest and most widely used algorithms for all of these is principal com‐\n",
      "ponent analysis. We’ll also look at two other algorithms: non-negative matrix factori‐\n",
      "zation (NMF), which is commonly used for feature extraction, and t-SNE, which is\n",
      "commonly used for visualization using two-dimensional scatter plots.\n",
      "Principal Component Analysis (PCA)\n",
      "Principal component analysis is a method that rotates the dataset in a way such that\n",
      "the rotated features are statistically uncorrelated. This rotation is often followed by\n",
      "selecting only a subset of the new features, according to how important they are for\n",
      "explaining the data. The following example ( Figure 3-3) illustrates the effect of PCA\n",
      "on a synthetic two-dimensional dataset:\n",
      "In[13]:\n",
      "mglearn.plots.plot_pca_illustration()\n",
      "The first plot (top left) shows the original data points, colored to distinguish among\n",
      "them. The algorithm proceeds by first finding the direction of maximum variance,\n",
      "labeled “Component 1. ” This is the direction (or vector) in the data that contains most\n",
      "of the information, or in other words, the direction along which the features are most\n",
      "correlated with each other. Then, the algorithm finds the direction that contains the\n",
      "most information while being orthogonal (at a right angle) to the first direction. In\n",
      "two dimensions, there is only one possible orientation that is at a right angle, but in\n",
      "higher-dimensional spaces there would be (infinitely) many orthogonal directions.\n",
      "Although the two components are drawn as arrows, it doesn’t really matter where the\n",
      "head and the tail are; we could have drawn the first component from the center up to\n",
      "140 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "the top left instead of down to the bottom right. The directions found using this pro‐\n",
      "cess are called principal components, as they are the main directions of variance in the\n",
      "data. In general, there are as many principal components as original features.\n",
      "Figure 3-3. Transformation of data with PCA\n",
      "The second plot (top right) shows the same data, but now rotated so that the first\n",
      "principal component aligns with the x-axis and the second principal component\n",
      "aligns with the y-axis. Before the rotation, the mean was subtracted from the data, so\n",
      "that the transformed data is centered around zero. In the rotated representation\n",
      "found by PCA, the two axes are uncorrelated, meaning that the correlation matrix of\n",
      "the data in this representation is zero except for the diagonal.\n",
      "We can use PCA for dimensionality reduction by retaining only some of the principal\n",
      "components. In this example, we might keep only the first principal component, as\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning | 141\n",
      "shown in the third panel in Figure 3-3  (bottom left). This reduces the data from a\n",
      "two-dimensional dataset to a one-dimensional dataset. Note, however, that instead of\n",
      "keeping only one of the original features, we found the most interesting direction\n",
      "(top left to bottom right in the first panel) and kept this direction, the first principal\n",
      "component.\n",
      "Finally, we can undo the rotation and add the mean back to the data. This will result\n",
      "in the data shown in the last panel in Figure 3-3. These points are in the original fea‐\n",
      "ture space, but we kept only the information contained in the first principal compo‐\n",
      "nent. This transformation is sometimes used to remove noise effects from the data or\n",
      "visualize what part of the information is retained using the principal components.\n",
      "Applying PCA to the cancer dataset for visualization\n",
      "One of the most common applications of PCA is visualizing high-dimensional data‐\n",
      "sets. As we saw in Chapter 1, it is hard to create scatter plots of data that has more\n",
      "than two features. For the Iris dataset, we were able to create a pair plot (Figure 1-3 in\n",
      "Chapter 1) that gave us a partial picture of the data by showing us all the possible\n",
      "combinations of two features. But if we want to look at the Breast Cancer dataset,\n",
      "even using a pair plot is tricky. This dataset has 30 features, which would result in\n",
      "30 * 14 = 420 scatter plots! We’ d never be able to look at all these plots in detail, let\n",
      "alone try to understand them.\n",
      "There is an even simpler visualization we can use, though—computing histograms of\n",
      "each of the features for the two classes, benign and malignant cancer (Figure 3-4):\n",
      "In[14]:\n",
      "fig, axes = plt.subplots(15, 2, figsize=(10, 20))\n",
      "malignant = cancer.data[cancer.target == 0]\n",
      "benign = cancer.data[cancer.target == 1]\n",
      "ax = axes.ravel()\n",
      "for i in range(30):\n",
      "    _, bins = np.histogram(cancer.data[:, i], bins=50)\n",
      "    ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(0), alpha=.5)\n",
      "    ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(2), alpha=.5)\n",
      "    ax[i].set_title(cancer.feature_names[i])\n",
      "    ax[i].set_yticks(())\n",
      "ax[0].set_xlabel(\"Feature magnitude\")\n",
      "ax[0].set_ylabel(\"Frequency\")\n",
      "ax[0].legend([\"malignant\", \"benign\"], loc=\"best\")\n",
      "fig.tight_layout()\n",
      "142 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-4. Per-class feature histograms on the Breast Cancer dataset\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning | 143\n",
      "Here we create a histogram for each of the features, counting how often a data point\n",
      "appears with a feature in a certain range (called a bin). Each plot overlays two histo‐\n",
      "grams, one for all of the points in the benign class (blue) and one for all the points in\n",
      "the malignant class (red). This gives us some idea of how each feature is distributed\n",
      "across the two classes, and allows us to venture a guess as to which features are better\n",
      "at distinguishing malignant and benign samples. For example, the feature “smooth‐\n",
      "ness error” seems quite uninformative, because the two histograms mostly overlap,\n",
      "while the feature “worst concave points” seems quite informative, because the histo‐\n",
      "grams are quite disjoint.\n",
      "However, this plot doesn’t show us anything about the interactions between variables\n",
      "and how these relate to the classes. Using PCA, we can capture the main interactions\n",
      "and get a slightly more complete picture. We can find the first two principal compo‐\n",
      "nents, and visualize the data in this new two-dimensional space with a single scatter\n",
      "plot.\n",
      "Before we apply PCA, we scale our data so that each feature has unit variance using\n",
      "StandardScaler:\n",
      "In[15]:\n",
      "from sklearn.datasets import load_breast_cancer\n",
      "cancer = load_breast_cancer()\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(cancer.data)\n",
      "X_scaled = scaler.transform(cancer.data)\n",
      "Learning the PCA transformation and applying it is as simple as applying a prepro‐\n",
      "cessing transformation. We instantiate the PCA object, find the principal components\n",
      "by calling the fit method, and then apply the rotation and dimensionality reduction\n",
      "by calling transform. By default, PCA only rotates (and shifts) the data, but keeps all\n",
      "principal components. To reduce the dimensionality of the data, we need to specify\n",
      "how many components we want to keep when creating the PCA object:\n",
      "In[16]:\n",
      "from sklearn.decomposition import PCA\n",
      "# keep the first two principal components of the data\n",
      "pca = PCA(n_components=2)\n",
      "# fit PCA model to breast cancer data\n",
      "pca.fit(X_scaled)\n",
      "# transform data onto the first two principal components\n",
      "X_pca = pca.transform(X_scaled)\n",
      "print(\"Original shape: {}\".format(str(X_scaled.shape)))\n",
      "print(\"Reduced shape: {}\".format(str(X_pca.shape)))\n",
      "144 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Out[16]:\n",
      "Original shape: (569, 30)\n",
      "Reduced shape: (569, 2)\n",
      "We can now plot the first two principal components (Figure 3-5):\n",
      "In[17]:\n",
      "# plot first vs. second principal component, colored by class\n",
      "plt.figure(figsize=(8, 8))\n",
      "mglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)\n",
      "plt.legend(cancer.target_names, loc=\"best\")\n",
      "plt.gca().set_aspect(\"equal\")\n",
      "plt.xlabel(\"First principal component\")\n",
      "plt.ylabel(\"Second principal component\")\n",
      "Figure 3-5. Two-dimensional scatter plot of the Breast Cancer dataset using the first two\n",
      "principal components\n",
      "It is important to note that PCA is an unsupervised method, and does not use any class\n",
      "information when finding the rotation. It simply looks at the correlations in the data.\n",
      "For the scatter plot shown here, we plotted the first principal component against the\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning | 145\n",
      "second principal component, and then used the class information to color the points.\n",
      "Y ou can see that the two classes separate quite well in this two-dimensional space.\n",
      "This leads us to believe that even a linear classifier (that would learn a line in this\n",
      "space) could do a reasonably good job at distinguishing the two classes. We can also\n",
      "see that the malignant (red) points are more spread out than the benign (blue) points\n",
      "—something that we could already see a bit from the histograms in Figure 3-4.\n",
      "A downside of PCA is that the two axes in the plot are often not very easy to interpret.\n",
      "The principal components correspond to directions in the original data, so they are\n",
      "combinations of the original features. However, these combinations are usually very\n",
      "complex, as we’ll see shortly. The principal components themselves are stored in the\n",
      "components_ attribute of the PCA object during fitting:\n",
      "In[18]:\n",
      "print(\"PCA component shape: {}\".format(pca.components_.shape))\n",
      "Out[18]:\n",
      "PCA component shape: (2, 30)\n",
      "Each row in components_ corresponds to one principal component, and they are sor‐\n",
      "ted by their importance (the first principal component comes first, etc.). The columns\n",
      "correspond to the original features attribute of the PCA in this example, “mean\n",
      "radius, ” “mean texture, ” and so on. Let’s have a look at the content of components_:\n",
      "In[19]:\n",
      "print(\"PCA components:\\n{}\".format(pca.components_))\n",
      "Out[19]:\n",
      "PCA components:\n",
      "[[ 0.219  0.104  0.228  0.221  0.143  0.239  0.258  0.261  0.138  0.064\n",
      "   0.206  0.017  0.211  0.203  0.015  0.17   0.154  0.183  0.042  0.103\n",
      "   0.228  0.104  0.237  0.225  0.128  0.21   0.229  0.251  0.123  0.132]\n",
      " [-0.234 -0.06  -0.215 -0.231  0.186  0.152  0.06  -0.035  0.19   0.367\n",
      "  -0.106  0.09  -0.089 -0.152  0.204  0.233  0.197  0.13   0.184  0.28\n",
      "  -0.22  -0.045 -0.2   -0.219  0.172  0.144  0.098 -0.008  0.142  0.275]]\n",
      "We can also visualize the coefficients using a heat map ( Figure 3-6), which might be\n",
      "easier to understand:\n",
      "In[20]:\n",
      "plt.matshow(pca.components_, cmap='viridis')\n",
      "plt.yticks([0, 1], [\"First component\", \"Second component\"])\n",
      "plt.colorbar()\n",
      "plt.xticks(range(len(cancer.feature_names)),\n",
      "           cancer.feature_names, rotation=60, ha='left')\n",
      "plt.xlabel(\"Feature\")\n",
      "plt.ylabel(\"Principal components\")\n",
      "146 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-6. Heat map of the first two principal components on the Breast Cancer dataset\n",
      "Y ou can see that in the first component, all features have the same sign (it’s negative,\n",
      "but as we mentioned earlier, it doesn’t matter which direction the arrow points in).\n",
      "That means that there is a general correlation between all features. As one measure‐\n",
      "ment is high, the others are likely to be high as well. The second component has\n",
      "mixed signs, and both of the components involve all of the 30 features. This mixing of\n",
      "all features is what makes explaining the axes in Figure 3-6 so tricky.\n",
      "Eigenfaces for feature extraction\n",
      "Another application of PCA that we mentioned earlier is feature extraction. The idea\n",
      "behind feature extraction is that it is possible to find a representation of your data\n",
      "that is better suited to analysis than the raw representation you were given. A great\n",
      "example of an application where feature extraction is helpful is with images. Images\n",
      "are made up of pixels, usually stored as red, green, and blue (RGB) intensities.\n",
      "Objects in images are usually made up of thousands of pixels, and only together are\n",
      "they meaningful.\n",
      "We will give a very simple application of feature extraction on images using PCA, by\n",
      "working with face images from the Labeled Faces in the Wild dataset. This dataset\n",
      "contains face images of celebrities downloaded from the Internet, and it includes\n",
      "faces of politicians, singers, actors, and athletes from the early 2000s. We use gray‐\n",
      "scale versions of these images, and scale them down for faster processing. Y ou can see\n",
      "some of the images in Figure 3-7:\n",
      "In[21]:\n",
      "from sklearn.datasets import fetch_lfw_people\n",
      "people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
      "image_shape = people.images[0].shape\n",
      "fix, axes = plt.subplots(2, 5, figsize=(15, 8),\n",
      "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
      "for target, image, ax in zip(people.target, people.images, axes.ravel()):\n",
      "    ax.imshow(image)\n",
      "    ax.set_title(people.target_names[target])\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning | 147\n",
      "Figure 3-7. Some images from the Labeled Faces in the Wild dataset\n",
      "There are 3,023 images, each 87×65 pixels large, belonging to 62 different people:\n",
      "In[22]:\n",
      "print(\"people.images.shape: {}\".format(people.images.shape))\n",
      "print(\"Number of classes: {}\".format(len(people.target_names)))\n",
      "Out[22]:\n",
      "people.images.shape: (3023, 87, 65)\n",
      "Number of classes: 62\n",
      "The dataset is a bit skewed, however, containing a lot of images of George W . Bush\n",
      "and Colin Powell, as you can see here:\n",
      "In[23]:\n",
      "# count how often each target appears\n",
      "counts = np.bincount(people.target)\n",
      "# print counts next to target names\n",
      "for i, (count, name) in enumerate(zip(counts, people.target_names)):\n",
      "    print(\"{0:25} {1:3}\".format(name, count), end='   ')\n",
      "    if (i + 1) % 3 == 0:\n",
      "        print()\n",
      "148 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Out[23]:\n",
      "Alejandro Toledo           39   Alvaro Uribe               35\n",
      "Amelie Mauresmo            21   Andre Agassi               36\n",
      "Angelina Jolie             20   Arnold Schwarzenegger      42\n",
      "Atal Bihari Vajpayee       24   Bill Clinton               29\n",
      "Carlos Menem               21   Colin Powell              236\n",
      "David Beckham              31   Donald Rumsfeld           121\n",
      "George W Bush             530   George Robertson           22\n",
      "Gerhard Schroeder         109   Gloria Macapagal Arroyo    44\n",
      "Gray Davis                 26   Guillermo Coria            30\n",
      "Hamid Karzai               22   Hans Blix                  39\n",
      "Hugo Chavez                71   Igor Ivanov                20\n",
      "[...]                           [...]\n",
      "Laura Bush                 41   Lindsay Davenport          22\n",
      "Lleyton Hewitt             41   Luiz Inacio Lula da Silva  48\n",
      "Mahmoud Abbas              29   Megawati Sukarnoputri      33\n",
      "Michael Bloomberg          20   Naomi Watts                22\n",
      "Nestor Kirchner            37   Paul Bremer                20\n",
      "Pete Sampras               22   Recep Tayyip Erdogan       30\n",
      "Ricardo Lagos              27   Roh Moo-hyun               32\n",
      "Rudolph Giuliani           26   Saddam Hussein             23\n",
      "Serena Williams            52   Silvio Berlusconi          33\n",
      "Tiger Woods                23   Tom Daschle                25\n",
      "Tom Ridge                  33   Tony Blair                144\n",
      "Vicente Fox                32   Vladimir Putin             49\n",
      "Winona Ryder               24\n",
      "To make the data less skewed, we will only take up to 50 images of each person\n",
      "(otherwise, the feature extraction would be overwhelmed by the likelihood of George\n",
      "W . Bush):\n",
      "In[24]:\n",
      "mask = np.zeros(people.target.shape, dtype=np.bool)\n",
      "for target in np.unique(people.target):\n",
      "    mask[np.where(people.target == target)[0][:50]] = 1\n",
      "X_people = people.data[mask]\n",
      "y_people = people.target[mask]\n",
      "# scale the grayscale values to be between 0 and 1\n",
      "# instead of 0 and 255 for better numeric stability\n",
      "X_people = X_people / 255.\n",
      "A common task in face recognition is to ask if a previously unseen face belongs to a\n",
      "known person from a database. This has applications in photo collection, social\n",
      "media, and security applications. One way to solve this problem would be to build a\n",
      "classifier where each person is a separate class. However, there are usually many dif‐\n",
      "ferent people in face databases, and very few images of the same person (i.e., very few\n",
      "training examples per class). That makes it hard to train most classifiers. Additionally,\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning | 149\n",
      "you often want to be able to add new people easily, without needing to retrain a large\n",
      "model.\n",
      "A simple solution is to use a one-nearest-neighbor classifier that looks for the most\n",
      "similar face image to the face you are classifying. This classifier could in principle\n",
      "work with only a single training example per class. Let’s take a look at how well\n",
      "KNeighborsClassifier does here:\n",
      "In[25]:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "# split the data into training and test sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X_people, y_people, stratify=y_people, random_state=0)\n",
      "# build a KNeighborsClassifier using one neighbor\n",
      "knn = KNeighborsClassifier(n_neighbors=1)\n",
      "knn.fit(X_train, y_train)\n",
      "print(\"Test set score of 1-nn: {:.2f}\".format(knn.score(X_test, y_test)))\n",
      "Out[25]:\n",
      "Test set score of 1-nn: 0.27\n",
      "We obtain an accuracy of 26.6%, which is not actually that bad for a 62-class classifi‐\n",
      "cation problem (random guessing would give you around 1/62 = 1.5% accuracy), but\n",
      "is also not great. We only correctly identify a person every fourth time.\n",
      "This is where PCA comes in. Computing distances in the original pixel space is quite\n",
      "a bad way to measure similarity between faces. When using a pixel representation to\n",
      "compare two images, we compare the grayscale value of each individual pixel to the\n",
      "value of the pixel in the corresponding position in the other image. This representa‐\n",
      "tion is quite different from how humans would interpret the image of a face, and it is\n",
      "hard to capture the facial features using this raw representation. For example, using\n",
      "pixel distances means that shifting a face by one pixel to the right corresponds to a\n",
      "drastic change, with a completely different representation. We hope that using distan‐\n",
      "ces along principal components can improve our accuracy. Here, we enable the\n",
      "whitening option of PCA, which rescales the principal components to have the same\n",
      "scale. This is the same as using StandardScaler after the transformation. Reusing the\n",
      "data from Figure 3-3 again, whitening corresponds to not only rotating the data, but\n",
      "also rescaling it so that the center panel is a circle instead of an ellipse (see\n",
      "Figure 3-8):\n",
      "In[26]:\n",
      "mglearn.plots.plot_pca_whitening()\n",
      "150 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-8. Transformation of data with PCA using whitening\n",
      "We fit the PCA object to the training data and extract the first 100 principal compo‐\n",
      "nents. Then we transform the training and test data:\n",
      "In[27]:\n",
      "pca = PCA(n_components=100, whiten=True, random_state=0).fit(X_train)\n",
      "X_train_pca = pca.transform(X_train)\n",
      "X_test_pca = pca.transform(X_test)\n",
      "print(\"X_train_pca.shape: {}\".format(X_train_pca.shape))\n",
      "Out[27]:\n",
      "X_train_pca.shape: (1537, 100)\n",
      "The new data has 100 features, the first 100 principal components. Now, we can use\n",
      "the new representation to classify our images using a one-nearest-neighbors classifier:\n",
      "In[28]:\n",
      "knn = KNeighborsClassifier(n_neighbors=1)\n",
      "knn.fit(X_train_pca, y_train)\n",
      "print(\"Test set accuracy: {:.2f}\".format(knn.score(X_test_pca, y_test)))\n",
      "Out[28]:\n",
      "Test set accuracy: 0.36\n",
      "Our accuracy improved quite significantly, from 26.6% to 35.7%, confirming our\n",
      "intuition that the principal components might provide a better representation of the\n",
      "data.\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning | 151\n",
      "For image data, we can also easily visualize the principal components that are found.\n",
      "Remember that components correspond to directions in the input space. The input\n",
      "space here is 50×37-pixel grayscale images, so directions within this space are also\n",
      "50×37-pixel grayscale images.\n",
      "Let’s look at the first couple of principal components (Figure 3-9):\n",
      "In[29]:\n",
      "print(\"pca.components_.shape: {}\".format(pca.components_.shape))\n",
      "Out[29]:\n",
      "pca.components_.shape: (100, 5655)\n",
      "In[30]:\n",
      "fix, axes = plt.subplots(3, 5, figsize=(15, 12),\n",
      "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
      "for i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):\n",
      "    ax.imshow(component.reshape(image_shape),\n",
      "              cmap='viridis')\n",
      "    ax.set_title(\"{}. component\".format((i + 1)))\n",
      "While we certainly cannot understand all aspects of these components, we can guess\n",
      "which aspects of the face images some of the components are capturing. The first\n",
      "component seems to mostly encode the contrast between the face and the back‐\n",
      "ground, the second component encodes differences in lighting between the right and\n",
      "the left half of the face, and so on. While this representation is slightly more semantic\n",
      "than the raw pixel values, it is still quite far from how a human might perceive a face.\n",
      "As the PCA model is based on pixels, the alignment of the face (the position of eyes,\n",
      "chin, and nose) and the lighting both have a strong influence on how similar two\n",
      "images are in their pixel representation. But alignment and lighting are probably not\n",
      "what a human would perceive first. When asking people to rate similarity of faces,\n",
      "they are more likely to use attributes like age, gender, facial expression, and hair style,\n",
      "which are attributes that are hard to infer from the pixel intensities. It’s important to\n",
      "keep in mind that algorithms often interpret data (particularly visual data, such as\n",
      "images, which humans are very familiar with) quite differently from how a human\n",
      "would.\n",
      "152 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-9. Component vectors of the first 15 principal components of the faces dataset\n",
      "Let’s come back to the specific case of PCA, though. We introduced the PCA transfor‐\n",
      "mation as rotating the data and then dropping the components with low variance.\n",
      "Another useful interpretation is to try to find some numbers (the new feature values\n",
      "after the PCA rotation) so that we can express the test points as a weighted sum of the\n",
      "principal components (see Figure 3-10).\n",
      "Figure 3-10. Schematic view of PCA as decomposing an image into a weighted sum of\n",
      "components\n",
      "Here, x0, x1, and so on are the coefficients of the principal components for this data\n",
      "point; in other words, they are the representation of the image in the rotated space.\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning | 153\n",
      "Another way we can try to understand what a PCA model is doing is by looking at\n",
      "the reconstructions of the original data using only some components. In Figure 3-3,\n",
      "after dropping the second component and arriving at the third panel, we undid the\n",
      "rotation and added the mean back to obtain new points in the original space with the\n",
      "second component removed, as shown in the last panel. We can do a similar transfor‐\n",
      "mation for the faces by reducing the data to only some principal components and\n",
      "then rotating back into the original space. This return to the original feature space\n",
      "can be done using the inverse_transform method. Here, we visualize the recon‐\n",
      "struction of some faces using 10, 50, 100, 500, or 2,000 components (Figure 3-11):\n",
      "In[32]:\n",
      "mglearn.plots.plot_pca_faces(X_train, X_test, image_shape)\n",
      "Figure 3-11. Reconstructing three face images using increasing numbers of principal\n",
      "components\n",
      "Y ou can see that when we use only the first 10 principal components, only the essence\n",
      "of the picture, like the face orientation and lighting, is captured. By using more and\n",
      "more principal components, more and more details in the image are preserved. This\n",
      "154 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "corresponds to extending the sum in Figure 3-10 to include more and more terms.\n",
      "Using as many components as there are pixels would mean that we would not discard\n",
      "any information after the rotation, and we would reconstruct the image perfectly.\n",
      "We can also try to use PCA to visualize all the faces in the dataset in a scatter plot\n",
      "using the first two principal components ( Figure 3-12), with classes given by who is\n",
      "shown in the image, similarly to what we did for the cancer dataset:\n",
      "In[33]:\n",
      "mglearn.discrete_scatter(X_train_pca[:, 0], X_train_pca[:, 1], y_train)\n",
      "plt.xlabel(\"First principal component\")\n",
      "plt.ylabel(\"Second principal component\")\n",
      "Figure 3-12. Scatter plot of the faces dataset using the first two principal components (see\n",
      "Figure 3-5 for the corresponding image for the cancer dataset)\n",
      "As you can see, when we use only the first two principal components the whole data\n",
      "is just a big blob, with no separation of classes visible. This is not very surprising,\n",
      "given that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐\n",
      "tures very rough characteristics of the faces.\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning | 155\n",
      "Non-Negative Matrix Factorization (NMF)\n",
      "Non-negative matrix factorization is another unsupervised learning algorithm that\n",
      "aims to extract useful features. It works similarly to PCA and can also be used for\n",
      "dimensionality reduction. As in PCA, we are trying to write each data point as a\n",
      "weighted sum of some components, as illustrated in Figure 3-10. But whereas in PCA\n",
      "we wanted components that were orthogonal and that explained as much variance of\n",
      "the data as possible, in NMF , we want the components and the coefficients to be non-\n",
      "negative; that is, we want both the components and the coefficients to be greater than\n",
      "or equal to zero. Consequently, this method can only be applied to data where each\n",
      "feature is non-negative, as a non-negative sum of non-negative components cannot\n",
      "become negative.\n",
      "The process of decomposing data into a non-negative weighted sum is particularly\n",
      "helpful for data that is created as the addition (or overlay) of several independent\n",
      "sources, such as an audio track of multiple people speaking, or music with many\n",
      "instruments. In these situations, NMF can identify the original components that\n",
      "make up the combined data. Overall, NMF leads to more interpretable components\n",
      "than PCA, as negative components and coefficients can lead to hard-to-interpret can‐\n",
      "cellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and\n",
      "negative parts, and as we mentioned in the description of PCA, the sign is actually\n",
      "arbitrary. Before we apply NMF to the face dataset, let’s briefly revisit the synthetic\n",
      "data.\n",
      "Applying NMF to synthetic data\n",
      "In contrast to when using PCA, we need to ensure that our data is positive for NMF\n",
      "to be able to operate on the data. This means where the data lies relative to the origin\n",
      "(0, 0) actually matters for NMF . Therefore, you can think of the non-negative compo‐\n",
      "nents that are extracted as directions from (0, 0) toward the data.\n",
      "The following example ( Figure 3-13 ) shows the results of NMF on the two-\n",
      "dimensional toy data:\n",
      "In[34]:\n",
      "mglearn.plots.plot_nmf_illustration()\n",
      "156 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-13. Components found by non-negative matrix factorization with two compo‐\n",
      "nents (left) and one component (right)\n",
      "For NMF with two components, as shown on the left, it is clear that all points in the\n",
      "data can be written as a positive combination of the two components. If there are\n",
      "enough components to perfectly reconstruct the data (as many components as there\n",
      "are features), the algorithm will choose directions that point toward the extremes of\n",
      "the data.\n",
      "If we only use a single component, NMF creates a component that points toward the\n",
      "mean, as pointing there best explains the data. Y ou can see that in contrast with PCA,\n",
      "reducing the number of components not only removes some directions, but creates\n",
      "an entirely different set of components! Components in NMF are also not ordered in\n",
      "any specific way, so there is no “first non-negative component”: all components play\n",
      "an equal part.\n",
      "NMF uses a random initialization, which might lead to different results depending on\n",
      "the random seed. In relatively simple cases such as the synthetic data with two com‐\n",
      "ponents, where all the data can be explained perfectly, the randomness has little effect\n",
      "(though it might change the order or scale of the components). In more complex sit‐\n",
      "uations, there might be more drastic changes.\n",
      "Applying NMF to face images\n",
      "Now, let’s apply NMF to the Labeled Faces in the Wild dataset we used earlier. The\n",
      "main parameter of NMF is how many components we want to extract. Usually this is\n",
      "lower than the number of input features (otherwise, the data could be explained by\n",
      "making each pixel a separate component).\n",
      "First, let’s inspect how the number of components impacts how well the data can be\n",
      "reconstructed using NMF (Figure 3-14):\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning | 157\n",
      "In[35]:\n",
      "mglearn.plots.plot_nmf_faces(X_train, X_test, image_shape)\n",
      "Figure 3-14. Reconstructing three face images using increasing numbers of components\n",
      "found by NMF\n",
      "The quality of the back-transformed data is similar to when using PCA, but slightly\n",
      "worse. This is expected, as PCA finds the optimum directions in terms of reconstruc‐\n",
      "tion. NMF is usually not used for its ability to reconstruct or encode data, but rather\n",
      "for finding interesting patterns within the data.\n",
      "As a first look into the data, let’s try extracting only a few components (say, 15).\n",
      "Figure 3-15 shows the result:\n",
      "158 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "In[36]:\n",
      "from sklearn.decomposition import NMF\n",
      "nmf = NMF(n_components=15, random_state=0)\n",
      "nmf.fit(X_train)\n",
      "X_train_nmf = nmf.transform(X_train)\n",
      "X_test_nmf = nmf.transform(X_test)\n",
      "fix, axes = plt.subplots(3, 5, figsize=(15, 12),\n",
      "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
      "for i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())):\n",
      "    ax.imshow(component.reshape(image_shape))\n",
      "    ax.set_title(\"{}. component\".format(i))\n",
      "Figure 3-15. The components found by NMF on the faces dataset when using 15 compo‐\n",
      "nents\n",
      "These components are all positive, and so resemble prototypes of faces much more so\n",
      "than the components shown for PCA in Figure 3-9. For example, one can clearly see\n",
      "that component 3 shows a face rotated somewhat to the right, while component 7\n",
      "shows a face somewhat rotated to the left. Let’s look at the images for which these\n",
      "components are particularly strong, shown in Figures 3-16 and 3-17:\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning | 159\n",
      "In[37]:\n",
      "compn = 3\n",
      "# sort by 3rd component, plot first 10 images\n",
      "inds = np.argsort(X_train_nmf[:, compn])[::-1]\n",
      "fig, axes = plt.subplots(2, 5, figsize=(15, 8),\n",
      "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
      "for i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n",
      "    ax.imshow(X_train[ind].reshape(image_shape))\n",
      "compn = 7\n",
      "# sort by 7th component, plot first 10 images\n",
      "inds = np.argsort(X_train_nmf[:, compn])[::-1]\n",
      "fig, axes = plt.subplots(2, 5, figsize=(15, 8),\n",
      "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
      "for i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n",
      "    ax.imshow(X_train[ind].reshape(image_shape))\n",
      "Figure 3-16. Faces that have a large coefficient for component 3\n",
      "160 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-17. Faces that have a large coefficient for component 7\n",
      "As expected, faces that have a high coefficient for component 3 are faces looking to\n",
      "the right (Figure 3-16), while faces with a high coefficient for component 7 are look‐\n",
      "ing to the left (Figure 3-17). As mentioned earlier, extracting patterns like these works\n",
      "best for data with additive structure, including audio, gene expression, and text data.\n",
      "Let’s walk through one example on synthetic data to see what this might look like.\n",
      "Let’s say we are interested in a signal that is a combination of three different sources\n",
      "(Figure 3-18):\n",
      "In[38]:\n",
      "S = mglearn.datasets.make_signals()\n",
      "plt.figure(figsize=(6, 1))\n",
      "plt.plot(S, '-')\n",
      "plt.xlabel(\"Time\")\n",
      "plt.ylabel(\"Signal\")\n",
      "Figure 3-18. Original signal sources\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning | 161\n",
      "Unfortunately we cannot observe the original signals, but only an additive mixture of\n",
      "all three of them. We want to recover the decomposition of the mixed signal into the\n",
      "original components. We assume that we have many different ways to observe the\n",
      "mixture (say 100 measurement devices), each of which provides us with a series of\n",
      "measurements:\n",
      "In[39]:\n",
      "# mix data into a 100-dimensional state\n",
      "A = np.random.RandomState(0).uniform(size=(100, 3))\n",
      "X = np.dot(S, A.T)\n",
      "print(\"Shape of measurements: {}\".format(X.shape))\n",
      "Out[39]:\n",
      "Shape of measurements: (2000, 100)\n",
      "We can use NMF to recover the three signals:\n",
      "In[40]:\n",
      "nmf = NMF(n_components=3, random_state=42)\n",
      "S_ = nmf.fit_transform(X)\n",
      "print(\"Recovered signal shape: {}\".format(S_.shape))\n",
      "Out[40]:\n",
      "Recovered signal shape: (2000, 3)\n",
      "For comparison, we also apply PCA:\n",
      "In[41]:\n",
      "pca = PCA(n_components=3)\n",
      "H = pca.fit_transform(X)\n",
      "Figure 3-19 shows the signal activity that was discovered by NMF and PCA:\n",
      "In[42]:\n",
      "models = [X, S, S_, H]\n",
      "names = ['Observations (first three measurements)',\n",
      "         'True sources',\n",
      "         'NMF recovered signals',\n",
      "         'PCA recovered signals']\n",
      "fig, axes = plt.subplots(4, figsize=(8, 4), gridspec_kw={'hspace': .5},\n",
      "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
      "for model, name, ax in zip(models, names, axes):\n",
      "    ax.set_title(name)\n",
      "    ax.plot(model[:, :3], '-')\n",
      "162 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-19. Recovering mixed sources using NMF and PCA\n",
      "The figure includes 3 of the 100 measurements from X for reference. As you can see,\n",
      "NMF did a reasonable job of discovering the original sources, while PCA failed and\n",
      "used the first component to explain the majority of the variation in the data. Keep in\n",
      "mind that the components produced by NMF have no natural ordering. In this exam‐\n",
      "ple, the ordering of the NMF components is the same as in the original signal (see the\n",
      "shading of the three curves), but this is purely accidental.\n",
      "There are many other algorithms that can be used to decompose each data point into\n",
      "a weighted sum of a fixed set of components, as PCA and NMF do. Discussing all of\n",
      "them is beyond the scope of this book, and describing the constraints made on the\n",
      "components and coefficients often involves probability theory. If you are interested in\n",
      "this kind of pattern extraction, we recommend that you study the sections of the sci\n",
      "kit_learn user guide on independent component analysis (ICA), factor analysis\n",
      "(FA), and sparse coding (dictionary learning), all of which you can find on the page\n",
      "about decomposition methods.\n",
      "Manifold Learning with t-SNE\n",
      "While PCA is often a good first approach for transforming your data so that you\n",
      "might be able to visualize it using a scatter plot, the nature of the method (applying a\n",
      "rotation and then dropping directions) limits its usefulness, as we saw with the scatter\n",
      "plot of the Labeled Faces in the Wild dataset. There is a class of algorithms for visuali‐\n",
      "zation called manifold learning algorithms that allow for much more complex map‐\n",
      "pings, and often provide better visualizations. A particularly useful one is the t-SNE\n",
      "algorithm.\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning | 163\n",
      "2 Not to be confused with the much larger MNIST dataset.\n",
      "Manifold learning algorithms are mainly aimed at visualization, and so are rarely\n",
      "used to generate more than two new features. Some of them, including t-SNE, com‐\n",
      "pute a new representation of the training data, but don’t allow transformations of new\n",
      "data. This means these algorithms cannot be applied to a test set: rather, they can only\n",
      "transform the data they were trained for. Manifold learning can be useful for explora‐\n",
      "tory data analysis, but is rarely used if the final goal is supervised learning. The idea\n",
      "behind t-SNE is to find a two-dimensional representation of the data that preserves\n",
      "the distances between points as best as possible. t-SNE starts with a random two-\n",
      "dimensional representation for each data point, and then tries to make points that are\n",
      "close in the original feature space closer, and points that are far apart in the original\n",
      "feature space farther apart. t-SNE puts more emphasis on points that are close by,\n",
      "rather than preserving distances between far-apart points. In other words, it tries to\n",
      "preserve the information indicating which points are neighbors to each other.\n",
      "We will apply the t-SNE manifold learning algorithm on a dataset of handwritten dig‐\n",
      "its that is included in scikit-learn.2 Each data point in this dataset is an 8×8 gray‐\n",
      "scale image of a handwritten digit between 0 and 1. Figure 3-20 shows an example\n",
      "image for each class:\n",
      "In[43]:\n",
      "from sklearn.datasets import load_digits\n",
      "digits = load_digits()\n",
      "fig, axes = plt.subplots(2, 5, figsize=(10, 5),\n",
      "                         subplot_kw={'xticks':(), 'yticks': ()})\n",
      "for ax, img in zip(axes.ravel(), digits.images):\n",
      "    ax.imshow(img)\n",
      "164 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-20. Example images from the digits dataset\n",
      "Let’s use PCA to visualize the data reduced to two dimensions. We plot the first two\n",
      "principal components, and color each dot by its class (see Figure 3-21):\n",
      "In[44]:\n",
      "# build a PCA model\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(digits.data)\n",
      "# transform the digits data onto the first two principal components\n",
      "digits_pca = pca.transform(digits.data)\n",
      "colors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n",
      "          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\", \"#535D8E\"]\n",
      "plt.figure(figsize=(10, 10))\n",
      "plt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\n",
      "plt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\n",
      "for i in range(len(digits.data)):\n",
      "    # actually plot the digits as text instead of using scatter\n",
      "    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\n",
      "             color = colors[digits.target[i]],\n",
      "             fontdict={'weight': 'bold', 'size': 9})\n",
      "plt.xlabel(\"First principal component\")\n",
      "plt.ylabel(\"Second principal component\")\n",
      "Here, we actually used the true digit classes as glyphs, to show which class is where.\n",
      "The digits zero, six, and four are relatively well separated using the first two principal\n",
      "components, though they still overlap. Most of the other digits overlap significantly.\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning | 165\n",
      "Figure 3-21. Scatter plot of the digits dataset using the first two principal components\n",
      "Let’s apply t-SNE to the same dataset, and compare the results. As t-SNE does not\n",
      "support transforming new data, the TSNE class has no transform method. Instead, we\n",
      "can call the fit_transform method, which will build the model and immediately\n",
      "return the transformed data (see Figure 3-22):\n",
      "In[45]:\n",
      "from sklearn.manifold import TSNE\n",
      "tsne = TSNE(random_state=42)\n",
      "# use fit_transform instead of fit, as TSNE has no transform method\n",
      "digits_tsne = tsne.fit_transform(digits.data)\n",
      "166 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "In[46]:\n",
      "plt.figure(figsize=(10, 10))\n",
      "plt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\n",
      "plt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\n",
      "for i in range(len(digits.data)):\n",
      "    # actually plot the digits as text instead of using scatter\n",
      "    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\n",
      "             color = colors[digits.target[i]],\n",
      "             fontdict={'weight': 'bold', 'size': 9})\n",
      "plt.xlabel(\"t-SNE feature 0\")\n",
      "plt.xlabel(\"t-SNE feature 1\")\n",
      "Figure 3-22. Scatter plot of the digits dataset using two components found by t-SNE\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning | 167\n",
      "The result of t-SNE is quite remarkable. All the classes are quite clearly separated.\n",
      "The ones and nines are somewhat split up, but most of the classes form a single dense\n",
      "group. Keep in mind that this method has no knowledge of the class labels: it is com‐\n",
      "pletely unsupervised. Still, it can find a representation of the data in two dimensions\n",
      "that clearly separates the classes, based solely on how close points are in the original\n",
      "space.\n",
      "The t-SNE algorithm has some tuning parameters, though it often works well with\n",
      "the default settings. Y ou can try playing with perplexity and early_exaggeration,\n",
      "but the effects are usually minor.\n",
      "Clustering\n",
      "As we described earlier, clustering is the task of partitioning the dataset into groups,\n",
      "called clusters. The goal is to split up the data in such a way that points within a single\n",
      "cluster are very similar and points in different clusters are different. Similarly to clas‐\n",
      "sification algorithms, clustering algorithms assign (or predict) a number to each data\n",
      "point, indicating which cluster a particular point belongs to.\n",
      "k-Means Clustering\n",
      "k-means clustering is one of the simplest and most commonly used clustering algo‐\n",
      "rithms. It tries to find cluster centers that are representative of certain regions of the\n",
      "data. The algorithm alternates between two steps: assigning each data point to the\n",
      "closest cluster center, and then setting each cluster center as the mean of the data\n",
      "points that are assigned to it. The algorithm is finished when the assignment of\n",
      "instances to clusters no longer changes. The following example ( Figure 3-23 ) illus‐\n",
      "trates the algorithm on a synthetic dataset:\n",
      "In[47]:\n",
      "mglearn.plots.plot_kmeans_algorithm()\n",
      "168 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-23. Input data and three steps of the k-means algorithm\n",
      "Cluster centers are shown as triangles, while data points are shown as circles. Colors\n",
      "indicate cluster membership. We specified that we are looking for three clusters, so\n",
      "the algorithm was initialized by declaring three data points randomly as cluster cen‐\n",
      "ters (see “Initialization”). Then the iterative algorithm starts. First, each data point is\n",
      "assigned to the cluster center it is closest to (see “ Assign Points (1)”). Next, the cluster\n",
      "centers are updated to be the mean of the assigned points (see “Recompute Centers\n",
      "(1)”). Then the process is repeated two more times. After the third iteration, the\n",
      "assignment of points to cluster centers remained unchanged, so the algorithm stops.\n",
      "Given new data points, k-means will assign each to the closest cluster center. The next\n",
      "example (Figure 3-24) shows the boundaries of the cluster centers that were learned\n",
      "in Figure 3-23:\n",
      "In[48]:\n",
      "mglearn.plots.plot_kmeans_boundaries()\n",
      "Clustering | 169\n",
      "3 If you don’t provide n_clusters, it is set to 8 by default. There is no particular reason why you should use this\n",
      "value.\n",
      "Figure 3-24. Cluster centers and cluster boundaries found by the k-means algorithm\n",
      "Applying k-means with scikit-learn is quite straightforward. Here, we apply it to\n",
      "the synthetic data that we used for the preceding plots. We instantiate the KMeans\n",
      "class, and set the number of clusters we are looking for.3 Then we call the fit method\n",
      "with the data:\n",
      "In[49]:\n",
      "from sklearn.datasets import make_blobs\n",
      "from sklearn.cluster import KMeans\n",
      "# generate synthetic two-dimensional data\n",
      "X, y = make_blobs(random_state=1)\n",
      "# build the clustering model\n",
      "kmeans = KMeans(n_clusters=3)\n",
      "kmeans.fit(X)\n",
      "During the algorithm, each training data point in X is assigned a cluster label. Y ou can\n",
      "find these labels in the kmeans.labels_ attribute:\n",
      "170 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "In[50]:\n",
      "print(\"Cluster memberships:\\n{}\".format(kmeans.labels_))\n",
      "Out[50]:\n",
      "Cluster memberships:\n",
      "[1 2 2 2 0 0 0 2 1 1 2 2 0 1 0 0 0 1 2 2 0 2 0 1 2 0 0 1 1 0 1 1 0 1 2 0 2\n",
      " 2 2 0 0 2 1 2 2 0 1 1 1 1 2 0 0 0 1 0 2 2 1 1 2 0 0 2 2 0 1 0 1 2 2 2 0 1\n",
      " 1 2 0 0 1 2 1 2 2 0 1 1 1 1 2 1 0 1 1 2 2 0 0 1 0 1]\n",
      "As we asked for three clusters, the clusters are numbered 0 to 2.\n",
      "Y ou can also assign cluster labels to new points, using the predict method. Each new\n",
      "point is assigned to the closest cluster center when predicting, but the existing model\n",
      "is not changed. Running predict on the training set returns the same result as\n",
      "labels_:\n",
      "In[51]:\n",
      "print(kmeans.predict(X))\n",
      "Out[51]:\n",
      "[1 2 2 2 0 0 0 2 1 1 2 2 0 1 0 0 0 1 2 2 0 2 0 1 2 0 0 1 1 0 1 1 0 1 2 0 2\n",
      " 2 2 0 0 2 1 2 2 0 1 1 1 1 2 0 0 0 1 0 2 2 1 1 2 0 0 2 2 0 1 0 1 2 2 2 0 1\n",
      " 1 2 0 0 1 2 1 2 2 0 1 1 1 1 2 1 0 1 1 2 2 0 0 1 0 1]\n",
      "Y ou can see that clustering is somewhat similar to classification, in that each item gets\n",
      "a label. However, there is no ground truth, and consequently the labels themselves\n",
      "have no a priori meaning. Let’s go back to the example of clustering face images that\n",
      "we discussed before. It might be that the cluster 3 found by the algorithm contains\n",
      "only faces of your friend Bela. Y ou can only know that after you look at the pictures,\n",
      "though, and the number 3 is arbitrary. The only information the algorithm gives you\n",
      "is that all faces labeled as 3 are similar.\n",
      "For the clustering we just computed on the two-dimensional toy dataset, that means\n",
      "that we should not assign any significance to the fact that one group was labeled 0\n",
      "and another one was labeled 1. Running the algorithm again might result in a differ‐\n",
      "ent numbering of clusters because of the random nature of the initialization.\n",
      "Here is a plot of this data again ( Figure 3-25). The cluster centers are stored in the\n",
      "cluster_centers_ attribute, and we plot them as triangles:\n",
      "In[52]:\n",
      "mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')\n",
      "mglearn.discrete_scatter(\n",
      "    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],\n",
      "    markers='^', markeredgewidth=2)\n",
      "Clustering | 171\n",
      "Figure 3-25. Cluster assignments and cluster centers found by k-means with three\n",
      "clusters\n",
      "We can also use more or fewer cluster centers (Figure 3-26):\n",
      "In[53]:\n",
      "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
      "# using two cluster centers:\n",
      "kmeans = KMeans(n_clusters=2)\n",
      "kmeans.fit(X)\n",
      "assignments = kmeans.labels_\n",
      "mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[0])\n",
      "# using five cluster centers:\n",
      "kmeans = KMeans(n_clusters=5)\n",
      "kmeans.fit(X)\n",
      "assignments = kmeans.labels_\n",
      "mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[1])\n",
      "172 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-26. Cluster assignments found by k-means using two clusters (left) and five\n",
      "clusters (right)\n",
      "Failure cases of k-means\n",
      "Even if you know the “right” number of clusters for a given dataset, k-means might\n",
      "not always be able to recover them. Each cluster is defined solely by its center, which\n",
      "means that each cluster is a convex shape. As a result of this, k-means can only cap‐\n",
      "ture relatively simple shapes. k-means also assumes that all clusters have the same\n",
      "“diameter” in some sense; it always draws the boundary between clusters to be exactly\n",
      "in the middle between the cluster centers. That can sometimes lead to surprising\n",
      "results, as shown in Figure 3-27:\n",
      "In[54]:\n",
      "X_varied, y_varied = make_blobs(n_samples=200,\n",
      "                                cluster_std=[1.0, 2.5, 0.5],\n",
      "                                random_state=170)\n",
      "y_pred = KMeans(n_clusters=3, random_state=0).fit_predict(X_varied)\n",
      "mglearn.discrete_scatter(X_varied[:, 0], X_varied[:, 1], y_pred)\n",
      "plt.legend([\"cluster 0\", \"cluster 1\", \"cluster 2\"], loc='best')\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "Clustering | 173\n",
      "Figure 3-27. Cluster assignments found by k-means when clusters have different\n",
      "densities\n",
      "One might have expected the dense region in the lower left to be the first cluster, the\n",
      "dense region in the upper right to be the second, and the less dense region in the cen‐\n",
      "ter to be the third. Instead, both cluster 0 and cluster 1 have some points that are far\n",
      "away from all the other points in these clusters that “reach” toward the center.\n",
      "k-means also assumes that all directions are equally important for each cluster. The\n",
      "following plot ( Figure 3-28) shows a two-dimensional dataset where there are three\n",
      "clearly separated parts in the data. However, these groups are stretched toward the\n",
      "diagonal. As k-means only considers the distance to the nearest cluster center, it can’t\n",
      "handle this kind of data:\n",
      "In[55]:\n",
      "# generate some random cluster data\n",
      "X, y = make_blobs(random_state=170, n_samples=600)\n",
      "rng = np.random.RandomState(74)\n",
      "# transform the data to be stretched\n",
      "transformation = rng.normal(size=(2, 2))\n",
      "X = np.dot(X, transformation)\n",
      "174 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "# cluster the data into three clusters\n",
      "kmeans = KMeans(n_clusters=3)\n",
      "kmeans.fit(X)\n",
      "y_pred = kmeans.predict(X)\n",
      "# plot the cluster assignments and cluster centers\n",
      "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm3)\n",
      "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
      "            marker='^', c=[0, 1, 2], s=100, linewidth=2, cmap=mglearn.cm3)\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "Figure 3-28. k-means fails to identify nonspherical clusters\n",
      "k-means also performs poorly if the clusters have more complex shapes, like the\n",
      "two_moons data we encountered in Chapter 2 (see Figure 3-29):\n",
      "In[56]:\n",
      "# generate synthetic two_moons data (with less noise this time)\n",
      "from sklearn.datasets import make_moons\n",
      "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
      "# cluster the data into two clusters\n",
      "kmeans = KMeans(n_clusters=2)\n",
      "kmeans.fit(X)\n",
      "y_pred = kmeans.predict(X)\n",
      "Clustering | 175\n",
      "# plot the cluster assignments and cluster centers\n",
      "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\n",
      "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
      "            marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2)\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "Figure 3-29. k-means fails to identify clusters with complex shapes\n",
      "Here, we would hope that the clustering algorithm can discover the two half-moon\n",
      "shapes. However, this is not possible using the k-means algorithm.\n",
      "Vector quantization, or seeing k-means as decomposition\n",
      "Even though k-means is a clustering algorithm, there are interesting parallels between\n",
      "k-means and the decomposition methods like PCA and NMF that we discussed ear‐\n",
      "lier. Y ou might remember that PCA tries to find directions of maximum variance in\n",
      "the data, while NMF tries to find additive components, which often correspond to\n",
      "“extremes” or “parts” of the data (see Figure 3-13). Both methods tried to express the\n",
      "data points as a sum over some components. k-means, on the other hand, tries to rep‐\n",
      "resent each data point using a cluster center. Y ou can think of that as each point being\n",
      "represented using only a single component, which is given by the cluster center. This\n",
      "view of k-means as a decomposition method, where each point is represented using a\n",
      "single component, is called vector quantization.\n",
      "176 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Let’s do a side-by-side comparison of PCA, NMF , and k-means, showing the compo‐\n",
      "nents extracted ( Figure 3-30 ), as well as reconstructions of faces from the test set\n",
      "using 100 components ( Figure 3-31). For k-means, the reconstruction is the closest\n",
      "cluster center found on the training set:\n",
      "In[57]:\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X_people, y_people, stratify=y_people, random_state=0)\n",
      "nmf = NMF(n_components=100, random_state=0)\n",
      "nmf.fit(X_train)\n",
      "pca = PCA(n_components=100, random_state=0)\n",
      "pca.fit(X_train)\n",
      "kmeans = KMeans(n_clusters=100, random_state=0)\n",
      "kmeans.fit(X_train)\n",
      "X_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))\n",
      "X_reconstructed_kmeans = kmeans.cluster_centers_[kmeans.predict(X_test)]\n",
      "X_reconstructed_nmf = np.dot(nmf.transform(X_test), nmf.components_)\n",
      "In[58]:\n",
      "fig, axes = plt.subplots(3, 5, figsize=(8, 8),\n",
      "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
      "fig.suptitle(\"Extracted Components\")\n",
      "for ax, comp_kmeans, comp_pca, comp_nmf in zip(\n",
      "        axes.T, kmeans.cluster_centers_, pca.components_, nmf.components_):\n",
      "    ax[0].imshow(comp_kmeans.reshape(image_shape))\n",
      "    ax[1].imshow(comp_pca.reshape(image_shape), cmap='viridis')\n",
      "    ax[2].imshow(comp_nmf.reshape(image_shape))\n",
      "axes[0, 0].set_ylabel(\"kmeans\")\n",
      "axes[1, 0].set_ylabel(\"pca\")\n",
      "axes[2, 0].set_ylabel(\"nmf\")\n",
      "fig, axes = plt.subplots(4, 5, subplot_kw={'xticks': (), 'yticks': ()},\n",
      "                         figsize=(8, 8))\n",
      "fig.suptitle(\"Reconstructions\")\n",
      "for ax, orig, rec_kmeans, rec_pca, rec_nmf in zip(\n",
      "        axes.T, X_test, X_reconstructed_kmeans, X_reconstructed_pca,\n",
      "        X_reconstructed_nmf):\n",
      "    ax[0].imshow(orig.reshape(image_shape))\n",
      "    ax[1].imshow(rec_kmeans.reshape(image_shape))\n",
      "    ax[2].imshow(rec_pca.reshape(image_shape))\n",
      "    ax[3].imshow(rec_nmf.reshape(image_shape))\n",
      "axes[0, 0].set_ylabel(\"original\")\n",
      "axes[1, 0].set_ylabel(\"kmeans\")\n",
      "axes[2, 0].set_ylabel(\"pca\")\n",
      "axes[3, 0].set_ylabel(\"nmf\")\n",
      "Clustering | 177\n",
      "Figure 3-30. Comparing k-means cluster centers to components found by PCA and NMF\n",
      "178 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-31. Comparing image reconstructions using k-means, PCA, and NMF with 100\n",
      "components (or cluster centers)—k-means uses only a single cluster center per image\n",
      "An interesting aspect of vector quantization using k-means is that we can use many\n",
      "more clusters than input dimensions to encode our data. Let’s go back to the\n",
      "two_moons data. Using PCA or NMF , there is nothing much we can do to this data, as\n",
      "it lives in only two dimensions. Reducing it to one dimension with PCA or NMF\n",
      "would completely destroy the structure of the data. But we can find a more expressive\n",
      "representation with k-means, by using more cluster centers (see Figure 3-32):\n",
      "Clustering | 179\n",
      "In[59]:\n",
      "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
      "kmeans = KMeans(n_clusters=10, random_state=0)\n",
      "kmeans.fit(X)\n",
      "y_pred = kmeans.predict(X)\n",
      "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=60, cmap='Paired')\n",
      "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=60,\n",
      "            marker='^', c=range(kmeans.n_clusters), linewidth=2, cmap='Paired')\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "print(\"Cluster memberships:\\n{}\".format(y_pred))\n",
      "Out[59]:\n",
      "Cluster memberships:\n",
      "[9 2 5 4 2 7 9 6 9 6 1 0 2 6 1 9 3 0 3 1 7 6 8 6 8 5 2 7 5 8 9 8 6 5 3 7 0\n",
      " 9 4 5 0 1 3 5 2 8 9 1 5 6 1 0 7 4 6 3 3 6 3 8 0 4 2 9 6 4 8 2 8 4 0 4 0 5\n",
      " 6 4 5 9 3 0 7 8 0 7 5 8 9 8 0 7 3 9 7 1 7 2 2 0 4 5 6 7 8 9 4 5 4 1 2 3 1\n",
      " 8 8 4 9 2 3 7 0 9 9 1 5 8 5 1 9 5 6 7 9 1 4 0 6 2 6 4 7 9 5 5 3 8 1 9 5 6\n",
      " 3 5 0 2 9 3 0 8 6 0 3 3 5 6 3 2 0 2 3 0 2 6 3 4 4 1 5 6 7 1 1 3 2 4 7 2 7\n",
      " 3 8 6 4 1 4 3 9 9 5 1 7 5 8 2]\n",
      "Figure 3-32. Using many k-means clusters to cover the variation in a complex dataset\n",
      "180 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "4 In this case, “best” means that the sum of variances of the clusters is small.\n",
      "We used 10 cluster centers, which means each point is now assigned a number\n",
      "between 0 and 9. We can see this as the data being represented using 10 components\n",
      "(that is, we have 10 new features), with all features being 0, apart from the one that\n",
      "represents the cluster center the point is assigned to. Using this 10-dimensional repre‐\n",
      "sentation, it would now be possible to separate the two half-moon shapes using a lin‐\n",
      "ear model, which would not have been possible using the original two features. It is\n",
      "also possible to get an even more expressive representation of the data by using the\n",
      "distances to each of the cluster centers as features. This can be accomplished using\n",
      "the transform method of kmeans:\n",
      "In[60]:\n",
      "distance_features = kmeans.transform(X)\n",
      "print(\"Distance feature shape: {}\".format(distance_features.shape))\n",
      "print(\"Distance features:\\n{}\".format(distance_features))\n",
      "Out[60]:\n",
      "Distance feature shape: (200, 10)\n",
      "Distance features:\n",
      "[[ 0.922  1.466  1.14  ...,  1.166  1.039  0.233]\n",
      " [ 1.142  2.517  0.12  ...,  0.707  2.204  0.983]\n",
      " [ 0.788  0.774  1.749 ...,  1.971  0.716  0.944]\n",
      " ...,\n",
      " [ 0.446  1.106  1.49  ...,  1.791  1.032  0.812]\n",
      " [ 1.39   0.798  1.981 ...,  1.978  0.239  1.058]\n",
      " [ 1.149  2.454  0.045 ...,  0.572  2.113  0.882]]\n",
      "k-means is a very popular algorithm for clustering, not only because it is relatively\n",
      "easy to understand and implement, but also because it runs relatively quickly. k-\n",
      "means scales easily to large datasets, and scikit-learn even includes a more scalable\n",
      "variant in the MiniBatchKMeans class, which can handle very large datasets.\n",
      "One of the drawbacks of k-means is that it relies on a random initialization, which\n",
      "means the outcome of the algorithm depends on a random seed. By default, scikit-\n",
      "learn runs the algorithm 10 times with 10 different random initializations, and\n",
      "returns the best result. 4 Further downsides of k-means are the relatively restrictive\n",
      "assumptions made on the shape of clusters, and the requirement to specify the num‐\n",
      "ber of clusters you are looking for (which might not be known in a real-world\n",
      "application).\n",
      "Next, we will look at two more clustering algorithms that improve upon these proper‐\n",
      "ties in some ways.\n",
      "Clustering | 181\n",
      "Agglomerative Clustering\n",
      "Agglomerative clustering refers to a collection of clustering algorithms that all build\n",
      "upon the same principles: the algorithm starts by declaring each point its own cluster,\n",
      "and then merges the two most similar clusters until some stopping criterion is satis‐\n",
      "fied. The stopping criterion implemented in scikit-learn is the number of clusters,\n",
      "so similar clusters are merged until only the specified number of clusters are left.\n",
      "There are several linkage criteria that specify how exactly the “most similar cluster” is\n",
      "measured. This measure is always defined between two existing clusters.\n",
      "The following three choices are implemented in scikit-learn:\n",
      "ward\n",
      "The default choice, ward picks the two clusters to merge such that the variance\n",
      "within all clusters increases the least. This often leads to clusters that are rela‐\n",
      "tively equally sized.\n",
      "average\n",
      "average linkage merges the two clusters that have the smallest average distance\n",
      "between all their points.\n",
      "complete\n",
      "complete linkage (also known as maximum linkage) merges the two clusters that\n",
      "have the smallest maximum distance between their points.\n",
      "ward works on most datasets, and we will use it in our examples. If the clusters have\n",
      "very dissimilar numbers of members (if one is much bigger than all the others, for\n",
      "example), average or complete might work better.\n",
      "The following plot ( Figure 3-33) illustrates the progression of agglomerative cluster‐\n",
      "ing on a two-dimensional dataset, looking for three clusters:\n",
      "In[61]:\n",
      "mglearn.plots.plot_agglomerative_algorithm()\n",
      "182 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "5 We could also use the labels_ attribute, as we did for k-means.\n",
      "Figure 3-33. Agglomerative clustering iteratively joins the two closest clusters\n",
      "Initially, each point is its own cluster. Then, in each step, the two clusters that are\n",
      "closest are merged. In the first four steps, two single-point clusters are picked and\n",
      "these are joined into two-point clusters. In step 5, one of the two-point clusters is\n",
      "extended to a third point, and so on. In step 9, there are only three clusters remain‐\n",
      "ing. As we specified that we are looking for three clusters, the algorithm then stops.\n",
      "Let’s have a look at how agglomerative clustering performs on the simple three-\n",
      "cluster data we used here. Because of the way the algorithm works, agglomerative\n",
      "clustering cannot make predictions for new data points. Therefore, Agglomerative\n",
      "Clustering has no predict method. To build the model and get the cluster member‐\n",
      "ships on the training set, use the fit_predict method instead.5 The result is shown\n",
      "in Figure 3-34:\n",
      "In[62]:\n",
      "from sklearn.cluster import AgglomerativeClustering\n",
      "X, y = make_blobs(random_state=1)\n",
      "agg = AgglomerativeClustering(n_clusters=3)\n",
      "assignment = agg.fit_predict(X)\n",
      "mglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "Clustering | 183\n",
      "Figure 3-34. Cluster assignment using agglomerative clustering with three clusters\n",
      "As expected, the algorithm recovers the clustering perfectly. While the scikit-learn\n",
      "implementation of agglomerative clustering requires you to specify the number of\n",
      "clusters you want the algorithm to find, agglomerative clustering methods provide\n",
      "some help with choosing the right number, which we will discuss next.\n",
      "Hierarchical clustering and dendrograms\n",
      "Agglomerative clustering produces what is known as a hierarchical clustering. The\n",
      "clustering proceeds iteratively, and every point makes a journey from being a single\n",
      "point cluster to belonging to some final cluster. Each intermediate step provides a\n",
      "clustering of the data (with a different number of clusters). It is sometimes helpful to\n",
      "look at all possible clusterings jointly. The next example (Figure 3-35) shows an over‐\n",
      "lay of all the possible clusterings shown in Figure 3-33, providing some insight into\n",
      "how each cluster breaks up into smaller clusters:\n",
      "In[63]:\n",
      "mglearn.plots.plot_agglomerative()\n",
      "184 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-35. Hierarchical cluster assignment (shown as lines) generated with agglomera‐\n",
      "tive clustering, with numbered data points (cf. Figure 3-36)\n",
      "While this visualization provides a very detailed view of the hierarchical clustering, it\n",
      "relies on the two-dimensional nature of the data and therefore cannot be used on\n",
      "datasets that have more than two features. There is, however, another tool to visualize\n",
      "hierarchical clustering, called a dendrogram, that can handle multidimensional\n",
      "datasets.\n",
      "Unfortunately, scikit-learn currently does not have the functionality to draw den‐\n",
      "drograms. However, you can generate them easily using SciPy. The SciPy clustering\n",
      "algorithms have a slightly different interface to the scikit-learn clustering algo‐\n",
      "rithms. SciPy provides a function that takes a data array X and computes a linkage\n",
      "array, which encodes hierarchical cluster similarities. We can then feed this linkage\n",
      "array into the scipy dendrogram function to plot the dendrogram (Figure 3-36):\n",
      "In[64]:\n",
      "# Import the dendrogram function and the ward clustering function from SciPy\n",
      "from scipy.cluster.hierarchy import dendrogram, ward\n",
      "X, y = make_blobs(random_state=0, n_samples=12)\n",
      "# Apply the ward clustering to the data array X\n",
      "# The SciPy ward function returns an array that specifies the distances\n",
      "# bridged when performing agglomerative clustering\n",
      "linkage_array = ward(X)\n",
      "Clustering | 185\n",
      "# Now we plot the dendrogram for the linkage_array containing the distances\n",
      "# between clusters\n",
      "dendrogram(linkage_array)\n",
      "# Mark the cuts in the tree that signify two or three clusters\n",
      "ax = plt.gca()\n",
      "bounds = ax.get_xbound()\n",
      "ax.plot(bounds, [7.25, 7.25], '--', c='k')\n",
      "ax.plot(bounds, [4, 4], '--', c='k')\n",
      "ax.text(bounds[1], 7.25, ' two clusters', va='center', fontdict={'size': 15})\n",
      "ax.text(bounds[1], 4, ' three clusters', va='center', fontdict={'size': 15})\n",
      "plt.xlabel(\"Sample index\")\n",
      "plt.ylabel(\"Cluster distance\")\n",
      "Figure 3-36. Dendrogram of the clustering shown in Figure 3-35 with lines indicating\n",
      "splits into two and three clusters\n",
      "The dendrogram shows data points as points on the bottom (numbered from 0 to\n",
      "11). Then, a tree is plotted with these points (representing single-point clusters) as the\n",
      "leaves, and a new node parent is added for each two clusters that are joined.\n",
      "Reading from bottom to top, the data points 1 and 4 are joined first (as you could see\n",
      "in Figure 3-33). Next, points 6 and 9 are joined into a cluster, and so on. At the top\n",
      "level, there are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the\n",
      "other consisting of points 1, 4, 3, 2, and 8. These correspond to the two largest clus‐\n",
      "ters in the lefthand side of the plot.\n",
      "186 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "The y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐\n",
      "rithm two clusters get merged. The length of each branch also shows how far apart\n",
      "the merged clusters are. The longest branches in this dendrogram are the three lines\n",
      "that are marked by the dashed line labeled “three clusters. ” That these are the longest\n",
      "branches indicates that going from three to two clusters meant merging some very\n",
      "far-apart points. We see this again at the top of the chart, where merging the two\n",
      "remaining clusters into a single cluster again bridges a relatively large distance.\n",
      "Unfortunately, agglomerative clustering still fails at separating complex shapes like\n",
      "the two_moons dataset. But the same is not true for the next algorithm we will look at,\n",
      "DBSCAN.\n",
      "DBSCAN\n",
      "Another very useful clustering algorithm is DBSCAN (which stands for “density-\n",
      "based spatial clustering of applications with noise”). The main benefits of DBSCAN\n",
      "are that it does not require the user to set the number of clusters a priori, it can cap‐\n",
      "ture clusters of complex shapes, and it can identify points that are not part of any\n",
      "cluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\n",
      "still scales to relatively large datasets.\n",
      "DBSCAN works by identifying points that are in “crowded” regions of the feature\n",
      "space, where many data points are close together. These regions are referred to as\n",
      "dense regions in feature space. The idea behind DBSCAN is that clusters form dense\n",
      "regions of data, separated by regions that are relatively empty.\n",
      "Points that are within a dense region are called core samples (or core points), and they\n",
      "are defined as follows. There are two parameters in DBSCAN: min_samples and eps.\n",
      "If there are at least min_samples many data points within a distance of eps to a given\n",
      "data point, that data point is classified as a core sample. Core samples that are closer\n",
      "to each other than the distance eps are put into the same cluster by DBSCAN.\n",
      "The algorithm works by picking an arbitrary point to start with. It then finds all\n",
      "points with distance eps or less from that point. If there are less than min_samples\n",
      "points within distance eps of the starting point, this point is labeled as noise, meaning\n",
      "that it doesn’t belong to any cluster. If there are more than min_samples points within\n",
      "a distance of eps, the point is labeled a core sample and assigned a new cluster label.\n",
      "Then, all neighbors (within eps) of the point are visited. If they have not been\n",
      "assigned a cluster yet, they are assigned the new cluster label that was just created. If\n",
      "they are core samples, their neighbors are visited in turn, and so on. The cluster\n",
      "grows until there are no more core samples within distance eps of the cluster. Then\n",
      "another point that hasn’t yet been visited is picked, and the same procedure is\n",
      "repeated.\n",
      "Clustering | 187\n",
      "In the end, there are three kinds of points: core points, points that are within distance\n",
      "eps of core points (called boundary points), and noise. When the DBSCAN algorithm\n",
      "is run on a particular dataset multiple times, the clustering of the core points is always\n",
      "the same, and the same points will always be labeled as noise. However, a boundary\n",
      "point might be neighbor to core samples of more than one cluster. Therefore, the\n",
      "cluster membership of boundary points depends on the order in which points are vis‐\n",
      "ited. Usually there are only few boundary points, and this slight dependence on the\n",
      "order of points is not important.\n",
      "Let’s apply DBSCAN on the synthetic dataset we used to demonstrate agglomerative\n",
      "clustering. Like agglomerative clustering, DBSCAN does not allow predictions on\n",
      "new test data, so we will use the fit_predict method to perform clustering and\n",
      "return the cluster labels in one step:\n",
      "In[65]:\n",
      "from sklearn.cluster import DBSCAN\n",
      "X, y = make_blobs(random_state=0, n_samples=12)\n",
      "dbscan = DBSCAN()\n",
      "clusters = dbscan.fit_predict(X)\n",
      "print(\"Cluster memberships:\\n{}\".format(clusters))\n",
      "Out[65]:\n",
      "Cluster memberships:\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "As you can see, all data points were assigned the label -1, which stands for noise. This\n",
      "is a consequence of the default parameter settings for eps and min_samples, which\n",
      "are not tuned for small toy datasets. The cluster assignments for different values of\n",
      "min_samples and eps are shown below, and visualized in Figure 3-37:\n",
      "In[66]:\n",
      "mglearn.plots.plot_dbscan()\n",
      "Out[66]:\n",
      "min_samples: 2 eps: 1.000000  cluster: [-1  0  0 -1  0 -1  1  1  0  1 -1 -1]\n",
      "min_samples: 2 eps: 1.500000  cluster: [0 1 1 1 1 0 2 2 1 2 2 0]\n",
      "min_samples: 2 eps: 2.000000  cluster: [0 1 1 1 1 0 0 0 1 0 0 0]\n",
      "min_samples: 2 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "min_samples: 3 eps: 1.000000  cluster: [-1  0  0 -1  0 -1  1  1  0  1 -1 -1]\n",
      "min_samples: 3 eps: 1.500000  cluster: [0 1 1 1 1 0 2 2 1 2 2 0]\n",
      "min_samples: 3 eps: 2.000000  cluster: [0 1 1 1 1 0 0 0 1 0 0 0]\n",
      "min_samples: 3 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "min_samples: 5 eps: 1.000000  cluster: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "min_samples: 5 eps: 1.500000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\n",
      "min_samples: 5 eps: 2.000000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\n",
      "min_samples: 5 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "188 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-37. Cluster assignments found by DBSCAN with varying settings for the\n",
      "min_samples and eps parameters\n",
      "In this plot, points that belong to clusters are solid, while the noise points are shown\n",
      "in white. Core samples are shown as large markers, while boundary points are dis‐\n",
      "played as smaller markers. Increasing eps (going from left to right in the figure)\n",
      "means that more points will be included in a cluster. This makes clusters grow, but\n",
      "might also lead to multiple clusters joining into one. Increasing min_samples (going\n",
      "from top to bottom in the figure) means that fewer points will be core points, and\n",
      "more points will be labeled as noise.\n",
      "The parameter eps is somewhat more important, as it determines what it means for\n",
      "points to be “close. ” Setting eps to be very small will mean that no points are core\n",
      "samples, and may lead to all points being labeled as noise. Setting eps to be very large\n",
      "will result in all points forming a single cluster.\n",
      "The min_samples setting mostly determines whether points in less dense regions will\n",
      "be labeled as outliers or as their own clusters. If you decrease min_samples, anything\n",
      "that would have been a cluster with less than min_samples many samples will now be\n",
      "labeled as noise. min_samples therefore determines the minimum cluster size. Y ou\n",
      "can see this very clearly in Figure 3-37, when going from min_samples=3 to min_sam\n",
      "ples=5 with eps=1.5. With min_samples=3, there are three clusters: one of four\n",
      "Clustering | 189\n",
      "points, one of five points, and one of three points. Using min_samples=5, the two\n",
      "smaller clusters (with three and four points) are now labeled as noise, and only the\n",
      "cluster with five samples remains.\n",
      "While DBSCAN doesn’t require setting the number of clusters explicitly, setting eps\n",
      "implicitly controls how many clusters will be found. Finding a good setting for eps is\n",
      "sometimes easier after scaling the data using StandardScaler or MinMaxScaler, as\n",
      "using these scaling techniques will ensure that all features have similar ranges.\n",
      "Figure 3-38  shows the result of running DBSCAN on the two_moons dataset. The\n",
      "algorithm actually finds the two half-circles and separates them using the default\n",
      "settings:\n",
      "In[67]:\n",
      "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
      "# rescale the data to zero mean and unit variance\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(X)\n",
      "X_scaled = scaler.transform(X)\n",
      "dbscan = DBSCAN()\n",
      "clusters = dbscan.fit_predict(X_scaled)\n",
      "# plot the cluster assignments\n",
      "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm2, s=60)\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "As the algorithm produced the desired number of clusters (two), the parameter set‐\n",
      "tings seem to work well. If we decrease eps to 0.2 (from the default of 0.5), we will\n",
      "get eight clusters, which is clearly too many. Increasing eps to 0.7 results in a single\n",
      "cluster.\n",
      "When using DBSCAN, you need to be careful about handling the returned cluster\n",
      "assignments. The use of -1 to indicate noise might result in unexpected effects when\n",
      "using the cluster labels to index another array.\n",
      "190 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\n",
      "Comparing and Evaluating Clustering Algorithms\n",
      "One of the challenges in applying clustering algorithms is that it is very hard to assess\n",
      "how well an algorithm worked, and to compare outcomes between different algo‐\n",
      "rithms. After talking about the algorithms behind k-means, agglomerative clustering,\n",
      "and DBSCAN, we will now compare them on some real-world datasets.\n",
      "Evaluating clustering with ground truth\n",
      "There are metrics that can be used to assess the outcome of a clustering algorithm\n",
      "relative to a ground truth clustering, the most important ones being the adjusted rand\n",
      "index (ARI) and normalized mutual information (NMI), which both provide a quanti‐\n",
      "tative measure between 0 and 1.\n",
      "Here, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\n",
      "using ARI. We also include what it looks like when we randomly assign points to two\n",
      "clusters for comparison (see Figure 3-39):\n",
      "Clustering | 191\n",
      "In[68]:\n",
      "from sklearn.metrics.cluster import adjusted_rand_score\n",
      "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
      "# rescale the data to zero mean and unit variance\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(X)\n",
      "X_scaled = scaler.transform(X)\n",
      "fig, axes = plt.subplots(1, 4, figsize=(15, 3),\n",
      "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
      "# make a list of algorithms to use\n",
      "algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n",
      "              DBSCAN()]\n",
      "# create a random cluster assignment for reference\n",
      "random_state = np.random.RandomState(seed=0)\n",
      "random_clusters = random_state.randint(low=0, high=2, size=len(X))\n",
      "# plot random assignment\n",
      "axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n",
      "                cmap=mglearn.cm3, s=60)\n",
      "axes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\n",
      "        adjusted_rand_score(y, random_clusters)))\n",
      "for ax, algorithm in zip(axes[1:], algorithms):\n",
      "    # plot the cluster assignments and cluster centers\n",
      "    clusters = algorithm.fit_predict(X_scaled)\n",
      "    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters,\n",
      "               cmap=mglearn.cm3, s=60)\n",
      "    ax.set_title(\"{} - ARI: {:.2f}\".format(algorithm.__class__.__name__,\n",
      "                                           adjusted_rand_score(y, clusters)))\n",
      "Figure 3-39. Comparing random assignment, k-means, agglomerative clustering, and\n",
      "DBSCAN on the two_moons dataset using the supervised ARI score\n",
      "The adjusted rand index provides intuitive results, with a random cluster assignment\n",
      "having a score of 0 and DBSCAN (which recovers the desired clustering perfectly)\n",
      "having a score of 1.\n",
      "192 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "A common mistake when evaluating clustering in this way is to use accuracy_score\n",
      "instead of adjusted_rand_score, normalized_mutual_info_score, or some other\n",
      "clustering metric. The problem in using accuracy is that it requires the assigned clus‐\n",
      "ter labels to exactly match the ground truth. However, the cluster labels themselves\n",
      "are meaningless—the only thing that matters is which points are in the same cluster:\n",
      "In[69]:\n",
      "from sklearn.metrics import accuracy_score\n",
      "# these two labelings of points correspond to the same clustering\n",
      "clusters1 = [0, 0, 1, 1, 0]\n",
      "clusters2 = [1, 1, 0, 0, 1]\n",
      "# accuracy is zero, as none of the labels are the same\n",
      "print(\"Accuracy: {:.2f}\".format(accuracy_score(clusters1, clusters2)))\n",
      "# adjusted rand score is 1, as the clustering is exactly the same\n",
      "print(\"ARI: {:.2f}\".format(adjusted_rand_score(clusters1, clusters2)))\n",
      "Out[69]:\n",
      "Accuracy: 0.00\n",
      "ARI: 1.00\n",
      "Evaluating clustering without ground truth\n",
      "Although we have just shown one way to evaluate clustering algorithms, in practice,\n",
      "there is a big problem with using measures like ARI. When applying clustering algo‐\n",
      "rithms, there is usually no ground truth to which to compare the results. If we knew\n",
      "the right clustering of the data, we could use this information to build a supervised\n",
      "model like a classifier. Therefore, using metrics like ARI and NMI usually only helps\n",
      "in developing algorithms, not in assessing success in an application.\n",
      "There are scoring metrics for clustering that don’t require ground truth, like the sil‐\n",
      "houette coefficient. However, these often don’t work well in practice. The silhouette\n",
      "score computes the compactness of a cluster, where higher is better, with a perfect\n",
      "score of 1. While compact clusters are good, compactness doesn’t allow for complex\n",
      "shapes.\n",
      "Here is an example comparing the outcome of k-means, agglomerative clustering,\n",
      "and DBSCAN on the two-moons dataset using the silhouette score (Figure 3-40):\n",
      "In[70]:\n",
      "from sklearn.metrics.cluster import silhouette_score\n",
      "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
      "# rescale the data to zero mean and unit variance\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(X)\n",
      "X_scaled = scaler.transform(X)\n",
      "Clustering | 193\n",
      "fig, axes = plt.subplots(1, 4, figsize=(15, 3),\n",
      "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
      "# create a random cluster assignment for reference\n",
      "random_state = np.random.RandomState(seed=0)\n",
      "random_clusters = random_state.randint(low=0, high=2, size=len(X))\n",
      "# plot random assignment\n",
      "axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n",
      "    cmap=mglearn.cm3, s=60)\n",
      "axes[0].set_title(\"Random assignment: {:.2f}\".format(\n",
      "    silhouette_score(X_scaled, random_clusters)))\n",
      "algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n",
      "              DBSCAN()]\n",
      "for ax, algorithm in zip(axes[1:], algorithms):\n",
      "    clusters = algorithm.fit_predict(X_scaled)\n",
      "    # plot the cluster assignments and cluster centers\n",
      "    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm3,\n",
      "               s=60)\n",
      "    ax.set_title(\"{} : {:.2f}\".format(algorithm.__class__.__name__,\n",
      "                                      silhouette_score(X_scaled, clusters)))\n",
      "Figure 3-40. Comparing random assignment, k-means, agglomerative clustering, and\n",
      "DBSCAN on the two_moons dataset using the unsupervised silhouette score—the more\n",
      "intuitive result of DBSCAN has a lower silhouette score than the assignments found by\n",
      "k-means\n",
      "As you can see, k-means gets the highest silhouette score, even though we might pre‐\n",
      "fer the result produced by DBSCAN. A slightly better strategy for evaluating clusters\n",
      "is using robustness-based clustering metrics. These run an algorithm after adding\n",
      "some noise to the data, or using different parameter settings, and compare the out‐\n",
      "comes. The idea is that if many algorithm parameters and many perturbations of the\n",
      "data return the same result, it is likely to be trustworthy. Unfortunately, this strategy is\n",
      "not implemented in scikit-learn at the time of writing.\n",
      "Even if we get a very robust clustering, or a very high silhouette score, we still don’t\n",
      "know if there is any semantic meaning in the clustering, or whether the clustering\n",
      "194 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "reflects an aspect of the data that we are interested in. Let’s go back to the example of\n",
      "face images. We hope to find groups of similar faces—say, men and women, or old\n",
      "people and young people, or people with beards and without. Let’s say we cluster the\n",
      "data into two clusters, and all algorithms agree about which points should be clus‐\n",
      "tered together. We still don’t know if the clusters that are found correspond in any\n",
      "way to the concepts we are interested in. It could be that they found side views versus\n",
      "front views, or pictures taken at night versus pictures taken during the day, or pic‐\n",
      "tures taken with iPhones versus pictures taken with Android phones. The only way to\n",
      "know whether the clustering corresponds to anything we are interested in is to ana‐\n",
      "lyze the clusters manually.\n",
      "Comparing algorithms on the faces dataset\n",
      "Let’s apply the k-means, DBSCAN, and agglomerative clustering algorithms to the\n",
      "Labeled Faces in the Wild dataset, and see if any of them find interesting structure.\n",
      "We will use the eigenface representation of the data, as produced by\n",
      "PCA(whiten=True), with 100 components:\n",
      "In[71]:\n",
      "# extract eigenfaces from lfw data and transform data\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=100, whiten=True, random_state=0)\n",
      "pca.fit_transform(X_people)\n",
      "X_pca = pca.transform(X_people)\n",
      "We saw earlier that this is a more semantic representation of the face images than the\n",
      "raw pixels. It will also make computation faster. A good exercise would be for you to\n",
      "run the following experiments on the original data, without PCA, and see if you find\n",
      "similar clusters.\n",
      "Analyzing the faces dataset with DBSCAN.    We will start by applying DBSCAN, which we\n",
      "just discussed:\n",
      "In[72]:\n",
      "# apply DBSCAN with default parameters\n",
      "dbscan = DBSCAN()\n",
      "labels = dbscan.fit_predict(X_pca)\n",
      "print(\"Unique labels: {}\".format(np.unique(labels)))\n",
      "Out[72]:\n",
      "Unique labels: [-1]\n",
      "We see that all the returned labels are –1, so all of the data was labeled as “noise” by\n",
      "DBSCAN. There are two things we can change to help this: we can make eps higher,\n",
      "to expand the neighborhood of each point, and set min_samples lower, to consider\n",
      "smaller groups of points as clusters. Let’s try changing min_samples first:\n",
      "Clustering | 195\n",
      "In[73]:\n",
      "dbscan = DBSCAN(min_samples=3)\n",
      "labels = dbscan.fit_predict(X_pca)\n",
      "print(\"Unique labels: {}\".format(np.unique(labels)))\n",
      "Out[73]:\n",
      "Unique labels: [-1]\n",
      "Even when considering groups of three points, everything is labeled as noise. So, we\n",
      "need to increase eps:\n",
      "In[74]:\n",
      "dbscan = DBSCAN(min_samples=3, eps=15)\n",
      "labels = dbscan.fit_predict(X_pca)\n",
      "print(\"Unique labels: {}\".format(np.unique(labels)))\n",
      "Out[74]:\n",
      "Unique labels: [-1  0]\n",
      "Using a much larger eps of 15, we get only a single cluster and noise points. We can\n",
      "use this result to find out what the “noise” looks like compared to the rest of the data.\n",
      "To understand better what’s happening, let’s look at how many points are noise, and\n",
      "how many points are inside the cluster:\n",
      "In[75]:\n",
      "# Count number of points in all clusters and noise.\n",
      "# bincount doesn't allow negative numbers, so we need to add 1.\n",
      "# The first number in the result corresponds to noise points.\n",
      "print(\"Number of points per cluster: {}\".format(np.bincount(labels + 1)))\n",
      "Out[75]:\n",
      "Number of points per cluster: [  27 2036]\n",
      "There are very few noise points—only 27—so we can look at all of them (see\n",
      "Figure 3-41):\n",
      "In[76]:\n",
      "noise = X_people[labels==-1]\n",
      "fig, axes = plt.subplots(3, 9, subplot_kw={'xticks': (), 'yticks': ()},\n",
      "                         figsize=(12, 4))\n",
      "for image, ax in zip(noise, axes.ravel()):\n",
      "    ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n",
      "196 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-41. Samples from the faces dataset labeled as noise by DBSCAN\n",
      "Comparing these images to the random sample of face images from Figure 3-7, we\n",
      "can guess why they were labeled as noise: the fifth image in the first row shows a per‐\n",
      "son drinking from a glass, there are images of people wearing hats, and in the last\n",
      "image there’s a hand in front of the person’s face. The other images contain odd angles\n",
      "or crops that are too close or too wide.\n",
      "This kind of analysis—trying to find “the odd one out”—is called outlier detection. If\n",
      "this was a real application, we might try to do a better job of cropping images, to get\n",
      "more homogeneous data. There is little we can do about people in photos sometimes\n",
      "wearing hats, drinking, or holding something in front of their faces, but it’s good to\n",
      "know that these are issues in the data that any algorithm we might apply needs to\n",
      "handle.\n",
      "If we want to find more interesting clusters than just one large one, we need to set eps\n",
      "smaller, somewhere between 15 and 0.5 (the default). Let’s have a look at what differ‐\n",
      "ent values of eps result in:\n",
      "In[77]:\n",
      "for eps in [1, 3, 5, 7, 9, 11, 13]:\n",
      "    print(\"\\neps={}\".format(eps))\n",
      "    dbscan = DBSCAN(eps=eps, min_samples=3)\n",
      "    labels = dbscan.fit_predict(X_pca)\n",
      "    print(\"Clusters present: {}\".format(np.unique(labels)))\n",
      "    print(\"Cluster sizes: {}\".format(np.bincount(labels + 1)))\n",
      "Out[78]:\n",
      "eps=1\n",
      "Clusters present: [-1]\n",
      "Cluster sizes: [2063]\n",
      "eps=3\n",
      "Clusters present: [-1]\n",
      "Cluster sizes: [2063]\n",
      "Clustering | 197\n",
      "eps=5\n",
      "Clusters present: [-1]\n",
      "Cluster sizes: [2063]\n",
      "eps=7\n",
      "Clusters present: [-1  0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Cluster sizes: [2006  4  6  6  6  9  3  3  4  3  3  3  3  4]\n",
      "eps=9\n",
      "Clusters present: [-1  0  1  2]\n",
      "Cluster sizes: [1269  788    3    3]\n",
      "eps=11\n",
      "Clusters present: [-1  0]\n",
      "Cluster sizes: [ 430 1633]\n",
      "eps=13\n",
      "Clusters present: [-1  0]\n",
      "Cluster sizes: [ 112 1951]\n",
      "For low settings of eps, all points are labeled as noise. For eps=7, we get many noise\n",
      "points and many smaller clusters. For eps=9 we still get many noise points, but we get\n",
      "one big cluster and some smaller clusters. Starting from eps=11, we get only one large\n",
      "cluster and noise.\n",
      "What is interesting to note is that there is never more than one large cluster. At most,\n",
      "there is one large cluster containing most of the points, and some smaller clusters.\n",
      "This indicates that there are not two or three different kinds of face images in the data\n",
      "that are very distinct, but rather that all images are more or less equally similar to (or\n",
      "dissimilar from) the rest.\n",
      "The results for eps=7 look most interesting, with many small clusters. We can investi‐\n",
      "gate this clustering in more detail by visualizing all of the points in each of the 13\n",
      "small clusters (Figure 3-42):\n",
      "In[78]:\n",
      "dbscan = DBSCAN(min_samples=3, eps=7)\n",
      "labels = dbscan.fit_predict(X_pca)\n",
      "for cluster in range(max(labels) + 1):\n",
      "    mask = labels == cluster\n",
      "    n_images =  np.sum(mask)\n",
      "    fig, axes = plt.subplots(1, n_images, figsize=(n_images * 1.5, 4),\n",
      "                             subplot_kw={'xticks': (), 'yticks': ()})\n",
      "    for image, label, ax in zip(X_people[mask], y_people[mask], axes):\n",
      "        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n",
      "        ax.set_title(people.target_names[label].split()[-1])\n",
      "198 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-42. Clusters found by DBSCAN with eps=7\n",
      "Some of the clusters correspond to people with very distinct faces (within this data‐\n",
      "set), such as Sharon or Koizumi. Within each cluster, the orientation of the face is also\n",
      "Clustering | 199\n",
      "quite fixed, as well as the facial expression. Some of the clusters contain faces of mul‐\n",
      "tiple people, but they share a similar orientation and expression.\n",
      "This concludes our analysis of the DBSCAN algorithm applied to the faces dataset. As\n",
      "you can see, we are doing a manual analysis here, different from the much more auto‐\n",
      "matic search approach we could use for supervised learning based on R2 score or\n",
      "accuracy.\n",
      "Let’s move on to applying k-means and agglomerative clustering.\n",
      "Analyzing the faces dataset with k-means.    We saw that it was not possible to create\n",
      "more than one big cluster using DBSCAN. Agglomerative clustering and k-means are\n",
      "much more likely to create clusters of even size, but we do need to set a target num‐\n",
      "ber of clusters. We could set the number of clusters to the known number of people in\n",
      "the dataset, though it is very unlikely that an unsupervised clustering algorithm will\n",
      "recover them. Instead, we can start with a low number of clusters, like 10, which\n",
      "might allow us to analyze each of the clusters:\n",
      "In[79]:\n",
      "# extract clusters with k-means\n",
      "km = KMeans(n_clusters=10, random_state=0)\n",
      "labels_km = km.fit_predict(X_pca)\n",
      "print(\"Cluster sizes k-means: {}\".format(np.bincount(labels_km)))\n",
      "Out[79]:\n",
      "Cluster sizes k-means: [269 128 170 186 386 222 237  64 253 148]\n",
      "As you can see, k-means clustering partitioned the data into relatively similarly sized\n",
      "clusters from 64 to 386. This is quite different from the result of DBSCAN.\n",
      "We can further analyze the outcome of k-means by visualizing the cluster centers\n",
      "(Figure 3-43 ). As we clustered in the representation produced by PCA, we need to\n",
      "rotate the cluster centers back into the original space to visualize them, using\n",
      "pca.inverse_transform:\n",
      "In[80]:\n",
      "fig, axes = plt.subplots(2, 5, subplot_kw={'xticks': (), 'yticks': ()},\n",
      "                         figsize=(12, 4))\n",
      "for center, ax in zip(km.cluster_centers_, axes.ravel()):\n",
      "    ax.imshow(pca.inverse_transform(center).reshape(image_shape),\n",
      "              vmin=0, vmax=1)\n",
      "200 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-43. Cluster centers found by k-means when setting the number of clusters to 10\n",
      "The cluster centers found by k-means are very smooth versions of faces. This is not\n",
      "very surprising, given that each center is an average of 64 to 386 face images. Working\n",
      "with a reduced PCA representation adds to the smoothness of the images (compared\n",
      "to the faces reconstructed using 100 PCA dimensions in Figure 3-11). The clustering\n",
      "seems to pick up on different orientations of the face, different expressions (the third\n",
      "cluster center seems to show a smiling face), and the presence of shirt collars (see the\n",
      "second-to-last cluster center).\n",
      "For a more detailed view, in Figure 3-44 we show for each cluster center the five most\n",
      "typical images in the cluster (the images assigned to the cluster that are closest to the\n",
      "cluster center) and the five most atypical images in the cluster (the images assigned to\n",
      "the cluster that are furthest from the cluster center):\n",
      "In[81]:\n",
      "mglearn.plots.plot_kmeans_faces(km, pca, X_pca, X_people,\n",
      "                                y_people, people.target_names)\n",
      "Clustering | 201\n",
      "Figure 3-44. Sample images for each cluster found by k-means—the cluster centers are\n",
      "on the left, followed by the five closest points to each center and the five points that are\n",
      "assigned to the cluster but are furthest away from the center\n",
      "202 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-44 confirms our intuition about smiling faces for the third cluster, and also\n",
      "the importance of orientation for the other clusters. The “atypical” points are not very\n",
      "similar to the cluster centers, though, and their assignment seems somewhat arbi‐\n",
      "trary. This can be attributed to the fact that k-means partitions all the data points and\n",
      "doesn’t have a concept of “noise” points, as DBSCAN does. Using a larger number of\n",
      "clusters, the algorithm could find finer distinctions. However, adding more clusters\n",
      "makes manual inspection even harder.\n",
      "Analyzing the faces dataset with agglomerative clustering.    Now, let’s look at the results of\n",
      "agglomerative clustering:\n",
      "In[82]:\n",
      "# extract clusters with ward agglomerative clustering\n",
      "agglomerative = AgglomerativeClustering(n_clusters=10)\n",
      "labels_agg = agglomerative.fit_predict(X_pca)\n",
      "print(\"Cluster sizes agglomerative clustering: {}\".format(\n",
      "    np.bincount(labels_agg)))\n",
      "Out[82]:\n",
      "Cluster sizes agglomerative clustering: [255 623  86 102 122 199 265  26 230 155]\n",
      "Agglomerative clustering also produces relatively equally sized clusters, with cluster\n",
      "sizes between 26 and 623. These are more uneven than those produced by k-means,\n",
      "but much more even than the ones produced by DBSCAN.\n",
      "We can compute the ARI to measure whether the two partitions of the data given by\n",
      "agglomerative clustering and k-means are similar:\n",
      "In[83]:\n",
      "print(\"ARI: {:.2f}\".format(adjusted_rand_score(labels_agg, labels_km)))\n",
      "Out[83]:\n",
      "ARI: 0.13\n",
      "An ARI of only 0.13 means that the two clusterings labels_agg and labels_km have\n",
      "little in common. This is not very surprising, given the fact that points further away\n",
      "from the cluster centers seem to have little in common for k-means.\n",
      "Next, we might want to plot the dendrogram ( Figure 3-45). We’ll limit the depth of\n",
      "the tree in the plot, as branching down to the individual 2,063 data points would\n",
      "result in an unreadably dense plot:\n",
      "Clustering | 203\n",
      "In[84]:\n",
      "linkage_array = ward(X_pca)\n",
      "# now we plot the dendrogram for the linkage_array\n",
      "# containing the distances between clusters\n",
      "plt.figure(figsize=(20, 5))\n",
      "dendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)\n",
      "plt.xlabel(\"Sample index\")\n",
      "plt.ylabel(\"Cluster distance\")\n",
      "Figure 3-45. Dendrogram of agglomerative clustering on the faces dataset\n",
      "Creating 10 clusters, we cut across the tree at the very top, where there are 10 vertical\n",
      "lines. In the dendrogram for the toy data shown in Figure 3-36, you could see by the\n",
      "length of the branches that two or three clusters might capture the data appropriately.\n",
      "For the faces data, there doesn’t seem to be a very natural cutoff point. There are\n",
      "some branches that represent more distinct groups, but there doesn’t appear to be a\n",
      "particular number of clusters that is a good fit. This is not surprising, given the results\n",
      "of DBSCAN, which tried to cluster all points together.\n",
      "Let’s visualize the 10 clusters, as we did for k-means earlier ( Figure 3-46). Note that\n",
      "there is no notion of cluster center in agglomerative clustering (though we could\n",
      "compute the mean), and we simply show the first couple of points in each cluster. We\n",
      "show the number of points in each cluster to the left of the first image:\n",
      "In[85]:\n",
      "n_clusters = 10\n",
      "for cluster in range(n_clusters):\n",
      "    mask = labels_agg == cluster\n",
      "    fig, axes = plt.subplots(1, 10, subplot_kw={'xticks': (), 'yticks': ()},\n",
      "                             figsize=(15, 8))\n",
      "    axes[0].set_ylabel(np.sum(mask))\n",
      "    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n",
      "                                      labels_agg[mask], axes):\n",
      "        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n",
      "        ax.set_title(people.target_names[label].split()[-1],\n",
      "                     fontdict={'fontsize': 9})\n",
      "204 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-46. Random images from the clusters generated by In[82]—each row corre‐\n",
      "sponds to one cluster; the number to the left lists the number of images in each cluster\n",
      "Clustering | 205\n",
      "While some of the clusters seem to have a semantic theme, many of them are too\n",
      "large to be actually homogeneous. To get more homogeneous clusters, we can run the\n",
      "algorithm again, this time with 40 clusters, and pick out some of the clusters that are\n",
      "particularly interesting (Figure 3-47):\n",
      "In[86]:\n",
      "# extract clusters with ward agglomerative clustering\n",
      "agglomerative = AgglomerativeClustering(n_clusters=40)\n",
      "labels_agg = agglomerative.fit_predict(X_pca)\n",
      "print(\"cluster sizes agglomerative clustering: {}\".format(np.bincount(labels_agg)))\n",
      "n_clusters = 40\n",
      "for cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters\n",
      "    mask = labels_agg == cluster\n",
      "    fig, axes = plt.subplots(1, 15, subplot_kw={'xticks': (), 'yticks': ()},\n",
      "                             figsize=(15, 8))\n",
      "    cluster_size = np.sum(mask)\n",
      "    axes[0].set_ylabel(\"#{}: {}\".format(cluster, cluster_size))\n",
      "    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n",
      "                                      labels_agg[mask], axes):\n",
      "        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n",
      "        ax.set_title(people.target_names[label].split()[-1],\n",
      "                     fontdict={'fontsize': 9})\n",
      "    for i in range(cluster_size, 15):\n",
      "        axes[i].set_visible(False)\n",
      "Out[86]:\n",
      "cluster sizes agglomerative clustering:\n",
      " [ 58  80  79  40 222  50  55  78 172  28  26  34  14  11  60  66 152  27\n",
      "  47  31  54   5   8  56   3   5   8  18  22  82  37  89  28  24  41  40\n",
      "  21  10 113  69]\n",
      "206 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Figure 3-47. Images from selected clusters found by agglomerative clustering when set‐\n",
      "ting the number of clusters to 40—the text to the left shows the index of the cluster and\n",
      "the total number of points in the cluster\n",
      "Here, the clustering seems to have picked up on “dark skinned and smiling, ” “collared\n",
      "shirt, ” “smiling woman, ” “Hussein, ” and “high forehead. ” We could also find these\n",
      "highly similar clusters using the dendrogram, if we did more a detailed analysis.\n",
      "Summary of Clustering Methods\n",
      "This section has shown that applying and evaluating clustering is a highly qualitative\n",
      "procedure, and often most helpful in the exploratory phase of data analysis. We\n",
      "looked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\n",
      "ing. All three have a way of controlling the granularity of clustering. k-means and\n",
      "agglomerative clustering allow you to specify the number of desired clusters, while\n",
      "DBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\n",
      "ences cluster size. All three methods can be used on large, real-world datasets, are rel‐\n",
      "atively easy to understand, and allow for clustering into many clusters.\n",
      "Each of the algorithms has somewhat different strengths. k-means allows for a char‐\n",
      "acterization of the clusters using the cluster means. It can also be viewed as a decom‐\n",
      "position method, where each data point is represented by its cluster center. DBSCAN\n",
      "allows for the detection of “noise points” that are not assigned any cluster, and it can\n",
      "help automatically determine the number of clusters. In contrast to the other two\n",
      "methods, it allow for complex cluster shapes, as we saw in the two_moons example.\n",
      "DBSCAN sometimes produces clusters of very differing size, which can be a strength\n",
      "or a weakness. Agglomerative clustering can provide a whole hierarchy of possible\n",
      "partitions of the data, which can be easily inspected via dendrograms.\n",
      "Clustering | 207\n",
      "Summary and Outlook\n",
      "This chapter introduced a range of unsupervised learning algorithms that can be\n",
      "applied for exploratory data analysis and preprocessing. Having the right representa‐\n",
      "tion of the data is often crucial for supervised or unsupervised learning to succeed,\n",
      "and preprocessing and decomposition methods play an important part in data prepa‐\n",
      "ration.\n",
      "Decomposition, manifold learning, and clustering are essential tools to further your\n",
      "understanding of your data, and can be the only ways to make sense of your data in\n",
      "the absence of supervision information. Even in a supervised setting, exploratory\n",
      "tools are important for a better understanding of the properties of the data. Often it is\n",
      "hard to quantify the usefulness of an unsupervised algorithm, though this shouldn’t\n",
      "deter you from using them to gather insights from your data. With these methods\n",
      "under your belt, you are now equipped with all the essential learning algorithms that\n",
      "machine learning practitioners use every day.\n",
      "We encourage you to try clustering and decomposition methods both on two-\n",
      "dimensional toy data and on real-world datasets included in scikit-learn, like the\n",
      "digits, iris, and cancer datasets.\n",
      "208 | Chapter 3: Unsupervised Learning and Preprocessing\n",
      "Summary of the Estimator Interface\n",
      "Let’s briefly review the API that we introduced in Chapters 2 and 3. All algorithms in\n",
      "scikit-learn, whether preprocessing, supervised learning, or unsupervised learning\n",
      "algorithms, are implemented as classes. These classes are called estimators in scikit-\n",
      "learn. To apply an algorithm, you first have to instantiate an object of the particular\n",
      "class:\n",
      "In[87]:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "logreg = LogisticRegression()\n",
      "The estimator class contains the algorithm, and also stores the model that is learned\n",
      "from data using the algorithm.\n",
      "Y ou should set any parameters of the model when constructing the model object.\n",
      "These parameters include regularization, complexity control, number of clusters to\n",
      "find, etc. All estimators have a fit method, which is used to build the model. The fit\n",
      "method always requires as its first argument the data X, represented as a NumPy array\n",
      "or a SciPy sparse matrix, where each row represents a single data point. The data X is\n",
      "always assumed to be a NumPy array or SciPy sparse matrix that has continuous\n",
      "(floating-point) entries. Supervised algorithms also require a y argument, which is a\n",
      "one-dimensional NumPy array containing target values for regression or classifica‐\n",
      "tion (i.e., the known output labels or responses).\n",
      "There are two main ways to apply a learned model in scikit-learn. To create a pre‐\n",
      "diction in the form of a new output like y, you use the predict method. To create a\n",
      "new representation of the input data X, you use the transform method. Table 3-1\n",
      "summarizes the use cases of the predict and transform methods.\n",
      "Table 3-1. scikit-learn API summary\n",
      "estimator.fit(x_train, [y_train])\n",
      "estimator.predict(X_text) estimator.transform(X_test)\n",
      "Classification Preprocessing\n",
      "Regression Dimensionality reduction\n",
      "Clustering Feature extraction\n",
      " Feature selection\n",
      "Additionally, all supervised models have a score(X_test, y_test)  method that\n",
      "allows an evaluation of the model. In Table 3-1, X_train and y_train refer to the\n",
      "training data and training labels, while X_test and y_test refer to the test data and\n",
      "test labels (if applicable).\n",
      "Summary and Outlook | 209\n",
      "\n",
      "CHAPTER 4\n",
      "Representing Data and\n",
      "Engineering Features\n",
      "So far, we’ve assumed that our data comes in as a two-dimensional array of floating-\n",
      "point numbers, where each column is a continuous feature that describes the data\n",
      "points. For many applications, this is not how the data is collected. A particularly\n",
      "common type of feature is the categorical features. Also known as discrete features,\n",
      "these are usually not numeric. The distinction between categorical features and con‐\n",
      "tinuous features is analogous to the distinction between classification and regression,\n",
      "only on the input side rather than the output side. Examples of continuous features\n",
      "that we have seen are pixel brightnesses and size measurements of plant flowers.\n",
      "Examples of categorical features are the brand of a product, the color of a product, or\n",
      "the department (books, clothing, hardware) it is sold in. These are all properties that\n",
      "can describe a product, but they don’t vary in a continuous way. A product belongs\n",
      "either in the clothing department or in the books department. There is no middle\n",
      "ground between books and clothing, and no natural order for the different categories\n",
      "(books is not greater or less than clothing, hardware is not between books and cloth‐\n",
      "ing, etc.).\n",
      "Regardless of the types of features your data consists of, how you represent them can\n",
      "have an enormous effect on the performance of machine learning models. We saw in\n",
      "Chapters 2 and 3 that scaling of the data is important. In other words, if you don’t\n",
      "rescale your data (say, to unit variance), then it makes a difference whether you repre‐\n",
      "sent a measurement in centimeters or inches. We also saw in Chapter 2 that it can be\n",
      "helpful to augment your data with additional features, like adding interactions (prod‐\n",
      "ucts) of features or more general polynomials.\n",
      "The question of how to represent your data best for a particular application is known\n",
      "as feature engineering, and it is one of the main tasks of data scientists and machine\n",
      "211\n",
      "learning practitioners trying to solve real-world problems. Representing your data in\n",
      "the right way can have a bigger influence on the performance of a supervised model\n",
      "than the exact parameters you choose.\n",
      "In this chapter, we will first go over the important and very common case of categori‐\n",
      "cal features, and then give some examples of helpful transformations for specific\n",
      "combinations of features and models.\n",
      "Categorical Variables\n",
      "As an example, we will use the dataset of adult incomes in the United States, derived\n",
      "from the 1994 census database. The task of the adult dataset is to predict whether a\n",
      "worker has an income of over $50,000 or under $50,000. The features in this dataset\n",
      "include the workers’ ages, how they are employed (self employed, private industry\n",
      "employee, government employee, etc.), their education, their gender, their working\n",
      "hours per week, occupation, and more. Table 4-1 shows the first few entries in the\n",
      "dataset.\n",
      "Table 4-1. The first few entries in the adult dataset\n",
      "age workclass education gender hours-per-week occupation income\n",
      "0 39 State-gov Bachelors Male 40 Adm-clerical <=50K\n",
      "1 50 Self-emp-not-inc Bachelors Male 13 Exec-managerial <=50K\n",
      "2 38 Private HS-grad Male 40 Handlers-cleaners <=50K\n",
      "3 53 Private 11th Male 40 Handlers-cleaners <=50K\n",
      "4 28 Private Bachelors Female 40 Prof-specialty <=50K\n",
      "5 37 Private Masters Female 40 Exec-managerial <=50K\n",
      "6 49 Private 9th Female 16 Other-service <=50K\n",
      "7 52 Self-emp-not-inc HS-grad Male 45 Exec-managerial >50K\n",
      "8 31 Private Masters Female 50 Prof-specialty >50K\n",
      "9 42 Private Bachelors Male 40 Exec-managerial >50K\n",
      "10 37 Private Some-college Male 80 Exec-managerial >50K\n",
      "The task is phrased as a classification task with the two classes being income <=50k\n",
      "and >50k. It would also be possible to predict the exact income, and make this a\n",
      "regression task. However, that would be much more difficult, and the 50K division is\n",
      "interesting to understand on its own.\n",
      "In this dataset, age and hours-per-week are continuous features, which we know\n",
      "how to treat. The workclass, education, sex, and occupation features are categori‐\n",
      "cal, however. All of them come from a fixed list of possible values, as opposed to a\n",
      "range, and denote a qualitative property, as opposed to a quantity.\n",
      "212 | Chapter 4: Representing Data and Engineering Features\n",
      "As a starting point, let’s say we want to learn a logistic regression classifier on this\n",
      "data. We know from Chapter 2 that a logistic regression makes predictions, ŷ, using\n",
      "the following formula:\n",
      "ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\n",
      "where w[i] and b are coefficients learned from the training set and x[i] are the input\n",
      "features. This formula makes sense when x[i] are numbers, but not when x[2] is\n",
      "\"Masters\" or \"Bachelors\". Clearly we need to represent our data in some different\n",
      "way when applying logistic regression. The next section will explain how we can\n",
      "overcome this problem.\n",
      "One-Hot-Encoding (Dummy Variables)\n",
      "By far the most common way to represent categorical variables is using the one-hot-\n",
      "encoding or one-out-of-N encoding, also known as dummy variables. The idea behind\n",
      "dummy variables is to replace a categorical variable with one or more new features\n",
      "that can have the values 0 and 1. The values 0 and 1 make sense in the formula for\n",
      "linear binary classification (and for all other models in scikit-learn), and we can\n",
      "represent any number of categories by introducing one new feature per category, as\n",
      "described here.\n",
      "Let’s say for the workclass feature we have possible values of \"Government\n",
      "Employee\", \"Private Employee\", \"Self Employed\", and \"Self Employed Incorpo\n",
      "rated\". To encode these four possible values, we create four new features, called \"Gov\n",
      "ernment Employee\", \"Private Employee\", \"Self Employed\", and \"Self Employed\n",
      "Incorporated\". A feature is 1 if workclass for this person has the corresponding\n",
      "value and 0 otherwise, so exactly one of the four new features will be 1 for each data\n",
      "point. This is why this is called one-hot or one-out-of-N encoding.\n",
      "The principle is illustrated in Table 4-2. A single feature is encoded using four new\n",
      "features. When using this data in a machine learning algorithm, we would drop the\n",
      "original workclass feature and only keep the 0–1 features.\n",
      "Table 4-2. Encoding the workclass feature using one-hot encoding\n",
      "workclass Government Employee Private Employee Self Employed Self Employed Incorporated\n",
      "Government Employee 1 0 0 0\n",
      "Private Employee 0 1 0 0\n",
      "Self Employed 0 0 1 0\n",
      "Self Employed Incorporated 0 0 0 1\n",
      "Categorical Variables | 213\n",
      "The one-hot encoding we use is quite similar, but not identical, to\n",
      "the dummy encoding used in statistics. For simplicity, we encode\n",
      "each category with a different binary feature. In statistics, it is com‐\n",
      "mon to encode a categorical feature with k different possible values\n",
      "into k–1 features (the last one is represented as all zeros). This is\n",
      "done to simplify the analysis (more technically, this will avoid mak‐\n",
      "ing the data matrix rank-deficient).\n",
      "There are two ways to convert your data to a one-hot encoding of categorical vari‐\n",
      "ables, using either pandas or scikit-learn. At the time of writing, using pandas is\n",
      "slightly easier, so let’s go this route. First we load the data using pandas from a\n",
      "comma-separated values (CSV) file:\n",
      "In[2]:\n",
      "import pandas as pd\n",
      "# The file has no headers naming the columns, so we pass header=None\n",
      "# and provide the column names explicitly in \"names\"\n",
      "data = pd.read_csv(\n",
      "    \"/home/andy/datasets/adult.data\", header=None, index_col=False,\n",
      "    names=['age', 'workclass', 'fnlwgt', 'education',  'education-num',\n",
      "           'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
      "           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
      "           'income'])\n",
      "# For illustration purposes, we only select some of the columns\n",
      "data = data[['age', 'workclass', 'education', 'gender', 'hours-per-week',\n",
      "             'occupation', 'income']]\n",
      "# IPython.display allows nice output formatting within the Jupyter notebook\n",
      "display(data.head())\n",
      "Table 4-3 shows the result.\n",
      "Table 4-3. The first five rows of the adult dataset\n",
      "age workclass education gender hours-per-week occupation income\n",
      "0 39 State-gov Bachelors Male 40 Adm-clerical <=50K\n",
      "1 50 Self-emp-not-inc Bachelors Male 13 Exec-managerial <=50K\n",
      "2 38 Private HS-grad Male 40 Handlers-cleaners <=50K\n",
      "3 53 Private 11th Male 40 Handlers-cleaners <=50K\n",
      "4 28 Private Bachelors Female 40 Prof-specialty <=50K\n",
      "Checking string-encoded categorical data\n",
      "After reading a dataset like this, it is often good to first check if a column actually\n",
      "contains meaningful categorical data. When working with data that was input by\n",
      "humans (say, users on a website), there might not be a fixed set of categories, and dif‐\n",
      "ferences in spelling and capitalization might require preprocessing. For example, it\n",
      "might be that some people specified gender as “male” and some as “man, ” and we\n",
      "214 | Chapter 4: Representing Data and Engineering Features\n",
      "might want to represent these two inputs using the same category. A good way to\n",
      "check the contents of a column is using the value_counts function of a pandas\n",
      "Series (the type of a single column in a DataFrame), to show us what the unique val‐\n",
      "ues are and how often they appear:\n",
      "In[3]:\n",
      "print(data.gender.value_counts())\n",
      "Out[3]:\n",
      " Male      21790\n",
      " Female    10771\n",
      "Name: gender, dtype: int64\n",
      "We can see that there are exactly two values for gender in this dataset, Male and\n",
      "Female, meaning the data is already in a good format to be represented using one-\n",
      "hot-encoding. In a real application, you should look at all columns and check their\n",
      "values. We will skip this here for brevity’s sake.\n",
      "There is a very simple way to encode the data in pandas, using the get_dummies func‐\n",
      "tion. The get_dummies function automatically transforms all columns that have\n",
      "object type (like strings) or are categorical (which is a special pandas concept that we\n",
      "haven’t talked about yet):\n",
      "In[4]:\n",
      "print(\"Original features:\\n\", list(data.columns), \"\\n\")\n",
      "data_dummies = pd.get_dummies(data)\n",
      "print(\"Features after get_dummies:\\n\", list(data_dummies.columns))\n",
      "Out[4]:\n",
      "Original features:\n",
      " ['age', 'workclass', 'education', 'gender', 'hours-per-week', 'occupation',\n",
      "  'income']\n",
      "Features after get_dummies:\n",
      " ['age', 'hours-per-week', 'workclass_ ?', 'workclass_ Federal-gov',\n",
      "  'workclass_ Local-gov', 'workclass_ Never-worked', 'workclass_ Private',\n",
      "  'workclass_ Self-emp-inc', 'workclass_ Self-emp-not-inc',\n",
      "  'workclass_ State-gov', 'workclass_ Without-pay', 'education_ 10th',\n",
      "  'education_ 11th', 'education_ 12th', 'education_ 1st-4th',\n",
      "   ...\n",
      "  'education_ Preschool', 'education_ Prof-school', 'education_ Some-college',\n",
      "  'gender_ Female', 'gender_ Male', 'occupation_ ?',\n",
      "  'occupation_ Adm-clerical', 'occupation_ Armed-Forces',\n",
      "  'occupation_ Craft-repair', 'occupation_ Exec-managerial',\n",
      "  'occupation_ Farming-fishing', 'occupation_ Handlers-cleaners',\n",
      "  ...\n",
      "  'occupation_ Tech-support', 'occupation_ Transport-moving',\n",
      "  'income_ <=50K', 'income_ >50K']\n",
      "Categorical Variables | 215\n",
      "Y ou can see that the continuous features age and hours-per-week were not touched,\n",
      "while the categorical features were expanded into one new feature for each possible\n",
      "value:\n",
      "In[5]:\n",
      "data_dummies.head()\n",
      "Out[5]:\n",
      "age hours-\n",
      "per-\n",
      "week\n",
      "workclass_ ? workclass_\n",
      "Federal-\n",
      "gov\n",
      "workclass_\n",
      "Local-gov\n",
      "… occupation_\n",
      "Tech-\n",
      "support\n",
      "occupation_\n",
      "Transport-\n",
      "moving\n",
      "income_\n",
      "<=50K\n",
      "income_\n",
      ">50K\n",
      "0 39 40 0.0 0.0 0.0 … 0.0 0.0 1.0 0.0\n",
      "1 50 13 0.0 0.0 0.0 … 0.0 0.0 1.0 0.0\n",
      "2 38 40 0.0 0.0 0.0 … 0.0 0.0 1.0 0.0\n",
      "3 53 40 0.0 0.0 0.0 … 0.0 0.0 1.0 0.0\n",
      "4 28 40 0.0 0.0 0.0 … 0.0 0.0 1.0 0.0\n",
      "5 rows × 46 columns\n",
      "We can now use the values attribute to convert the data_dummies DataFrame into a\n",
      "NumPy array, and then train a machine learning model on it. Be careful to separate\n",
      "the target variable (which is now encoded in two income columns) from the data\n",
      "before training a model. Including the output variable, or some derived property of\n",
      "the output variable, into the feature representation is a very common mistake in\n",
      "building supervised machine learning models.\n",
      "Be careful: column indexing in pandas includes the end of the\n",
      "range, so 'age':'occupation_ Transport-moving' is inclusive of\n",
      "occupation_ Transport-moving. This is different from slicing a\n",
      "NumPy array, where the end of a range is not included: for exam‐\n",
      "ple, np.arange(11)[0:10] doesn’t include the entry with index 10.\n",
      "In this case, we extract only the columns containing features—that is, all columns\n",
      "from age to occupation_ Transport-moving. This range contains all the features but\n",
      "not the target:\n",
      "In[6]:\n",
      "features = data_dummies.ix[:, 'age':'occupation_ Transport-moving']\n",
      "# Extract NumPy arrays\n",
      "X = features.values\n",
      "y = data_dummies['income_ >50K'].values\n",
      "print(\"X.shape: {}  y.shape: {}\".format(X.shape, y.shape))\n",
      "216 | Chapter 4: Representing Data and Engineering Features\n",
      "Out[6]:\n",
      "X.shape: (32561, 44)  y.shape: (32561,)\n",
      "Now the data is represented in a way that scikit-learn can work with, and we can\n",
      "proceed as usual:\n",
      "In[7]:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
      "logreg = LogisticRegression()\n",
      "logreg.fit(X_train, y_train)\n",
      "print(\"Test score: {:.2f}\".format(logreg.score(X_test, y_test)))\n",
      "Out[7]:\n",
      "Test score: 0.81\n",
      "In this example, we called get_dummies on a DataFrame containing\n",
      "both the training and the test data. This is important to ensure cat‐\n",
      "egorical values are represented in the same way in the training set\n",
      "and the test set.\n",
      "Imagine we have the training and test sets in two different Data\n",
      "Frames. If the \"Private Employee\" value for the workclass feature\n",
      "does not appear in the test set, pandas will assume there are only\n",
      "three possible values for this feature and will create only three new\n",
      "dummy features. Now our training and test sets have different\n",
      "numbers of features, and we can’t apply the model we learned on\n",
      "the training set to the test set anymore. Even worse, imagine the\n",
      "workclass feature has the values \"Government Employee\"  and\n",
      "\"Private Employee\" in the training set, and \"Self Employed\" and\n",
      "\"Self Employed Incorporated\"  in the test set. In both cases,\n",
      "pandas will create two new dummy features, so the encoded Data\n",
      "Frames will have the same number of features. However, the two\n",
      "dummy features have entirely different meanings in the training\n",
      "and test sets. The column that means \"Government Employee\" for\n",
      "the training set would encode \"Self Employed\" for the test set.\n",
      "If we built a machine learning model on this data it would work\n",
      "very badly, because it would assume the columns mean the same\n",
      "things (because they are in the same position) when in fact they\n",
      "mean very different things. To fix this, either call get_dummies on a\n",
      "DataFrame that contains both the training and the test data points,\n",
      "or make sure that the column names are the same for the training\n",
      "and test sets after calling get_dummies, to ensure they have the\n",
      "same semantics.\n",
      "Categorical Variables | 217\n",
      "Numbers Can Encode Categoricals\n",
      "In the example of the adult dataset, the categorical variables were encoded as strings.\n",
      "On the one hand, that opens up the possibility of spelling errors, but on the other\n",
      "hand, it clearly marks a variable as categorical. Often, whether for ease of storage or\n",
      "because of the way the data is collected, categorical variables are encoded as integers.\n",
      "For example, imagine the census data in the adult dataset was collected using a ques‐\n",
      "tionnaire, and the answers for workclass were recorded as 0 (first box ticked), 1 (sec‐\n",
      "ond box ticked), 2 (third box ticked), and so on. Now the column will contain\n",
      "numbers from 0 to 8, instead of strings like \"Private\", and it won’t be immediately\n",
      "obvious to someone looking at the table representing the dataset whether they should\n",
      "treat this variable as continuous or categorical. Knowing that the numbers indicate\n",
      "employment status, however, it is clear that these are very distinct states and should\n",
      "not be modeled by a single continuous variable.\n",
      "Categorical features are often encoded using integers. That they are\n",
      "numbers doesn’t mean that they should necessarily be treated as\n",
      "continuous features. It is not always clear whether an integer fea‐\n",
      "ture should be treated as continuous or discrete (and one-hot-\n",
      "encoded). If there is no ordering between the semantics that are\n",
      "encoded (like in the workclass example), the feature must be\n",
      "treated as discrete. For other cases, like five-star ratings, the better\n",
      "encoding depends on the particular task and data and which\n",
      "machine learning algorithm is used.\n",
      "The get_dummies function in pandas treats all numbers as continuous and will not\n",
      "create dummy variables for them. To get around this, you can either use scikit-\n",
      "learn’s OneHotEncoder, for which you can specify which variables are continuous\n",
      "and which are discrete, or convert numeric columns in the DataFrame to strings. To\n",
      "illustrate, let’s create a DataFrame object with two columns, one containing strings\n",
      "and one containing integers:\n",
      "In[8]:\n",
      "# create a DataFrame with an integer feature and a categorical string feature\n",
      "demo_df = pd.DataFrame({'Integer Feature': [0, 1, 2, 1],\n",
      "                        'Categorical Feature': ['socks', 'fox', 'socks', 'box']})\n",
      "display(demo_df)\n",
      "Table 4-4 shows the result.\n",
      "218 | Chapter 4: Representing Data and Engineering Features\n",
      "Table 4-4. DataFrame containing categorical string features and integer features\n",
      "Categorical Feature Integer Feature\n",
      "0 socks 0\n",
      "1 fox 1\n",
      "2 socks 2\n",
      "3 box 1\n",
      "Using get_dummies will only encode the string feature and will not change the integer\n",
      "feature, as you can see in Table 4-5:\n",
      "In[9]:\n",
      "pd.get_dummies(demo_df)\n",
      "Table 4-5. One-hot-encoded version of the data from Table 4-4, leaving the integer feature\n",
      "unchanged\n",
      "Integer Feature Categorical Feature_box Categorical Feature_fox Categorical Feature_socks\n",
      "0 0 0.0 0.0 1.0\n",
      "1 1 0.0 1.0 0.0\n",
      "2 2 0.0 0.0 1.0\n",
      "3 1 1.0 0.0 0.0\n",
      "If you want dummy variables to be created for the “Integer Feature” column, you can\n",
      "explicitly list the columns you want to encode using the columns parameter. Then,\n",
      "both features will be treated as categorical (see Table 4-6):\n",
      "In[10]:\n",
      "demo_df['Integer Feature'] = demo_df['Integer Feature'].astype(str)\n",
      "pd.get_dummies(demo_df, columns=['Integer Feature', 'Categorical Feature'])\n",
      "Table 4-6. One-hot encoding of the data shown in Table 4-4, encoding the integer and string\n",
      "features\n",
      "Integer\n",
      "Feature_0\n",
      "Integer\n",
      "Feature_1\n",
      "Integer\n",
      "Feature_2\n",
      "Categorical\n",
      "Feature_box\n",
      "Categorical\n",
      "Feature_fox\n",
      "Categorical\n",
      "Feature_socks\n",
      "0 1.0 0.0 0.0 0.0 0.0 1.0\n",
      "1 0.0 1.0 0.0 0.0 1.0 0.0\n",
      "2 0.0 0.0 1.0 0.0 0.0 1.0\n",
      "3 0.0 1.0 0.0 1.0 0.0 0.0\n",
      "Categorical Variables | 219\n",
      "Binning, Discretization, Linear Models, and Trees\n",
      "The best way to represent data depends not only on the semantics of the data, but also\n",
      "on the kind of model you are using. Linear models and tree-based models (such as\n",
      "decision trees, gradient boosted trees, and random forests), two large and very com‐\n",
      "monly used families, have very different properties when it comes to how they work\n",
      "with different feature representations. Let’s go back to the wave regression dataset that\n",
      "we used in Chapter 2. It has only a single input feature. Here is a comparison of a\n",
      "linear regression model and a decision tree regressor on this dataset (see Figure 4-1):\n",
      "In[11]:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "X, y = mglearn.datasets.make_wave(n_samples=100)\n",
      "line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)\n",
      "reg = DecisionTreeRegressor(min_samples_split=3).fit(X, y)\n",
      "plt.plot(line, reg.predict(line), label=\"decision tree\")\n",
      "reg = LinearRegression().fit(X, y)\n",
      "plt.plot(line, reg.predict(line), label=\"linear regression\")\n",
      "plt.plot(X[:, 0], y, 'o', c='k')\n",
      "plt.ylabel(\"Regression output\")\n",
      "plt.xlabel(\"Input feature\")\n",
      "plt.legend(loc=\"best\")\n",
      "As you know, linear models can only model linear relationships, which are lines in\n",
      "the case of a single feature. The decision tree can build a much more complex model\n",
      "of the data. However, this is strongly dependent on the representation of the data.\n",
      "One way to make linear models more powerful on continuous data is to use binning\n",
      "(also known as discretization) of the feature to split it up into multiple features, as\n",
      "described here.\n",
      "220 | Chapter 4: Representing Data and Engineering Features\n",
      "Figure 4-1. Comparing linear regression and a decision tree on the wave dataset\n",
      "We imagine a partition of the input range for the feature (in this case, the numbers\n",
      "from –3 to 3) into a fixed number of bins—say, 10. A data point will then be repre‐\n",
      "sented by which bin it falls into. To determine this, we first have to define the bins. In\n",
      "this case, we’ll define 10 bins equally spaced between –3 and 3. We use the\n",
      "np.linspace function for this, creating 11 entries, which will create 10 bins—they are\n",
      "the spaces in between two consecutive boundaries:\n",
      "In[12]:\n",
      "bins = np.linspace(-3, 3, 11)\n",
      "print(\"bins: {}\".format(bins))\n",
      "Out[12]:\n",
      "bins: [-3.  -2.4 -1.8 -1.2 -0.6  0.   0.6  1.2  1.8  2.4  3. ]\n",
      "Here, the first bin contains all data points with feature values –3 to –2.68, the second\n",
      "bin contains all points with feature values from –2.68 to –2.37, and so on.\n",
      "Next, we record for each data point which bin it falls into. This can be easily compu‐\n",
      "ted using the np.digitize function:\n",
      "Binning, Discretization, Linear Models, and Trees | 221\n",
      "In[13]:\n",
      "which_bin = np.digitize(X, bins=bins)\n",
      "print(\"\\nData points:\\n\", X[:5])\n",
      "print(\"\\nBin membership for data points:\\n\", which_bin[:5])\n",
      "Out[13]:\n",
      "Data points:\n",
      " [[-0.753]\n",
      "  [ 2.704]\n",
      "  [ 1.392]\n",
      "  [ 0.592]\n",
      " [-2.064]]\n",
      "Bin membership for data points:\n",
      " [[ 4]\n",
      "  [10]\n",
      "  [ 8]\n",
      "  [ 6]\n",
      " [ 2]]\n",
      "What we did here is transform the single continuous input feature in the wave dataset\n",
      "into a categorical feature that encodes which bin a data point is in. To use a scikit-\n",
      "learn model on this data, we transform this discrete feature to a one-hot encoding\n",
      "using the OneHotEncoder from the preprocessing module. The OneHotEncoder does\n",
      "the same encoding as pandas.get_dummies, though it currently only works on cate‐\n",
      "gorical variables that are integers:\n",
      "In[14]:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "# transform using the OneHotEncoder\n",
      "encoder = OneHotEncoder(sparse=False)\n",
      "# encoder.fit finds the unique values that appear in which_bin\n",
      "encoder.fit(which_bin)\n",
      "# transform creates the one-hot encoding\n",
      "X_binned = encoder.transform(which_bin)\n",
      "print(X_binned[:5])\n",
      "Out[14]:\n",
      "[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "Because we specified 10 bins, the transformed dataset X_binned now is made up of 10\n",
      "features:\n",
      "222 | Chapter 4: Representing Data and Engineering Features\n",
      "In[15]:\n",
      "print(\"X_binned.shape: {}\".format(X_binned.shape))\n",
      "Out[15]:\n",
      "X_binned.shape: (100, 10)\n",
      "Now we build a new linear regression model and a new decision tree model on the\n",
      "one-hot-encoded data. The result is visualized in Figure 4-2 , together with the bin\n",
      "boundaries, shown as dotted black lines:\n",
      "In[16]:\n",
      "line_binned = encoder.transform(np.digitize(line, bins=bins))\n",
      "reg = LinearRegression().fit(X_binned, y)\n",
      "plt.plot(line, reg.predict(line_binned), label='linear regression binned')\n",
      "reg = DecisionTreeRegressor(min_samples_split=3).fit(X_binned, y)\n",
      "plt.plot(line, reg.predict(line_binned), label='decision tree binned')\n",
      "plt.plot(X[:, 0], y, 'o', c='k')\n",
      "plt.vlines(bins, -3, 3, linewidth=1, alpha=.2)\n",
      "plt.legend(loc=\"best\")\n",
      "plt.ylabel(\"Regression output\")\n",
      "plt.xlabel(\"Input feature\")\n",
      "Figure 4-2. Comparing linear regression and decision tree regression on binned features\n",
      "Binning, Discretization, Linear Models, and Trees | 223\n",
      "The dashed line and solid line are exactly on top of each other, meaning the linear\n",
      "regression model and the decision tree make exactly the same predictions. For each\n",
      "bin, they predict a constant value. As features are constant within each bin, any\n",
      "model must predict the same value for all points within a bin. Comparing what the\n",
      "models learned before binning the features and after, we see that the linear model\n",
      "became much more flexible, because it now has a different value for each bin, while\n",
      "the decision tree model got much less flexible. Binning features generally has no ben‐\n",
      "eficial effect for tree-based models, as these models can learn to split up the data any‐\n",
      "where. In a sense, that means decision trees can learn whatever binning is most useful\n",
      "for predicting on this data. Additionally, decision trees look at multiple features at\n",
      "once, while binning is usually done on a per-feature basis. However, the linear model\n",
      "benefited greatly in expressiveness from the transformation of the data.\n",
      "If there are good reasons to use a linear model for a particular dataset—say, because it\n",
      "is very large and high-dimensional, but some features have nonlinear relations with\n",
      "the output—binning can be a great way to increase modeling power.\n",
      "Interactions and Polynomials\n",
      "Another way to enrich a feature representation, particularly for linear models, is\n",
      "adding interaction features and polynomial features of the original data. This kind of\n",
      "feature engineering is often used in statistical modeling, but it’s also common in many\n",
      "practical machine learning applications.\n",
      "As a first example, look again at Figure 4-2. The linear model learned a constant value\n",
      "for each bin in the wave dataset. We know, however, that linear models can learn not\n",
      "only offsets, but also slopes. One way to add a slope to the linear model on the binned\n",
      "data is to add the original feature (the x-axis in the plot) back in. This leads to an 11-\n",
      "dimensional dataset, as seen in Figure 4-3:\n",
      "In[17]:\n",
      "X_combined = np.hstack([X, X_binned])\n",
      "print(X_combined.shape)\n",
      "Out[17]:\n",
      "(100, 11)\n",
      "In[18]:\n",
      "reg = LinearRegression().fit(X_combined, y)\n",
      "line_combined = np.hstack([line, line_binned])\n",
      "plt.plot(line, reg.predict(line_combined), label='linear regression combined')\n",
      "for bin in bins:\n",
      "    plt.plot([bin, bin], [-3, 3], ':', c='k')\n",
      "224 | Chapter 4: Representing Data and Engineering Features\n",
      "plt.legend(loc=\"best\")\n",
      "plt.ylabel(\"Regression output\")\n",
      "plt.xlabel(\"Input feature\")\n",
      "plt.plot(X[:, 0], y, 'o', c='k')\n",
      "Figure 4-3. Linear regression using binned features and a single global slope\n",
      "In this example, the model learned an offset for each bin, together with a slope. The\n",
      "learned slope is downward, and shared across all the bins—there is a single x-axis fea‐\n",
      "ture, which has a single slope. Because the slope is shared across all bins, it doesn’t\n",
      "seem to be very helpful. We would rather have a separate slope for each bin! We can\n",
      "achieve this by adding an interaction or product feature that indicates which bin a\n",
      "data point is in and where it lies on the x-axis. This feature is a product of the bin\n",
      "indicator and the original feature. Let’s create this dataset:\n",
      "In[19]:\n",
      "X_product = np.hstack([X_binned, X * X_binned])\n",
      "print(X_product.shape)\n",
      "Out[19]:\n",
      "(100, 20)\n",
      "The dataset now has 20 features: the indicators for which bin a data point is in, and a\n",
      "product of the original feature and the bin indicator. Y ou can think of the product\n",
      "Interactions and Polynomials | 225\n",
      "feature as a separate copy of the x-axis feature for each bin. It is the original feature\n",
      "within the bin, and zero everywhere else. Figure 4-4  shows the result of the linear\n",
      "model on this new representation:\n",
      "In[20]:\n",
      "reg = LinearRegression().fit(X_product, y)\n",
      "line_product = np.hstack([line_binned, line * line_binned])\n",
      "plt.plot(line, reg.predict(line_product), label='linear regression product')\n",
      "for bin in bins:\n",
      "    plt.plot([bin, bin], [-3, 3], ':', c='k')\n",
      "plt.plot(X[:, 0], y, 'o', c='k')\n",
      "plt.ylabel(\"Regression output\")\n",
      "plt.xlabel(\"Input feature\")\n",
      "plt.legend(loc=\"best\")\n",
      "Figure 4-4. Linear regression with a separate slope per bin\n",
      "As you can see, now each bin has its own offset and slope in this model.\n",
      "226 | Chapter 4: Representing Data and Engineering Features\n",
      "Using binning is one way to expand a continuous feature. Another one is to use poly‐\n",
      "nomials of the original features. For a given feature x, we might want to consider\n",
      "x ** 2, x ** 3, x ** 4, and so on. This is implemented in PolynomialFeatures in\n",
      "the preprocessing module:\n",
      "In[21]:\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "# include polynomials up to x ** 10:\n",
      "# the default \"include_bias=True\" adds a feature that's constantly 1\n",
      "poly = PolynomialFeatures(degree=10, include_bias=False)\n",
      "poly.fit(X)\n",
      "X_poly = poly.transform(X)\n",
      "Using a degree of 10 yields 10 features:\n",
      "In[22]:\n",
      "print(\"X_poly.shape: {}\".format(X_poly.shape))\n",
      "Out[22]:\n",
      "X_poly.shape: (100, 10)\n",
      "Let’s compare the entries of X_poly to those of X:\n",
      "In[23]:\n",
      "print(\"Entries of X:\\n{}\".format(X[:5]))\n",
      "print(\"Entries of X_poly:\\n{}\".format(X_poly[:5]))\n",
      "Out[23]:\n",
      "Entries of X:\n",
      "[[-0.753]\n",
      " [ 2.704]\n",
      " [ 1.392]\n",
      " [ 0.592]\n",
      " [-2.064]]\n",
      "Entries of X_poly:\n",
      "[[    -0.753      0.567     -0.427      0.321     -0.242      0.182\n",
      "      -0.137      0.103     -0.078      0.058]\n",
      " [     2.704      7.313     19.777     53.482    144.632    391.125\n",
      "    1057.714   2860.360   7735.232  20918.278]\n",
      " [     1.392      1.938      2.697      3.754      5.226      7.274\n",
      "      10.125     14.094     19.618     27.307]\n",
      " [     0.592      0.350      0.207      0.123      0.073      0.043\n",
      "       0.025      0.015      0.009      0.005]\n",
      " [    -2.064      4.260     -8.791     18.144    -37.448     77.289\n",
      "    -159.516    329.222   -679.478   1402.367]]\n",
      "Y ou can obtain the semantics of the features by calling the get_feature_names\n",
      "method, which provides the exponent for each feature:\n",
      "Interactions and Polynomials | 227\n",
      "In[24]:\n",
      "print(\"Polynomial feature names:\\n{}\".format(poly.get_feature_names()))\n",
      "Out[24]:\n",
      "Polynomial feature names:\n",
      "['x0', 'x0^2', 'x0^3', 'x0^4', 'x0^5', 'x0^6', 'x0^7', 'x0^8', 'x0^9', 'x0^10']\n",
      "Y ou can see that the first column of X_poly corresponds exactly to X, while the other\n",
      "columns are the powers of the first entry. It’s interesting to see how large some of the\n",
      "values can get. The second column has entries above 20,000, orders of magnitude dif‐\n",
      "ferent from the rest.\n",
      "Using polynomial features together with a linear regression model yields the classical\n",
      "model of polynomial regression (see Figure 4-5):\n",
      "In[26]:\n",
      "reg = LinearRegression().fit(X_poly, y)\n",
      "line_poly = poly.transform(line)\n",
      "plt.plot(line, reg.predict(line_poly), label='polynomial linear regression')\n",
      "plt.plot(X[:, 0], y, 'o', c='k')\n",
      "plt.ylabel(\"Regression output\")\n",
      "plt.xlabel(\"Input feature\")\n",
      "plt.legend(loc=\"best\")\n",
      "Figure 4-5. Linear regression with tenth-degree polynomial features\n",
      "228 | Chapter 4: Representing Data and Engineering Features\n",
      "As you can see, polynomial features yield a very smooth fit on this one-dimensional\n",
      "data. However, polynomials of high degree tend to behave in extreme ways on the\n",
      "boundaries or in regions with little data.\n",
      "As a comparison, here is a kernel SVM model learned on the original data, without\n",
      "any transformation (see Figure 4-6):\n",
      "In[26]:\n",
      "from sklearn.svm import SVR\n",
      "for gamma in [1, 10]:\n",
      "    svr = SVR(gamma=gamma).fit(X, y)\n",
      "    plt.plot(line, svr.predict(line), label='SVR gamma={}'.format(gamma))\n",
      "plt.plot(X[:, 0], y, 'o', c='k')\n",
      "plt.ylabel(\"Regression output\")\n",
      "plt.xlabel(\"Input feature\")\n",
      "plt.legend(loc=\"best\")\n",
      "Figure 4-6. Comparison of different gamma parameters for an SVM with RBF kernel\n",
      "Using a more complex model, a kernel SVM, we are able to learn a similarly complex\n",
      "prediction to the polynomial regression without an explicit transformation of the\n",
      "features.\n",
      "Interactions and Polynomials | 229\n",
      "As a more realistic application of interactions and polynomials, let’s look again at the\n",
      "Boston Housing dataset. We already used polynomial features on this dataset in\n",
      "Chapter 2. Now let’s have a look at how these features were constructed, and at how\n",
      "much the polynomial features help. First we load the data, and rescale it to be\n",
      "between 0 and 1 using MinMaxScaler:\n",
      "In[27]:\n",
      "from sklearn.datasets import load_boston\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "boston = load_boston()\n",
      "X_train, X_test, y_train, y_test = train_test_split\n",
      "    (boston.data, boston.target, random_state=0)\n",
      "# rescale data\n",
      "scaler = MinMaxScaler()\n",
      "X_train_scaled = scaler.fit_transform(X_train)\n",
      "X_test_scaled = scaler.transform(X_test)\n",
      "Now, we extract polynomial features and interactions up to a degree of 2:\n",
      "In[28]:\n",
      "poly = PolynomialFeatures(degree=2).fit(X_train_scaled)\n",
      "X_train_poly = poly.transform(X_train_scaled)\n",
      "X_test_poly = poly.transform(X_test_scaled)\n",
      "print(\"X_train.shape: {}\".format(X_train.shape))\n",
      "print(\"X_train_poly.shape: {}\".format(X_train_poly.shape))\n",
      "Out[28]:\n",
      "X_train.shape: (379, 13)\n",
      "X_train_poly.shape: (379, 105)\n",
      "The data originally had 13 features, which were expanded into 105 interaction fea‐\n",
      "tures. These new features represent all possible interactions between two different\n",
      "original features, as well as the square of each original feature. degree=2 here means\n",
      "that we look at all features that are the product of up to two original features. The\n",
      "exact correspondence between input and output features can be found using the\n",
      "get_feature_names method:\n",
      "In[29]:\n",
      "print(\"Polynomial feature names:\\n{}\".format(poly.get_feature_names()))\n",
      "Out[29]:\n",
      "Polynomial feature names:\n",
      "['1', 'x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10',\n",
      "'x11', 'x12', 'x0^2', 'x0 x1', 'x0 x2', 'x0 x3', 'x0 x4', 'x0 x5', 'x0 x6',\n",
      "'x0 x7', 'x0 x8', 'x0 x9', 'x0 x10', 'x0 x11', 'x0 x12', 'x1^2', 'x1 x2',\n",
      "230 | Chapter 4: Representing Data and Engineering Features\n",
      "'x1 x3', 'x1 x4', 'x1 x5', 'x1 x6', 'x1 x7', 'x1 x8', 'x1 x9', 'x1 x10',\n",
      "'x1 x11', 'x1 x12', 'x2^2', 'x2 x3', 'x2 x4', 'x2 x5', 'x2 x6', 'x2 x7',\n",
      "'x2 x8', 'x2 x9', 'x2 x10', 'x2 x11', 'x2 x12', 'x3^2', 'x3 x4', 'x3 x5',\n",
      "'x3 x6', 'x3 x7', 'x3 x8', 'x3 x9', 'x3 x10', 'x3 x11', 'x3 x12', 'x4^2',\n",
      "'x4 x5', 'x4 x6', 'x4 x7', 'x4 x8', 'x4 x9', 'x4 x10', 'x4 x11', 'x4 x12',\n",
      "'x5^2', 'x5 x6', 'x5 x7', 'x5 x8', 'x5 x9', 'x5 x10', 'x5 x11', 'x5 x12',\n",
      "'x6^2', 'x6 x7', 'x6 x8', 'x6 x9', 'x6 x10', 'x6 x11', 'x6 x12', 'x7^2',\n",
      "'x7 x8', 'x7 x9', 'x7 x10', 'x7 x11', 'x7 x12', 'x8^2', 'x8 x9', 'x8 x10',\n",
      "'x8 x11', 'x8 x12', 'x9^2', 'x9 x10', 'x9 x11', 'x9 x12', 'x10^2', 'x10 x11',\n",
      "'x10 x12', 'x11^2', 'x11 x12', 'x12^2']\n",
      "The first new feature is a constant feature, called \"1\" here. The next 13 features are\n",
      "the original features (called \"x0\" to \"x12\"). Then follows the first feature squared\n",
      "(\"x0^2\") and combinations of the first and the other features.\n",
      "Let’s compare the performance using Ridge on the data with and without interac‐\n",
      "tions:\n",
      "In[30]:\n",
      "from sklearn.linear_model import Ridge\n",
      "ridge = Ridge().fit(X_train_scaled, y_train)\n",
      "print(\"Score without interactions: {:.3f}\".format(\n",
      "    ridge.score(X_test_scaled, y_test)))\n",
      "ridge = Ridge().fit(X_train_poly, y_train)\n",
      "print(\"Score with interactions: {:.3f}\".format(\n",
      "    ridge.score(X_test_poly, y_test)))\n",
      "Out[30]:\n",
      "Score without interactions: 0.621\n",
      "Score with interactions: 0.753\n",
      "Clearly, the interactions and polynomial features gave us a good boost in perfor‐\n",
      "mance when using Ridge. When using a more complex model like a random forest,\n",
      "the story is a bit different, though:\n",
      "In[31]:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "rf = RandomForestRegressor(n_estimators=100).fit(X_train_scaled, y_train)\n",
      "print(\"Score without interactions: {:.3f}\".format(\n",
      "    rf.score(X_test_scaled, y_test)))\n",
      "rf = RandomForestRegressor(n_estimators=100).fit(X_train_poly, y_train)\n",
      "print(\"Score with interactions: {:.3f}\".format(rf.score(X_test_poly, y_test)))\n",
      "Out[31]:\n",
      "Score without interactions: 0.799\n",
      "Score with interactions: 0.763\n",
      "Interactions and Polynomials | 231\n",
      "Y ou can see that even without additional features, the random forest beats the\n",
      "performance of Ridge. Adding interactions and polynomials actually decreases per‐\n",
      "formance slightly.\n",
      "Univariate Nonlinear Transformations\n",
      "We just saw that adding squared or cubed features can help linear models for regres‐\n",
      "sion. There are other transformations that often prove useful for transforming certain\n",
      "features: in particular, applying mathematical functions like log, exp, or sin. While\n",
      "tree-based models only care about the ordering of the features, linear models and\n",
      "neural networks are very tied to the scale and distribution of each feature, and if there\n",
      "is a nonlinear relation between the feature and the target, that becomes hard to model\n",
      "—particularly in regression. The functions log and exp can help by adjusting the rel‐\n",
      "ative scales in the data so that they can be captured better by a linear model or neural\n",
      "network. We saw an application of that in Chapter 2 with the memory price data. The\n",
      "sin and cos functions can come in handy when dealing with data that encodes peri‐\n",
      "odic patterns.\n",
      "Most models work best when each feature (and in regression also the target) is loosely\n",
      "Gaussian distributed—that is, a histogram of each feature should have something\n",
      "resembling the familiar “bell curve” shape. Using transformations like log and exp is\n",
      "a hacky but simple and efficient way to achieve this. A particularly common case\n",
      "when such a transformation can be helpful is when dealing with integer count data.\n",
      "By count data, we mean features like “how often did user A log in?” Counts are never\n",
      "negative, and often follow particular statistical patterns. We are using a synthetic\n",
      "dataset of counts here that has properties similar to those you can find in the wild.\n",
      "The features are all integer-valued, while the response is continuous:\n",
      "In[32]:\n",
      "rnd = np.random.RandomState(0)\n",
      "X_org = rnd.normal(size=(1000, 3))\n",
      "w = rnd.normal(size=3)\n",
      "X = rnd.poisson(10 * np.exp(X_org))\n",
      "y = np.dot(X_org, w)\n",
      "Let’s look at the first 10 entries of the first feature. All are integer values and positive,\n",
      "but apart from that it’s hard to make out a particular pattern.\n",
      "If we count the appearance of each value, the distribution of values becomes clearer:\n",
      "232 | Chapter 4: Representing Data and Engineering Features\n",
      "In[33]:\n",
      "print(\"Number of feature appearances:\\n{}\".format(np.bincount(X[:, 0])))\n",
      "Out[33]:\n",
      "Number of feature appearances:\n",
      "[28 38 68 48 61 59 45 56 37 40 35 34 36 26 23 26 27 21 23 23 18 21 10  9 17\n",
      "  9  7 14 12  7  3  8  4  5  5  3  4  2  4  1  1  3  2  5  3  8  2  5  2  1\n",
      "  2  3  3  2  2  3  3  0  1  2  1  0  0  3  1  0  0  0  1  3  0  1  0  2  0\n",
      "  1  1  0  0  0  0  1  0  0  2  2  0  1  1  0  0  0  0  1  1  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  1  1  0  0  1  0  0  0  0  0  0  0  1  0  0  0  0\n",
      "  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]\n",
      "The value 2 seems to be the most common, with 62 appearances ( bincount always\n",
      "starts at 0), and the counts for higher values fall quickly. However, there are some\n",
      "very high values, like 134 appearing twice. We visualize the counts in Figure 4-7:\n",
      "In[34]:\n",
      "bins = np.bincount(X[:, 0])\n",
      "plt.bar(range(len(bins)), bins, color='w')\n",
      "plt.ylabel(\"Number of appearances\")\n",
      "plt.xlabel(\"Value\")\n",
      "Figure 4-7. Histogram of feature values for X[0]\n",
      "Univariate Nonlinear Transformations | 233\n",
      "1 This is a Poisson distribution, which is quite fundamental to count data.\n",
      "Features X[:, 1] and X[:, 2] have similar properties. This kind of distribution of\n",
      "values (many small ones and a few very large ones) is very common in practice. 1\n",
      "However, it is something most linear models can’t handle very well. Let’s try to fit a\n",
      "ridge regression to this model:\n",
      "In[35]:\n",
      "from sklearn.linear_model import Ridge\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
      "score = Ridge().fit(X_train, y_train).score(X_test, y_test)\n",
      "print(\"Test score: {:.3f}\".format(score))\n",
      "Out[35]:\n",
      "Test score: 0.622\n",
      "As you can see from the relatively low R2 score, Ridge was not able to really capture\n",
      "the relationship between X and y. Applying a logarithmic transformation can help,\n",
      "though. Because the value 0 appears in the data (and the logarithm is not defined at\n",
      "0), we can’t actually just apply log, but we have to compute log(X + 1):\n",
      "In[36]:\n",
      "X_train_log = np.log(X_train + 1)\n",
      "X_test_log = np.log(X_test + 1)\n",
      "After the transformation, the distribution of the data is less asymmetrical and doesn’t\n",
      "have very large outliers anymore (see Figure 4-8):\n",
      "In[37]:\n",
      "plt.hist(np.log(X_train_log[:, 0] + 1), bins=25, color='gray')\n",
      "plt.ylabel(\"Number of appearances\")\n",
      "plt.xlabel(\"Value\")\n",
      "234 | Chapter 4: Representing Data and Engineering Features\n",
      "2 This is a very crude approximation of using Poisson regression, which would be the proper solution from a\n",
      "probabilistic standpoint.\n",
      "Figure 4-8. Histogram of feature values for X[0] after logarithmic transformation\n",
      "Building a ridge model on the new data provides a much better fit:\n",
      "In[38]:\n",
      "score = Ridge().fit(X_train_log, y_train).score(X_test_log, y_test)\n",
      "print(\"Test score: {:.3f}\".format(score))\n",
      "Out[38]:\n",
      "Test score: 0.875\n",
      "Finding the transformation that works best for each combination of dataset and\n",
      "model is somewhat of an art. In this example, all the features had the same properties.\n",
      "This is rarely the case in practice, and usually only a subset of the features should be\n",
      "transformed, or sometimes each feature needs to be transformed in a different way.\n",
      "As we mentioned earlier, these kinds of transformations are irrelevant for tree-based\n",
      "models but might be essential for linear models. Sometimes it is also a good idea to\n",
      "transform the target variable y in regression. Trying to predict counts (say, number of\n",
      "orders) is a fairly common task, and using the log(y + 1)  transformation often\n",
      "helps.2\n",
      "Univariate Nonlinear Transformations | 235\n",
      "As you saw in the previous examples, binning, polynomials, and interactions can\n",
      "have a huge influence on how models perform on a given dataset. This is particularly\n",
      "true for less complex models like linear models and naive Bayes models. Tree-based\n",
      "models, on the other hand, are often able to discover important interactions them‐\n",
      "selves, and don’t require transforming the data explicitly most of the time. Other\n",
      "models, like SVMs, nearest neighbors, and neural networks, might sometimes benefit\n",
      "from using binning, interactions, or polynomials, but the implications there are usu‐\n",
      "ally much less clear than in the case of linear models.\n",
      "Automatic Feature Selection\n",
      "With so many ways to create new features, you might get tempted to increase the\n",
      "dimensionality of the data way beyond the number of original features. However,\n",
      "adding more features makes all models more complex, and so increases the chance of\n",
      "overfitting. When adding new features, or with high-dimensional datasets in general,\n",
      "it can be a good idea to reduce the number of features to only the most useful ones,\n",
      "and discard the rest. This can lead to simpler models that generalize better. But how\n",
      "can you know how good each feature is? There are three basic strategies: univariate\n",
      "statistics, model-based selection , and iterative selection. We will discuss all three of\n",
      "them in detail. All of these methods are supervised methods, meaning they need the\n",
      "target for fitting the model. This means we need to split the data into training and test\n",
      "sets, and fit the feature selection only on the training part of the data.\n",
      "Univariate Statistics\n",
      "In univariate statistics, we compute whether there is a statistically significant relation‐\n",
      "ship between each feature and the target. Then the features that are related with the\n",
      "highest confidence are selected. In the case of classification, this is also known as\n",
      "analysis of variance (ANOV A). A key property of these tests is that they are univari‐\n",
      "ate, meaning that they only consider each feature individually. Consequently, a fea‐\n",
      "ture will be discarded if it is only informative when combined with another feature.\n",
      "Univariate tests are often very fast to compute, and don’t require building a model.\n",
      "On the other hand, they are completely independent of the model that you might\n",
      "want to apply after the feature selection.\n",
      "To use univariate feature selection in scikit-learn, you need to choose a test, usu‐\n",
      "ally either f_classif (the default) for classification or f_regression for regression,\n",
      "and a method to discard features based on the p-values determined in the test. All\n",
      "methods for discarding parameters use a threshold to discard all features with too\n",
      "high a p-value (which means they are unlikely to be related to the target). The meth‐\n",
      "ods differ in how they compute this threshold, with the simplest ones being SelectKB\n",
      "est, which selects a fixed number k of features, and SelectPercentile, which selects\n",
      "a fixed percentage of features. Let’s apply the feature selection for classification on the\n",
      "236 | Chapter 4: Representing Data and Engineering Features\n",
      "cancer dataset. To make the task a bit harder, we’ll add some noninformative noise\n",
      "features to the data. We expect the feature selection to be able to identify the features\n",
      "that are noninformative and remove them:\n",
      "In[39]:\n",
      "from sklearn.datasets import load_breast_cancer\n",
      "from sklearn.feature_selection import SelectPercentile\n",
      "from sklearn.model_selection import train_test_split\n",
      "cancer = load_breast_cancer()\n",
      "# get deterministic random numbers\n",
      "rng = np.random.RandomState(42)\n",
      "noise = rng.normal(size=(len(cancer.data), 50))\n",
      "# add noise features to the data\n",
      "# the first 30 features are from the dataset, the next 50 are noise\n",
      "X_w_noise = np.hstack([cancer.data, noise])\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X_w_noise, cancer.target, random_state=0, test_size=.5)\n",
      "# use f_classif (the default) and SelectPercentile to select 50% of features\n",
      "select = SelectPercentile(percentile=50)\n",
      "select.fit(X_train, y_train)\n",
      "# transform training set\n",
      "X_train_selected = select.transform(X_train)\n",
      "print(\"X_train.shape: {}\".format(X_train.shape))\n",
      "print(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\n",
      "Out[39]:\n",
      "X_train.shape: (284, 80)\n",
      "X_train_selected.shape: (284, 40)\n",
      "As you can see, the number of features was reduced from 80 to 40 (50 percent of the\n",
      "original number of features). We can find out which features have been selected using\n",
      "the get_support method, which returns a Boolean mask of the selected features\n",
      "(visualized in Figure 4-9):\n",
      "In[40]:\n",
      "mask = select.get_support()\n",
      "print(mask)\n",
      "# visualize the mask -- black is True, white is False\n",
      "plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n",
      "plt.xlabel(\"Sample index\")\n",
      "Out[40]:\n",
      "[ True  True  True  True  True  True  True  True  True False  True False\n",
      "  True  True  True  True  True  True False False  True  True  True  True\n",
      "  True  True  True  True  True  True False False False  True False  True\n",
      "Automatic Feature Selection | 237\n",
      "False False  True False False False False  True False False  True False\n",
      " False  True False  True False False False False False False  True False\n",
      "  True False False False False  True False  True False False False False\n",
      "  True  True False  True False False False False]\n",
      "Figure 4-9. Features selected by SelectPercentile\n",
      "As you can see from the visualization of the mask, most of the selected features are\n",
      "the original features, and most of the noise features were removed. However, the\n",
      "recovery of the original features is not perfect. Let’s compare the performance of\n",
      "logistic regression on all features against the performance using only the selected\n",
      "features:\n",
      "In[41]:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "# transform test data\n",
      "X_test_selected = select.transform(X_test)\n",
      "lr = LogisticRegression()\n",
      "lr.fit(X_train, y_train)\n",
      "print(\"Score with all features: {:.3f}\".format(lr.score(X_test, y_test)))\n",
      "lr.fit(X_train_selected, y_train)\n",
      "print(\"Score with only selected features: {:.3f}\".format(\n",
      "    lr.score(X_test_selected, y_test)))\n",
      "Out[41]:\n",
      "Score with all features: 0.930\n",
      "Score with only selected features: 0.940\n",
      "In this case, removing the noise features improved performance, even though some\n",
      "of the original features were lost. This was a very simple synthetic example, and out‐\n",
      "comes on real data are usually mixed. Univariate feature selection can still be very\n",
      "helpful, though, if there is such a large number of features that building a model on\n",
      "them is infeasible, or if you suspect that many features are completely uninformative.\n",
      "Model-Based Feature Selection\n",
      "Model-based feature selection uses a supervised machine learning model to judge the\n",
      "importance of each feature, and keeps only the most important ones. The supervised\n",
      "model that is used for feature selection doesn’t need to be the same model that is used\n",
      "for the final supervised modeling. The feature selection model needs to provide some\n",
      "measure of importance for each feature, so that they can be ranked by this measure.\n",
      "Decision trees and decision tree–based models provide a feature_importances_\n",
      "238 | Chapter 4: Representing Data and Engineering Features\n",
      "attribute, which directly encodes the importance of each feature. Linear models have\n",
      "coefficients, which can also be used to capture feature importances by considering the\n",
      "absolute values. As we saw in Chapter 2, linear models with L1 penalty learn sparse\n",
      "coefficients, which only use a small subset of features. This can be viewed as a form of\n",
      "feature selection for the model itself, but can also be used as a preprocessing step to\n",
      "select features for another model. In contrast to univariate selection, model-based\n",
      "selection considers all features at once, and so can capture interactions (if the model\n",
      "can capture them). To use model-based feature selection, we need to use the\n",
      "SelectFromModel transformer:\n",
      "In[42]:\n",
      "from sklearn.feature_selection import SelectFromModel\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "select = SelectFromModel(\n",
      "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
      "    threshold=\"median\")\n",
      "The SelectFromModel class selects all features that have an importance measure of\n",
      "the feature (as provided by the supervised model) greater than the provided thresh‐\n",
      "old. To get a comparable result to what we got with univariate feature selection, we\n",
      "used the median as a threshold, so that half of the features will be selected. We use a\n",
      "random forest classifier with 100 trees to compute the feature importances. This is a\n",
      "quite complex model and much more powerful than using univariate tests. Now let’s\n",
      "actually fit the model:\n",
      "In[43]:\n",
      "select.fit(X_train, y_train)\n",
      "X_train_l1 = select.transform(X_train)\n",
      "print(\"X_train.shape: {}\".format(X_train.shape))\n",
      "print(\"X_train_l1.shape: {}\".format(X_train_l1.shape))\n",
      "Out[43]:\n",
      "X_train.shape: (284, 80)\n",
      "X_train_l1.shape: (284, 40)\n",
      "Again, we can have a look at the features that were selected (Figure 4-10):\n",
      "In[44]:\n",
      "mask = select.get_support()\n",
      "# visualize the mask -- black is True, white is False\n",
      "plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n",
      "plt.xlabel(\"Sample index\")\n",
      "Figure 4-10. Features selected by SelectFromModel using the RandomForestClassifier\n",
      "Automatic Feature Selection | 239\n",
      "This time, all but two of the original features were selected. Because we specified to\n",
      "select 40 features, some of the noise features are also selected. Let’s take a look at the\n",
      "performance:\n",
      "In[45]:\n",
      "X_test_l1 = select.transform(X_test)\n",
      "score = LogisticRegression().fit(X_train_l1, y_train).score(X_test_l1, y_test)\n",
      "print(\"Test score: {:.3f}\".format(score))\n",
      "Out[45]:\n",
      "Test score: 0.951\n",
      "With the better feature selection, we also gained some improvements here.\n",
      "Iterative Feature Selection\n",
      "In univariate testing we used no model, while in model-based selection we used a sin‐\n",
      "gle model to select features. In iterative feature selection, a series of models are built,\n",
      "with varying numbers of features. There are two basic methods: starting with no fea‐\n",
      "tures and adding features one by one until some stopping criterion is reached, or\n",
      "starting with all features and removing features one by one until some stopping crite‐\n",
      "rion is reached. Because a series of models are built, these methods are much more\n",
      "computationally expensive than the methods we discussed previously. One particular\n",
      "method of this kind is recursive feature elimination (RFE), which starts with all fea‐\n",
      "tures, builds a model, and discards the least important feature according to the\n",
      "model. Then a new model is built using all but the discarded feature, and so on until\n",
      "only a prespecified number of features are left. For this to work, the model used for\n",
      "selection needs to provide some way to determine feature importance, as was the case\n",
      "for the model-based selection. Here, we use the same random forest model that we\n",
      "used earlier, and get the results shown in Figure 4-11:\n",
      "In[46]:\n",
      "from sklearn.feature_selection import RFE\n",
      "select = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\n",
      "             n_features_to_select=40)\n",
      "select.fit(X_train, y_train)\n",
      "# visualize the selected features:\n",
      "mask = select.get_support()\n",
      "plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n",
      "plt.xlabel(\"Sample index\")\n",
      "240 | Chapter 4: Representing Data and Engineering Features\n",
      "Figure 4-11. Features selected by recursive feature elimination with the random forest\n",
      "classifier model\n",
      "The feature selection got better compared to the univariate and model-based selec‐\n",
      "tion, but one feature was still missed. Running this code also takes significantly longer\n",
      "than that for the model-based selection, because a random forest model is trained 40\n",
      "times, once for each feature that is dropped. Let’s test the accuracy of the logistic\n",
      "regression model when using RFE for feature selection:\n",
      "In[47]:\n",
      "X_train_rfe= select.transform(X_train)\n",
      "X_test_rfe= select.transform(X_test)\n",
      "score = LogisticRegression().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\n",
      "print(\"Test score: {:.3f}\".format(score))\n",
      "Out[47]:\n",
      "Test score: 0.951\n",
      "We can also use the model used inside the RFE to make predictions. This uses only\n",
      "the feature set that was selected:\n",
      "In[48]:\n",
      "print(\"Test score: {:.3f}\".format(select.score(X_test, y_test)))\n",
      "Out[48]:\n",
      "Test score: 0.951\n",
      "Here, the performance of the random forest used inside the RFE is the same as that\n",
      "achieved by training a logistic regression model on top of the selected features. In\n",
      "other words, once we’ve selected the right features, the linear model performs as well\n",
      "as the random forest.\n",
      "If you are unsure when selecting what to use as input to your machine learning algo‐\n",
      "rithms, automatic feature selection can be quite helpful. It is also great for reducing\n",
      "the amount of features needed—for example, to speed up prediction or to allow for\n",
      "more interpretable models. In most real-world cases, applying feature selection is\n",
      "unlikely to provide large gains in performance. However, it is still a valuable tool in\n",
      "the toolbox of the feature engineer.\n",
      "Automatic Feature Selection | 241\n",
      "Utilizing Expert Knowledge\n",
      "Feature engineering is often an important place to use expert knowledge for a particu‐\n",
      "lar application. While the purpose of machine learning in many cases is to avoid hav‐\n",
      "ing to create a set of expert-designed rules, that doesn’t mean that prior knowledge of\n",
      "the application or domain should be discarded. Often, domain experts can help in\n",
      "identifying useful features that are much more informative than the initial represen‐\n",
      "tation of the data. Imagine you work for a travel agency and want to predict flight\n",
      "prices. Let’s say you have a record of prices together with dates, airlines, start loca‐\n",
      "tions, and destinations. A machine learning model might be able to build a decent\n",
      "model from that. Some important factors in flight prices, however, cannot be learned.\n",
      "For example, flights are usually more expensive during peak vacation months and\n",
      "around holidays. While the dates of some holidays (like Christmas) are fixed, and\n",
      "their effect can therefore be learned from the date, others might depend on the phases\n",
      "of the moon (like Hanukkah and Easter) or be set by authorities (like school holi‐\n",
      "days). These events cannot be learned from the data if each flight is only recorded\n",
      "using the (Gregorian) date. However, it is easy to add a feature that encodes whether a\n",
      "flight was on, preceding, or following a public or school holiday. In this way, prior\n",
      "knowledge about the nature of the task can be encoded in the features to aid a\n",
      "machine learning algorithm. Adding a feature does not force a machine learning\n",
      "algorithm to use it, and even if the holiday information turns out to be noninforma‐\n",
      "tive for flight prices, augmenting the data with this information doesn’t hurt.\n",
      "We’ll now look at one particular case of using expert knowledge—though in this case\n",
      "it might be more rightfully called “common sense. ” The task is predicting bicycle rent‐\n",
      "als in front of Andreas’s house.\n",
      "In New Y ork, Citi Bike operates a network of bicycle rental stations with a subscrip‐\n",
      "tion system. The stations are all over the city and provide a convenient way to get\n",
      "around. Bike rental data is made public in an anonymized form  and has been ana‐\n",
      "lyzed in various ways. The task we want to solve is to predict for a given time and day\n",
      "how many people will rent a bike in front of Andreas’s house—so he knows if any\n",
      "bikes will be left for him.\n",
      "We first load the data for August 2015 for this particular station as a pandas Data\n",
      "Frame. We resample the data into three-hour intervals to obtain the main trends for\n",
      "each day:\n",
      "In[49]:\n",
      "citibike = mglearn.datasets.load_citibike()\n",
      "242 | Chapter 4: Representing Data and Engineering Features\n",
      "In[50]:\n",
      "print(\"Citi Bike data:\\n{}\".format(citibike.head()))\n",
      "Out[50]:\n",
      "Citi Bike data:\n",
      "starttime\n",
      "2015-08-01 00:00:00     3.0\n",
      "2015-08-01 03:00:00     0.0\n",
      "2015-08-01 06:00:00     9.0\n",
      "2015-08-01 09:00:00    41.0\n",
      "2015-08-01 12:00:00    39.0\n",
      "Freq: 3H, Name: one, dtype: float64\n",
      "The following example shows a visualization of the rental frequencies for the whole\n",
      "month (Figure 4-12):\n",
      "In[51]:\n",
      "plt.figure(figsize=(10, 3))\n",
      "xticks = pd.date_range(start=citibike.index.min(), end=citibike.index.max(),\n",
      "                       freq='D')\n",
      "plt.xticks(xticks, xticks.strftime(\"%a %m-%d\"), rotation=90, ha=\"left\")\n",
      "plt.plot(citibike, linewidth=1)\n",
      "plt.xlabel(\"Date\")\n",
      "plt.ylabel(\"Rentals\")\n",
      "Figure 4-12. Number of bike rentals over time for a selected Citi Bike station\n",
      "Looking at the data, we can clearly distinguish day and night for each 24-hour inter‐\n",
      "val. The patterns for weekdays and weekends also seem to be quite different. When\n",
      "evaluating a prediction task on a time series like this, we usually want to learn from\n",
      "the past and predict for the future. This means when doing a split into a training and a\n",
      "test set, we want to use all the data up to a certain date as the training set and all the\n",
      "data past that date as the test set. This is how we would usually use time series predic‐\n",
      "tion: given everything that we know about rentals in the past, what do we think will\n",
      "Utilizing Expert Knowledge | 243\n",
      "happen tomorrow? We will use the first 184 data points, corresponding to the first 23\n",
      "days, as our training set, and the remaining 64 data points, corresponding to the\n",
      "remaining 8 days, as our test set.\n",
      "The only feature that we are using in our prediction task is the date and time when a\n",
      "particular number of rentals occurred. So, the input feature is the date and time—say,\n",
      "2015-08-01 00:00:00—and the output is the number of rentals in the following\n",
      "three hours (three in this case, according to our DataFrame).\n",
      "A (surprisingly) common way that dates are stored on computers is using POSIX\n",
      "time, which is the number of seconds since January 1970 00:00:00 (aka the beginning\n",
      "of Unix time). As a first try, we can use this single integer feature as our data repre‐\n",
      "sentation:\n",
      "In[52]:\n",
      "# extract the target values (number of rentals)\n",
      "y = citibike.values\n",
      "# convert the time to POSIX time using \"%s\"\n",
      "X = citibike.index.strftime(\"%s\").astype(\"int\").reshape(-1, 1)\n",
      "We first define a function to split the data into training and test sets, build the model,\n",
      "and visualize the result:\n",
      "In[54]:\n",
      "# use the first 184 data points for training, and the rest for testing\n",
      "n_train = 184\n",
      "# function to evaluate and plot a regressor on a given feature set\n",
      "def eval_on_features(features, target, regressor):\n",
      "    # split the given features into a training and a test set\n",
      "    X_train, X_test = features[:n_train], features[n_train:]\n",
      "    # also split the target array\n",
      "    y_train, y_test = target[:n_train], target[n_train:]\n",
      "    regressor.fit(X_train, y_train)\n",
      "    print(\"Test-set R^2: {:.2f}\".format(regressor.score(X_test, y_test)))\n",
      "    y_pred = regressor.predict(X_test)\n",
      "    y_pred_train = regressor.predict(X_train)\n",
      "    plt.figure(figsize=(10, 3))\n",
      "    plt.xticks(range(0, len(X), 8), xticks.strftime(\"%a %m-%d\"), rotation=90,\n",
      "               ha=\"left\")\n",
      "    plt.plot(range(n_train), y_train, label=\"train\")\n",
      "    plt.plot(range(n_train, len(y_test) + n_train), y_test, '-', label=\"test\")\n",
      "    plt.plot(range(n_train), y_pred_train, '--', label=\"prediction train\")\n",
      "    plt.plot(range(n_train, len(y_test) + n_train), y_pred, '--',\n",
      "             label=\"prediction test\")\n",
      "    plt.legend(loc=(1.01, 0))\n",
      "    plt.xlabel(\"Date\")\n",
      "    plt.ylabel(\"Rentals\")\n",
      "244 | Chapter 4: Representing Data and Engineering Features\n",
      "We saw earlier that random forests require very little preprocessing of the data, which\n",
      "makes this seem like a good model to start with. We use the POSIX time feature X and\n",
      "pass a random forest regressor to our eval_on_features function. Figure 4-13 shows\n",
      "the result:\n",
      "In[55]:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
      "plt.figure()\n",
      "eval_on_features(X, y, regressor)\n",
      "Out[55]:\n",
      "Test-set R^2: -0.04\n",
      "Figure 4-13. Predictions made by a random forest using only the POSIX time\n",
      "The predictions on the training set are quite good, as is usual for random forests.\n",
      "However, for the test set, a constant line is predicted. The R2 is –0.03, which means\n",
      "that we learned nothing. What happened?\n",
      "The problem lies in the combination of our feature and the random forest. The value\n",
      "of the POSIX time feature for the test set is outside of the range of the feature values\n",
      "in the training set: the points in the test set have timestamps that are later than all the\n",
      "points in the training set. Trees, and therefore random forests, cannot extrapolate to\n",
      "feature ranges outside the training set. The result is that the model simply predicts the\n",
      "target value of the closest point in the training set—which is the last time it observed\n",
      "any data.\n",
      "Clearly we can do better than this. This is where our “expert knowledge” comes in.\n",
      "From looking at the rental figures in the training data, two factors seem to be very\n",
      "important: the time of day and the day of the week. So, let’s add these two features.\n",
      "We can’t really learn anything from the POSIX time, so we drop that feature. First,\n",
      "let’s use only the hour of the day. As Figure 4-14 shows, now the predictions have the\n",
      "same pattern for each day of the week:\n",
      "Utilizing Expert Knowledge | 245\n",
      "In[56]:\n",
      "X_hour = citibike.index.hour.reshape(-1, 1)\n",
      "eval_on_features(X_hour, y, regressor)\n",
      "Out[56]:\n",
      "Test-set R^2: 0.60\n",
      "Figure 4-14. Predictions made by a random forest using only the hour of the day\n",
      "The R2 is already much better, but the predictions clearly miss the weekly pattern.\n",
      "Now let’s also add the day of the week (see Figure 4-15):\n",
      "In[57]:\n",
      "X_hour_week = np.hstack([citibike.index.dayofweek.reshape(-1, 1),\n",
      "                         citibike.index.hour.reshape(-1, 1)])\n",
      "eval_on_features(X_hour_week, y, regressor)\n",
      "Out[57]:\n",
      "Test-set R^2: 0.84\n",
      "Figure 4-15. Predictions with a random forest using day of week and hour of day\n",
      "features\n",
      "246 | Chapter 4: Representing Data and Engineering Features\n",
      "Now we have a model that captures the periodic behavior by considering the day of\n",
      "week and time of day. It has an R2 of 0.84, and shows pretty good predictive perfor‐\n",
      "mance. What this model likely is learning is the mean number of rentals for each\n",
      "combination of weekday and time of day from the first 23 days of August. This\n",
      "actually does not require a complex model like a random forest, so let’s try with a\n",
      "simpler model, LinearRegression (see Figure 4-16):\n",
      "In[58]:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "eval_on_features(X_hour_week, y, LinearRegression())\n",
      "Out[58]:\n",
      "Test-set R^2: 0.13\n",
      "Figure 4-16. Predictions made by linear regression using day of week and hour of day as\n",
      "features\n",
      "LinearRegression works much worse, and the periodic pattern looks odd. The rea‐\n",
      "son for this is that we encoded day of week and time of day using integers, which are\n",
      "interpreted as categorical variables. Therefore, the linear model can only learn a lin‐\n",
      "ear function of the time of day—and it learned that later in the day, there are more\n",
      "rentals. However, the patterns are much more complex than that. We can capture this\n",
      "by interpreting the integers as categorical variables, by transforming them using One\n",
      "HotEncoder (see Figure 4-17):\n",
      "In[59]:\n",
      "enc = OneHotEncoder()\n",
      "X_hour_week_onehot = enc.fit_transform(X_hour_week).toarray()\n",
      "Utilizing Expert Knowledge | 247\n",
      "In[60]:\n",
      "eval_on_features(X_hour_week_onehot, y, Ridge())\n",
      "Out[60]:\n",
      "Test-set R^2: 0.62\n",
      "Figure 4-17. Predictions made by linear regression using a one-hot encoding of hour of\n",
      "day and day of week\n",
      "This gives us a much better match than the continuous feature encoding. Now the\n",
      "linear model learns one coefficient for each day of the week, and one coefficient for\n",
      "each time of the day. That means that the “time of day” pattern is shared over all days\n",
      "of the week, though.\n",
      "Using interaction features, we can allow the model to learn one coefficient for each\n",
      "combination of day and time of day (see Figure 4-18):\n",
      "In[61]:\n",
      "poly_transformer = PolynomialFeatures(degree=2, interaction_only=True,\n",
      "                                      include_bias=False)\n",
      "X_hour_week_onehot_poly = poly_transformer.fit_transform(X_hour_week_onehot)\n",
      "lr = Ridge()\n",
      "eval_on_features(X_hour_week_onehot_poly, y, lr)\n",
      "Out[61]:\n",
      "Test-set R^2: 0.85\n",
      "248 | Chapter 4: Representing Data and Engineering Features\n",
      "Figure 4-18. Predictions made by linear regression using a product of the day of week\n",
      "and hour of day features\n",
      "This transformation finally yields a model that performs similarly well to the random\n",
      "forest. A big benefit of this model is that it is very clear what is learned: one coeffi‐\n",
      "cient for each day and time. We can simply plot the coefficients learned by the model,\n",
      "something that would not be possible for the random forest.\n",
      "First, we create feature names for the hour and day features:\n",
      "In[62]:\n",
      "hour = [\"%02d:00\" % i for i in range(0, 24, 3)]\n",
      "day = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "features =  day + hour\n",
      "Then we name all the interaction features extracted by PolynomialFeatures, using\n",
      "the get_feature_names method, and keep only the features with nonzero coeffi‐\n",
      "cients:\n",
      "In[63]:\n",
      "features_poly = poly_transformer.get_feature_names(features)\n",
      "features_nonzero = np.array(features_poly)[lr.coef_ != 0]\n",
      "coef_nonzero = lr.coef_[lr.coef_ != 0]\n",
      "Now we can visualize the coefficients learned by the linear model, as seen in\n",
      "Figure 4-19:\n",
      "In[64]:\n",
      "plt.figure(figsize=(15, 2))\n",
      "plt.plot(coef_nonzero, 'o')\n",
      "plt.xticks(np.arange(len(coef_nonzero)), features_nonzero, rotation=90)\n",
      "plt.xlabel(\"Feature magnitude\")\n",
      "plt.ylabel(\"Feature\")\n",
      "Utilizing Expert Knowledge | 249\n",
      "Figure 4-19. Coefficients of the linear regression model using a product of hour and day\n",
      "Summary and Outlook\n",
      "In this chapter, we discussed how to deal with different data types (in particular, with\n",
      "categorical variables). We emphasized the importance of representing data in a way\n",
      "that is suitable for the machine learning algorithm—for example, by one-hot-\n",
      "encoding categorical variables. We also discussed the importance of engineering new\n",
      "features, and the possibility of utilizing expert knowledge in creating derived features\n",
      "from your data. In particular, linear models might benefit greatly from generating\n",
      "new features via binning and adding polynomials and interactions, while more com‐\n",
      "plex, nonlinear models like random forests and SVMs might be able to learn more\n",
      "complex tasks without explicitly expanding the feature space. In practice, the features\n",
      "that are used (and the match between features and method) is often the most impor‐\n",
      "tant piece in making a machine learning approach work well.\n",
      "Now that you have a good idea of how to represent your data in an appropriate way\n",
      "and which algorithm to use for which task, the next chapter will focus on evaluating\n",
      "the performance of machine learning models and selecting the right parameter\n",
      "settings.\n",
      "250 | Chapter 4: Representing Data and Engineering Features\n",
      "CHAPTER 5\n",
      "Model Evaluation and Improvement\n",
      "Having discussed the fundamentals of supervised and unsupervised learning, and\n",
      "having explored a variety of machine learning algorithms, we will now dive more\n",
      "deeply into evaluating models and selecting parameters.\n",
      "We will focus on the supervised methods, regression and classification, as evaluating\n",
      "and selecting models in unsupervised learning is often a very qualitative process (as\n",
      "we saw in Chapter 3).\n",
      "To evaluate our supervised models, so far we have split our dataset into a training set\n",
      "and a test set using the train_test_split function, built a model on the training set\n",
      "by calling the fit method, and evaluated it on the test set using the score method,\n",
      "which for classification computes the fraction of correctly classified samples. Here’s\n",
      "an example of that process:\n",
      "In[2]:\n",
      "from sklearn.datasets import make_blobs\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "# create a synthetic dataset\n",
      "X, y = make_blobs(random_state=0)\n",
      "# split data and labels into a training and a test set\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
      "# instantiate a model and fit it to the training set\n",
      "logreg = LogisticRegression().fit(X_train, y_train)\n",
      "# evaluate the model on the test set\n",
      "print(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\n",
      "Out[2]:\n",
      "Test set score: 0.88\n",
      "251\n",
      "Remember, the reason we split our data into training and test sets is that we are inter‐\n",
      "ested in measuring how well our model generalizes to new, previously unseen data.\n",
      "We are not interested in how well our model fit the training set, but rather in how\n",
      "well it can make predictions for data that was not observed during training.\n",
      "In this chapter, we will expand on two aspects of this evaluation. We will first intro‐\n",
      "duce cross-validation, a more robust way to assess generalization performance, and\n",
      "discuss methods to evaluate classification and regression performance that go beyond\n",
      "the default measures of accuracy and R2 provided by the score method.\n",
      "We will also discuss grid search, an effective method for adjusting the parameters in\n",
      "supervised models for the best generalization performance.\n",
      "Cross-Validation\n",
      "Cross-validation is a statistical method of evaluating generalization performance that\n",
      "is more stable and thorough than using a split into a training and a test set. In cross-\n",
      "validation, the data is instead split repeatedly and multiple models are trained. The\n",
      "most commonly used version of cross-validation is k-fold cross-validation, where k is\n",
      "a user-specified number, usually 5 or 10. When performing five-fold cross-validation,\n",
      "the data is first partitioned into five parts of (approximately) equal size, called folds.\n",
      "Next, a sequence of models is trained. The first model is trained using the first fold as\n",
      "the test set, and the remaining folds (2–5) are used as the training set. The model is\n",
      "built using the data in folds 2–5, and then the accuracy is evaluated on fold 1. Then\n",
      "another model is built, this time using fold 2 as the test set and the data in folds 1, 3,\n",
      "4, and 5 as the training set. This process is repeated using folds 3, 4, and 5 as test sets.\n",
      "For each of these five splits of the data into training and test sets, we compute the\n",
      "accuracy. In the end, we have collected five accuracy values. The process is illustrated\n",
      "in Figure 5-1:\n",
      "In[3]:\n",
      "mglearn.plots.plot_cross_validation()\n",
      "Figure 5-1. Data splitting in five-fold cross-validation\n",
      "Usually, the first fifth of the data is the first fold, the second fifth of the data is the\n",
      "second fold, and so on.\n",
      "252 | Chapter 5: Model Evaluation and Improvement\n",
      "Cross-Validation in scikit-learn\n",
      "Cross-validation is implemented in scikit-learn using the cross_val_score func‐\n",
      "tion from the model_selection module. The parameters of the cross_val_score\n",
      "function are the model we want to evaluate, the training data, and the ground-truth\n",
      "labels. Let’s evaluate LogisticRegression on the iris dataset:\n",
      "In[4]:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "iris = load_iris()\n",
      "logreg = LogisticRegression()\n",
      "scores = cross_val_score(logreg, iris.data, iris.target)\n",
      "print(\"Cross-validation scores: {}\".format(scores))\n",
      "Out[4]:\n",
      "Cross-validation scores: [ 0.961  0.922  0.958]\n",
      "By default, cross_val_score performs three-fold cross-validation, returning three\n",
      "accuracy values. We can change the number of folds used by changing the cv parame‐\n",
      "ter:\n",
      "In[5]:\n",
      "scores = cross_val_score(logreg, iris.data, iris.target, cv=5)\n",
      "print(\"Cross-validation scores: {}\".format(scores))\n",
      "Out[5]:\n",
      "Cross-validation scores: [ 1.     0.967  0.933  0.9    1.   ]\n",
      "A common way to summarize the cross-validation accuracy is to compute the mean:\n",
      "In[6]:\n",
      "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\n",
      "Out[6]:\n",
      "Average cross-validation score: 0.96\n",
      "Using the mean cross-validation we can conclude that we expect the model to be\n",
      "around 96% accurate on average. Looking at all five scores produced by the five-fold\n",
      "cross-validation, we can also conclude that there is a relatively high variance in the\n",
      "accuracy between folds, ranging from 100% accuracy to 90% accuracy. This could\n",
      "imply that the model is very dependent on the particular folds used for training, but it\n",
      "could also just be a consequence of the small size of the dataset.\n",
      "Cross-Validation | 253\n",
      "Benefits of Cross-Validation\n",
      "There are several benefits to using cross-validation instead of a single split into a\n",
      "training and a test set. First, remember that train_test_split performs a random\n",
      "split of the data. Imagine that we are “lucky” when randomly splitting the data, and\n",
      "all examples that are hard to classify end up in the training set. In that case, the test\n",
      "set will only contain “easy” examples, and our test set accuracy will be unrealistically\n",
      "high. Conversely, if we are “unlucky, ” we might have randomly put all the hard-to-\n",
      "classify examples in the test set and consequently obtain an unrealistically low score.\n",
      "However, when using cross-validation, each example will be in the training set exactly\n",
      "once: each example is in one of the folds, and each fold is the test set once. Therefore,\n",
      "the model needs to generalize well to all of the samples in the dataset for all of the\n",
      "cross-validation scores (and their mean) to be high.\n",
      "Having multiple splits of the data also provides some information about how sensi‐\n",
      "tive our model is to the selection of the training dataset. For the iris dataset, we saw\n",
      "accuracies between 90% and 100%. This is quite a range, and it provides us with an\n",
      "idea about how the model might perform in the worst case and best case scenarios\n",
      "when applied to new data.\n",
      "Another benefit of cross-validation as compared to using a single split of the data is\n",
      "that we use our data more effectively. When using train_test_split, we usually use\n",
      "75% of the data for training and 25% of the data for evaluation. When using five-fold\n",
      "cross-validation, in each iteration we can use four-fifths of the data (80%) to fit the\n",
      "model. When using 10-fold cross-validation, we can use nine-tenths of the data\n",
      "(90%) to fit the model. More data will usually result in more accurate models.\n",
      "The main disadvantage of cross-validation is increased computational cost. As we are\n",
      "now training k models instead of a single model, cross-validation will be roughly k\n",
      "times slower than doing a single split of the data.\n",
      "It is important to keep in mind that cross-validation is not a way to\n",
      "build a model that can be applied to new data. Cross-validation\n",
      "does not return a model. When calling cross_val_score, multiple\n",
      "models are built internally, but the purpose of cross-validation is\n",
      "only to evaluate how well a given algorithm will generalize when\n",
      "trained on a specific dataset.\n",
      "Stratified k-Fold Cross-Validation and Other Strategies\n",
      "Splitting the dataset into k folds by starting with the first one- k-th part of the data, as\n",
      "described in the previous section, might not always be a good idea. For example, let’s\n",
      "have a look at the iris dataset:\n",
      "254 | Chapter 5: Model Evaluation and Improvement\n",
      "In[7]:\n",
      "from sklearn.datasets import load_iris\n",
      "iris = load_iris()\n",
      "print(\"Iris labels:\\n{}\".format(iris.target))\n",
      "Out[7]:\n",
      "Iris labels:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "As you can see, the first third of the data is the class 0, the second third is the class 1,\n",
      "and the last third is the class 2. Imagine doing three-fold cross-validation on this\n",
      "dataset. The first fold would be only class 0, so in the first split of the data, the test set\n",
      "would be only class 0, and the training set would be only classes 1 and 2. As the\n",
      "classes in training and test sets would be different for all three splits, the three-fold\n",
      "cross-validation accuracy would be zero on this dataset. That is not very helpful, as\n",
      "we can do much better than 0% accuracy on iris.\n",
      "As the simple k-fold strategy fails here, scikit-learn does not use it for classifica‐\n",
      "tion, but rather uses stratified k-fold cross-validation. In stratified cross-validation, we\n",
      "split the data such that the proportions between classes are the same in each fold as\n",
      "they are in the whole dataset, as illustrated in Figure 5-2:\n",
      "In[8]:\n",
      "mglearn.plots.plot_stratified_cross_validation()\n",
      "Figure 5-2. Comparison of standard cross-validation and stratified cross-validation\n",
      "when the data is ordered by class label\n",
      "Cross-Validation | 255\n",
      "For example, if 90% of your samples belong to class A and 10% of your samples\n",
      "belong to class B, then stratified cross-validation ensures that in each fold, 90% of\n",
      "samples belong to class A and 10% of samples belong to class B.\n",
      "It is usually a good idea to use stratified k-fold cross-validation instead of k-fold\n",
      "cross-validation to evaluate a classifier, because it results in more reliable estimates of\n",
      "generalization performance. In the case of only 10% of samples belonging to class B,\n",
      "using standard k-fold cross-validation it might easily happen that one fold only con‐\n",
      "tains samples of class A. Using this fold as a test set would not be very informative\n",
      "about the overall performance of the classifier.\n",
      "For regression, scikit-learn uses the standard k-fold cross-validation by default. It\n",
      "would be possible to also try to make each fold representative of the different values\n",
      "the regression target has, but this is not a commonly used strategy and would be sur‐\n",
      "prising to most users.\n",
      "More control over cross-validation\n",
      "We saw earlier that we can adjust the number of folds that are used in\n",
      "cross_val_score using the cv parameter. However, scikit-learn allows for much\n",
      "finer control over what happens during the splitting of the data by providing a cross-\n",
      "validation splitter as the cv parameter. For most use cases, the defaults of k-fold cross-\n",
      "validation for regression and stratified k-fold for classification work well, but there\n",
      "are some cases where you might want to use a different strategy. Say, for example, we\n",
      "want to use the standard k-fold cross-validation on a classification dataset to repro‐\n",
      "duce someone else’s results. To do this, we first have to import the KFold splitter class\n",
      "from the model_selection module and instantiate it with the number of folds we\n",
      "want to use:\n",
      "In[9]:\n",
      "from sklearn.model_selection import KFold\n",
      "kfold = KFold(n_splits=5)\n",
      "Then, we can pass the kfold splitter object as the cv parameter to cross_val_score:\n",
      "In[10]:\n",
      "print(\"Cross-validation scores:\\n{}\".format(\n",
      "      cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\n",
      "Out[10]:\n",
      "Cross-validation scores:\n",
      "[ 1.     0.933  0.433  0.967  0.433]\n",
      "This way, we can verify that it is indeed a really bad idea to use three-fold (nonstrati‐\n",
      "fied) cross-validation on the iris dataset:\n",
      "256 | Chapter 5: Model Evaluation and Improvement\n",
      "In[11]:\n",
      "kfold = KFold(n_splits=3)\n",
      "print(\"Cross-validation scores:\\n{}\".format(\n",
      "    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\n",
      "Out[11]:\n",
      "Cross-validation scores:\n",
      "[ 0.  0.  0.]\n",
      "Remember: each fold corresponds to one of the classes in the iris dataset, and so\n",
      "nothing can be learned. Another way to resolve this problem is to shuffle the data\n",
      "instead of stratifying the folds, to remove the ordering of the samples by label. We can\n",
      "do that by setting the shuffle parameter of KFold to True. If we shuffle the data, we\n",
      "also need to fix the random_state to get a reproducible shuffling. Otherwise, each run\n",
      "of cross_val_score would yield a different result, as each time a different split would\n",
      "be used (this might not be a problem, but can be surprising). Shuffling the data before\n",
      "splitting it yields a much better result:\n",
      "In[12]:\n",
      "kfold = KFold(n_splits=3, shuffle=True, random_state=0)\n",
      "print(\"Cross-validation scores:\\n{}\".format(\n",
      "    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\n",
      "Out[12]:\n",
      "Cross-validation scores:\n",
      "[ 0.9   0.96  0.96]\n",
      "Leave-one-out cross-validation\n",
      "Another frequently used cross-validation method is leave-one-out. Y ou can think of\n",
      "leave-one-out cross-validation as k-fold cross-validation where each fold is a single\n",
      "sample. For each split, you pick a single data point to be the test set. This can be very\n",
      "time consuming, particularly for large datasets, but sometimes provides better esti‐\n",
      "mates on small datasets:\n",
      "In[13]:\n",
      "from sklearn.model_selection import LeaveOneOut\n",
      "loo = LeaveOneOut()\n",
      "scores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\n",
      "print(\"Number of cv iterations: \", len(scores))\n",
      "print(\"Mean accuracy: {:.2f}\".format(scores.mean()))\n",
      "Out[13]:\n",
      "Number of cv iterations:  150\n",
      "Mean accuracy: 0.95\n",
      "Cross-Validation | 257\n",
      "Shuffle-split cross-validation\n",
      "Another, very flexible strategy for cross-validation is shuffle-split cross-validation. In\n",
      "shuffle-split cross-validation, each split samples train_size many points for the\n",
      "training set and test_size many (disjoint) point for the test set. This splitting is\n",
      "repeated n_iter times. Figure 5-3  illustrates running four iterations of splitting a\n",
      "dataset consisting of 10 points, with a training set of 5 points and test sets of 2 points\n",
      "each (you can use integers for train_size and test_size to use absolute sizes for\n",
      "these sets, or floating-point numbers to use fractions of the whole dataset):\n",
      "In[14]:\n",
      "mglearn.plots.plot_shuffle_split()\n",
      "Figure 5-3. ShuffleSplit with 10 points, train_size=5, test_size=2, and n_iter=4\n",
      "The following code splits the dataset into 50% training set and 50% test set for 10\n",
      "iterations:\n",
      "In[15]:\n",
      "from sklearn.model_selection import ShuffleSplit\n",
      "shuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\n",
      "scores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)\n",
      "print(\"Cross-validation scores:\\n{}\".format(scores))\n",
      "Out[15]:\n",
      "Cross-validation scores:\n",
      "[ 0.96   0.907  0.947  0.96   0.96   0.907  0.893  0.907  0.92   0.973]\n",
      "Shuffle-split cross-validation allows for control over the number of iterations inde‐\n",
      "pendently of the training and test sizes, which can sometimes be helpful. It also allows\n",
      "for using only part of the data in each iteration, by providing train_size and\n",
      "test_size settings that don’t add up to one. Subsampling the data in this way can be\n",
      "useful for experimenting with large datasets.\n",
      "There is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS\n",
      "plit, which can provide more reliable results for classification tasks.\n",
      "258 | Chapter 5: Model Evaluation and Improvement\n",
      "Cross-validation with groups\n",
      "Another very common setting for cross-validation is when there are groups in the\n",
      "data that are highly related. Say you want to build a system to recognize emotions\n",
      "from pictures of faces, and you collect a dataset of pictures of 100 people where each\n",
      "person is captured multiple times, showing various emotions. The goal is to build a\n",
      "classifier that can correctly identify emotions of people not in the dataset. Y ou could\n",
      "use the default stratified cross-validation to measure the performance of a classifier\n",
      "here. However, it is likely that pictures of the same person will be in both the training\n",
      "and the test set. It will be much easier for a classifier to detect emotions in a face that\n",
      "is part of the training set, compared to a completely new face. To accurately evaluate\n",
      "the generalization to new faces, we must therefore ensure that the training and test\n",
      "sets contain images of different people.\n",
      "To achieve this, we can use GroupKFold, which takes an array of groups as argument\n",
      "that we can use to indicate which person is in the image. The groups array here indi‐\n",
      "cates groups in the data that should not be split when creating the training and test\n",
      "sets, and should not be confused with the class label.\n",
      "This example of groups in the data is common in medical applications, where you\n",
      "might have multiple samples from the same patient, but are interested in generalizing\n",
      "to new patients. Similarly, in speech recognition, you might have multiple recordings\n",
      "of the same speaker in your dataset, but are interested in recognizing speech of new\n",
      "speakers.\n",
      "The following is an example of using a synthetic dataset with a grouping given by the\n",
      "groups array. The dataset consists of 12 data points, and for each of the data points,\n",
      "groups specifies which group (think patient) the point belongs to. The groups specify\n",
      "that there are four groups, and the first three samples belong to the first group, the\n",
      "next four samples belong to the second group, and so on:\n",
      "In[17]:\n",
      "from sklearn.model_selection import GroupKFold\n",
      "# create synthetic dataset\n",
      "X, y = make_blobs(n_samples=12, random_state=0)\n",
      "# assume the first three samples belong to the same group,\n",
      "# then the next four, etc.\n",
      "groups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]\n",
      "scores = cross_val_score(logreg, X, y, groups, cv=GroupKFold(n_splits=3))\n",
      "print(\"Cross-validation scores:\\n{}\".format(scores))\n",
      "Out[17]:\n",
      "Cross-validation scores:\n",
      "[ 0.75   0.8    0.667]\n",
      "The samples don’t need to be ordered by group; we just did this for illustration pur‐\n",
      "poses. The splits that are calculated based on these labels are visualized in Figure 5-4.\n",
      "Cross-Validation | 259\n",
      "As you can see, for each split, each group is either entirely in the training set or\n",
      "entirely in the test set:\n",
      "In[16]:\n",
      "mglearn.plots.plot_label_kfold()\n",
      "Figure 5-4. Label-dependent splitting with GroupKFold\n",
      "There are more splitting strategies for cross-validation in scikit-learn, which allow\n",
      "for an even greater variety of use cases (you can find these in the scikit-learn user\n",
      "guide). However, the standard KFold, StratifiedKFold, and GroupKFold are by far\n",
      "the most commonly used ones.\n",
      "Grid Search\n",
      "Now that we know how to evaluate how well a model generalizes, we can take the\n",
      "next step and improve the model’s generalization performance by tuning its parame‐\n",
      "ters. We discussed the parameter settings of many of the algorithms in scikit-learn\n",
      "in Chapters 2 and 3, and it is important to understand what the parameters mean\n",
      "before trying to adjust them. Finding the values of the important parameters of a\n",
      "model (the ones that provide the best generalization performance) is a tricky task, but\n",
      "necessary for almost all models and datasets. Because it is such a common task, there\n",
      "are standard methods in scikit-learn to help you with it. The most commonly used\n",
      "method is grid search, which basically means trying all possible combinations of the\n",
      "parameters of interest.\n",
      "Consider the case of a kernel SVM with an RBF (radial basis function) kernel, as\n",
      "implemented in the SVC class. As we discussed in Chapter 2, there are two important\n",
      "parameters: the kernel bandwidth, gamma, and the regularization parameter, C. Say we\n",
      "want to try the values 0.001, 0.01, 0.1, 1, 10, and 100 for the parameter C, and the\n",
      "same for gamma. Because we have six different settings for C and gamma that we want to\n",
      "try, we have 36 combinations of parameters in total. Looking at all possible combina‐\n",
      "tions creates a table (or grid) of parameter settings for the SVM, as shown here:\n",
      "260 | Chapter 5: Model Evaluation and Improvement\n",
      "C = 0.001 C = 0.01 … C = 10\n",
      "gamma=0.001 SVC(C=0.001, gamma=0.001) SVC(C=0.01, gamma=0.001) … SVC(C=10, gamma=0.001)\n",
      "gamma=0.01 SVC(C=0.001, gamma=0.01) SVC(C=0.01, gamma=0.01) … SVC(C=10, gamma=0.01)\n",
      "… … … … …\n",
      "gamma=100 SVC(C=0.001, gamma=100) SVC(C=0.01, gamma=100) … SVC(C=10, gamma=100)\n",
      "Simple Grid Search\n",
      "We can implement a simple grid search just as for loops over the two parameters,\n",
      "training and evaluating a classifier for each combination:\n",
      "In[18]:\n",
      "# naive grid search implementation\n",
      "from sklearn.svm import SVC\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    iris.data, iris.target, random_state=0)\n",
      "print(\"Size of training set: {}   size of test set: {}\".format(\n",
      "      X_train.shape[0], X_test.shape[0]))\n",
      "best_score = 0\n",
      "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
      "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
      "        # for each combination of parameters, train an SVC\n",
      "        svm = SVC(gamma=gamma, C=C)\n",
      "        svm.fit(X_train, y_train)\n",
      "        # evaluate the SVC on the test set\n",
      "        score = svm.score(X_test, y_test)\n",
      "        # if we got a better score, store the score and parameters\n",
      "        if score > best_score:\n",
      "            best_score = score\n",
      "            best_parameters = {'C': C, 'gamma': gamma}\n",
      "print(\"Best score: {:.2f}\".format(best_score))\n",
      "print(\"Best parameters: {}\".format(best_parameters))\n",
      "Out[18]:\n",
      "Size of training set: 112   size of test set: 38\n",
      "Best score: 0.97\n",
      "Best parameters: {'C': 100, 'gamma': 0.001}\n",
      "The Danger of Overfitting the Parameters and the Validation Set\n",
      "Given this result, we might be tempted to report that we found a model that performs\n",
      "with 97% accuracy on our dataset. However, this claim could be overly optimistic (or\n",
      "just wrong), for the following reason: we tried many different parameters and\n",
      "Grid Search | 261\n",
      "selected the one with best accuracy on the test set, but this accuracy won’t necessarily\n",
      "carry over to new data. Because we used the test data to adjust the parameters, we can\n",
      "no longer use it to assess how good the model is. This is the same reason we needed\n",
      "to split the data into training and test sets in the first place; we need an independent\n",
      "dataset to evaluate, one that was not used to create the model.\n",
      "One way to resolve this problem is to split the data again, so we have three sets: the\n",
      "training set to build the model, the validation (or development) set to select the\n",
      "parameters of the model, and the test set to evaluate the performance of the selected\n",
      "parameters. Figure 5-5 shows what this looks like:\n",
      "In[19]:\n",
      "mglearn.plots.plot_threefold_split()\n",
      "Figure 5-5. A threefold split of data into training set, validation set, and test set\n",
      "After selecting the best parameters using the validation set, we can rebuild a model\n",
      "using the parameter settings we found, but now training on both the training data\n",
      "and the validation data. This way, we can use as much data as possible to build our\n",
      "model. This leads to the following implementation:\n",
      "In[20]:\n",
      "from sklearn.svm import SVC\n",
      "# split data into train+validation set and test set\n",
      "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
      "    iris.data, iris.target, random_state=0)\n",
      "# split train+validation set into training and validation sets\n",
      "X_train, X_valid, y_train, y_valid = train_test_split(\n",
      "    X_trainval, y_trainval, random_state=1)\n",
      "print(\"Size of training set: {}   size of validation set: {}   size of test set:\"\n",
      "      \" {}\\n\".format(X_train.shape[0], X_valid.shape[0], X_test.shape[0]))\n",
      "best_score = 0\n",
      "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
      "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
      "        # for each combination of parameters, train an SVC\n",
      "        svm = SVC(gamma=gamma, C=C)\n",
      "        svm.fit(X_train, y_train)\n",
      "        # evaluate the SVC on the test set\n",
      "        score = svm.score(X_valid, y_valid)\n",
      "        # if we got a better score, store the score and parameters\n",
      "        if score > best_score:\n",
      "            best_score = score\n",
      "            best_parameters = {'C': C, 'gamma': gamma}\n",
      "262 | Chapter 5: Model Evaluation and Improvement\n",
      "# rebuild a model on the combined training and validation set,\n",
      "# and evaluate it on the test set\n",
      "svm = SVC(**best_parameters)\n",
      "svm.fit(X_trainval, y_trainval)\n",
      "test_score = svm.score(X_test, y_test)\n",
      "print(\"Best score on validation set: {:.2f}\".format(best_score))\n",
      "print(\"Best parameters: \", best_parameters)\n",
      "print(\"Test set score with best parameters: {:.2f}\".format(test_score))\n",
      "Out[20]:\n",
      "Size of training set: 84   size of validation set: 28   size of test set: 38\n",
      "Best score on validation set: 0.96\n",
      "Best parameters:  {'C': 10, 'gamma': 0.001}\n",
      "Test set score with best parameters: 0.92\n",
      "The best score on the validation set is 96%: slightly lower than before, probably\n",
      "because we used less data to train the model ( X_train is smaller now because we split\n",
      "our dataset twice). However, the score on the test set—the score that actually tells us\n",
      "how well we generalize—is even lower, at 92%. So we can only claim to classify new\n",
      "data 92% correctly, not 97% correctly as we thought before!\n",
      "The distinction between the training set, validation set, and test set is fundamentally\n",
      "important to applying machine learning methods in practice. Any choices made\n",
      "based on the test set accuracy “leak” information from the test set into the model.\n",
      "Therefore, it is important to keep a separate test set, which is only used for the final\n",
      "evaluation. It is good practice to do all exploratory analysis and model selection using\n",
      "the combination of a training and a validation set, and reserve the test set for a final\n",
      "evaluation—this is even true for exploratory visualization. Strictly speaking, evaluat‐\n",
      "ing more than one model on the test set and choosing the better of the two will result\n",
      "in an overly optimistic estimate of how accurate the model is.\n",
      "Grid Search with Cross-Validation\n",
      "While the method of splitting the data into a training, a validation, and a test set that\n",
      "we just saw is workable, and relatively commonly used, it is quite sensitive to how\n",
      "exactly the data is split. From the output of the previous code snippet we can see that\n",
      "GridSearchCV selects 'C': 10, 'gamma': 0.001  as the best parameters, while the\n",
      "output of the code in the previous section selects 'C': 100, 'gamma': 0.001 as the\n",
      "best parameters. For a better estimate of the generalization performance, instead of\n",
      "using a single split into a training and a validation set, we can use cross-validation to\n",
      "evaluate the performance of each parameter combination. This method can be coded\n",
      "up as follows:\n",
      "Grid Search | 263\n",
      "In[21]:\n",
      "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
      "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
      "        # for each combination of parameters,\n",
      "        # train an SVC\n",
      "        svm = SVC(gamma=gamma, C=C)\n",
      "        # perform cross-validation\n",
      "        scores = cross_val_score(svm, X_trainval, y_trainval, cv=5)\n",
      "        # compute mean cross-validation accuracy\n",
      "        score = np.mean(scores)\n",
      "        # if we got a better score, store the score and parameters\n",
      "        if score > best_score:\n",
      "            best_score = score\n",
      "            best_parameters = {'C': C, 'gamma': gamma}\n",
      "# rebuild a model on the combined training and validation set\n",
      "svm = SVC(**best_parameters)\n",
      "svm.fit(X_trainval, y_trainval)\n",
      "To evaluate the accuracy of the SVM using a particular setting of C and gamma using\n",
      "five-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine,\n",
      "the main downside of the use of cross-validation is the time it takes to train all these\n",
      "models.\n",
      "The following visualization ( Figure 5-6) illustrates how the best parameter setting is\n",
      "selected in the preceding code:\n",
      "In[22]:\n",
      "mglearn.plots.plot_cross_val_selection()\n",
      "Figure 5-6. Results of grid search with cross-validation\n",
      "For each parameter setting (only a subset is shown), five accuracy values are compu‐\n",
      "ted, one for each split in the cross-validation. Then the mean validation accuracy is\n",
      "computed for each parameter setting. The parameters with the highest mean valida‐\n",
      "tion accuracy are chosen, marked by the circle.\n",
      "264 | Chapter 5: Model Evaluation and Improvement\n",
      "As we said earlier, cross-validation is a way to evaluate a given algo‐\n",
      "rithm on a specific dataset. However, it is often used in conjunction\n",
      "with parameter search methods like grid search. For this reason,\n",
      "many people use the term cross-validation colloquially to refer to\n",
      "grid search with cross-validation.\n",
      "The overall process of splitting the data, running the grid search, and evaluating the\n",
      "final parameters is illustrated in Figure 5-7:\n",
      "In[23]:\n",
      "mglearn.plots.plot_grid_search_overview()\n",
      "Figure 5-7. Overview of the process of parameter selection and model evaluation with\n",
      "GridSearchCV\n",
      "Because grid search with cross-validation is such a commonly used method to adjust\n",
      "parameters, scikit-learn provides the GridSearchCV class, which implements it in\n",
      "the form of an estimator. To use the GridSearchCV class, you first need to specify the\n",
      "parameters you want to search over using a dictionary. GridSearchCV will then per‐\n",
      "form all the necessary model fits. The keys of the dictionary are the names of parame‐\n",
      "ters we want to adjust (as given when constructing the model—in this case, C and\n",
      "gamma), and the values are the parameter settings we want to try out. Trying the val‐\n",
      "ues 0.001, 0.01, 0.1, 1, 10, and 100 for C and gamma translates to the following\n",
      "dictionary:\n",
      "In[24]:\n",
      "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
      "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
      "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
      "Out[24]:\n",
      "Parameter grid:\n",
      "{'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
      "Grid Search | 265\n",
      "1 A scikit-learn estimator that is created using another estimator is called a meta-estimator. GridSearchCV is\n",
      "the most commonly used meta-estimator, but we will see more later.\n",
      "We can now instantiate the GridSearchCV class with the model ( SVC), the parameter\n",
      "grid to search ( param_grid), and the cross-validation strategy we want to use (say,\n",
      "five-fold stratified cross-validation):\n",
      "In[25]:\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.svm import SVC\n",
      "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
      "GridSearchCV will use cross-validation in place of the split into a training and valida‐\n",
      "tion set that we used before. However, we still need to split the data into a training\n",
      "and a test set, to avoid overfitting the parameters:\n",
      "In[26]:\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    iris.data, iris.target, random_state=0)\n",
      "The grid_search object that we created behaves just like a classifier; we can call the\n",
      "standard methods fit, predict, and score on it.1 However, when we call fit, it will\n",
      "run cross-validation for each combination of parameters we specified in param_grid:\n",
      "In[27]:\n",
      "grid_search.fit(X_train, y_train)\n",
      "Fitting the GridSearchCV object not only searches for the best parameters, but also\n",
      "automatically fits a new model on the whole training dataset with the parameters that\n",
      "yielded the best cross-validation performance. What happens in fit is therefore\n",
      "equivalent to the result of the In[21] code we saw at the beginning of this section. The\n",
      "GridSearchCV class provides a very convenient interface to access the retrained\n",
      "model using the predict and score methods. To evaluate how well the best found\n",
      "parameters generalize, we can call score on the test set:\n",
      "In[28]:\n",
      "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))\n",
      "Out[28]:\n",
      "Test set score: 0.97\n",
      "Choosing the parameters using cross-validation, we actually found a model that ach‐\n",
      "ieves 97% accuracy on the test set. The important thing here is that we did not use the\n",
      "test set to choose the parameters. The parameters that were found are scored in the\n",
      "266 | Chapter 5: Model Evaluation and Improvement\n",
      "best_params_ attribute, and the best cross-validation accuracy (the mean accuracy\n",
      "over the different splits for this parameter setting) is stored in best_score_:\n",
      "In[29]:\n",
      "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
      "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
      "Out[29]:\n",
      "Best parameters: {'C': 100, 'gamma': 0.01}\n",
      "Best cross-validation score: 0.97\n",
      "Again, be careful not to confuse best_score_ with the generaliza‐\n",
      "tion performance of the model as computed by the score method\n",
      "on the test set. Using the score method (or evaluating the output of\n",
      "the predict method) employs a model trained on the whole train‐\n",
      "ing set. The best_score_ attribute stores the mean cross-validation\n",
      "accuracy, with cross-validation performed on the training set.\n",
      "Sometimes it is helpful to have access to the actual model that was found—for exam‐\n",
      "ple, to look at coefficients or feature importances. Y ou can access the model with the\n",
      "best parameters trained on the whole training set using the best_estimator_\n",
      "attribute:\n",
      "In[30]:\n",
      "print(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))\n",
      "Out[30]:\n",
      "Best estimator:\n",
      "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
      "   decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "   tol=0.001, verbose=False)\n",
      "Because grid_search itself has predict and score methods, using best_estimator_\n",
      "is not needed to make predictions or evaluate the model.\n",
      "Analyzing the result of cross-validation\n",
      "It is often helpful to visualize the results of cross-validation, to understand how the\n",
      "model generalization depends on the parameters we are searching. As grid searches\n",
      "are quite computationally expensive to run, often it is a good idea to start with a rela‐\n",
      "tively coarse and small grid. We can then inspect the results of the cross-validated\n",
      "grid search, and possibly expand our search. The results of a grid search can be found\n",
      "in the cv_results_ attribute, which is a dictionary storing all aspects of the search. It\n",
      "Grid Search | 267\n",
      "contains a lot of details, as you can see in the following output, and is best looked at\n",
      "after converting it to a pandas DataFrame:\n",
      "In[31]:\n",
      "import pandas as pd\n",
      "# convert to DataFrame\n",
      "results = pd.DataFrame(grid_search.cv_results_)\n",
      "# show the first 5 rows\n",
      "display(results.head())\n",
      "Out[31]:\n",
      "    param_C   param_gamma   params                        mean_test_score\n",
      "0   0.001     0.001         {'C': 0.001, 'gamma': 0.001}       0.366\n",
      "1   0.001      0.01         {'C': 0.001, 'gamma': 0.01}        0.366\n",
      "2   0.001       0.1         {'C': 0.001, 'gamma': 0.1}         0.366\n",
      "3   0.001         1         {'C': 0.001, 'gamma': 1}           0.366\n",
      "4   0.001        10         {'C': 0.001, 'gamma': 10}          0.366\n",
      "       rank_test_score  split0_test_score  split1_test_score  split2_test_score\n",
      "0               22              0.375           0.347           0.363\n",
      "1               22              0.375           0.347           0.363\n",
      "2               22              0.375           0.347           0.363\n",
      "3               22              0.375           0.347           0.363\n",
      "4               22              0.375           0.347           0.363\n",
      "       split3_test_score  split4_test_score  std_test_score\n",
      "0           0.363              0.380           0.011\n",
      "1           0.363              0.380           0.011\n",
      "2           0.363              0.380           0.011\n",
      "3           0.363              0.380           0.011\n",
      "4           0.363              0.380           0.011\n",
      "Each row in results corresponds to one particular parameter setting. For each set‐\n",
      "ting, the results of all cross-validation splits are recorded, as well as the mean and\n",
      "standard deviation over all splits. As we were searching a two-dimensional grid of\n",
      "parameters (C and gamma), this is best visualized as a heat map ( Figure 5-8). First we\n",
      "extract the mean validation scores, then we reshape the scores so that the axes corre‐\n",
      "spond to C and gamma:\n",
      "In[32]:\n",
      "scores = np.array(results.mean_test_score).reshape(6, 6)\n",
      "# plot the mean cross-validation scores\n",
      "mglearn.tools.heatmap(scores, xlabel='gamma', xticklabels=param_grid['gamma'],\n",
      "                      ylabel='C', yticklabels=param_grid['C'], cmap=\"viridis\")\n",
      "268 | Chapter 5: Model Evaluation and Improvement\n",
      "Figure 5-8. Heat map of mean cross-validation score as a function of C and gamma\n",
      "Each point in the heat map corresponds to one run of cross-validation, with a partic‐\n",
      "ular parameter setting. The color encodes the cross-validation accuracy, with light\n",
      "colors meaning high accuracy and dark colors meaning low accuracy. Y ou can see\n",
      "that SVC is very sensitive to the setting of the parameters. For many of the parameter\n",
      "settings, the accuracy is around 40%, which is quite bad; for other settings the accu‐\n",
      "racy is around 96%. We can take away from this plot several things. First, the parame‐\n",
      "ters we adjusted are very important for obtaining good performance. Both parameters\n",
      "(C and gamma) matter a lot, as adjusting them can change the accuracy from 40% to\n",
      "96%. Additionally, the ranges we picked for the parameters are ranges in which we\n",
      "see significant changes in the outcome. It’s also important to note that the ranges for\n",
      "the parameters are large enough: the optimum values for each parameter are not on\n",
      "the edges of the plot.\n",
      "Now let’s look at some plots (shown in Figure 5-9 ) where the result is less ideal,\n",
      "because the search ranges were not chosen properly:\n",
      "Figure 5-9. Heat map visualizations of misspecified search grids\n",
      "Grid Search | 269\n",
      "In[33]:\n",
      "fig, axes = plt.subplots(1, 3, figsize=(13, 5))\n",
      "param_grid_linear = {'C': np.linspace(1, 2, 6),\n",
      "                     'gamma':  np.linspace(1, 2, 6)}\n",
      "param_grid_one_log = {'C': np.linspace(1, 2, 6),\n",
      "                      'gamma':  np.logspace(-3, 2, 6)}\n",
      "param_grid_range = {'C': np.logspace(-3, 2, 6),\n",
      "                    'gamma':  np.logspace(-7, -2, 6)}\n",
      "for param_grid, ax in zip([param_grid_linear, param_grid_one_log,\n",
      "                           param_grid_range], axes):\n",
      "    grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
      "    grid_search.fit(X_train, y_train)\n",
      "    scores = grid_search.cv_results_['mean_test_score'].reshape(6, 6)\n",
      "    # plot the mean cross-validation scores\n",
      "    scores_image = mglearn.tools.heatmap(\n",
      "        scores, xlabel='gamma', ylabel='C', xticklabels=param_grid['gamma'],\n",
      "        yticklabels=param_grid['C'], cmap=\"viridis\", ax=ax)\n",
      "plt.colorbar(scores_image, ax=axes.tolist())\n",
      "The first panel shows no changes at all, with a constant color over the whole parame‐\n",
      "ter grid. In this case, this is caused by improper scaling and range of the parameters C\n",
      "and gamma. However, if no change in accuracy is visible over the different parameter\n",
      "settings, it could also be that a parameter is just not important at all. It is usually good\n",
      "to try very extreme values first, to see if there are any changes in the accuracy as a\n",
      "result of changing a parameter.\n",
      "The second panel shows a vertical stripe pattern. This indicates that only the setting\n",
      "of the gamma parameter makes any difference. This could mean that the gamma param‐\n",
      "eter is searching over interesting values but the C parameter is not—or it could mean\n",
      "the C parameter is not important.\n",
      "The third panel shows changes in both C and gamma. However, we can see that in the\n",
      "entire bottom left of the plot, nothing interesting is happening. We can probably\n",
      "exclude the very small values from future grid searches. The optimum parameter set‐\n",
      "ting is at the top right. As the optimum is in the border of the plot, we can expect that\n",
      "there might be even better values beyond this border, and we might want to change\n",
      "our search range to include more parameters in this region.\n",
      "Tuning the parameter grid based on the cross-validation scores is perfectly fine, and a\n",
      "good way to explore the importance of different parameters. However, you should\n",
      "not test different parameter ranges on the final test set—as we discussed earlier, eval‐\n",
      "270 | Chapter 5: Model Evaluation and Improvement\n",
      "uation of the test set should happen only once we know exactly what model we want\n",
      "to use.\n",
      "Search over spaces that are not grids\n",
      "In some cases, trying all possible combinations of all parameters as GridSearchCV\n",
      "usually does, is not a good idea. For example, SVC has a kernel parameter, and\n",
      "depending on which kernel is chosen, other parameters will be relevant. If ker\n",
      "nel='linear', the model is linear, and only the C parameter is used. If kernel='rbf',\n",
      "both the C and gamma parameters are used (but not other parameters like degree). In\n",
      "this case, searching over all possible combinations of C, gamma, and kernel wouldn’t\n",
      "make sense: if kernel='linear', gamma is not used, and trying different values for\n",
      "gamma would be a waste of time. To deal with these kinds of “conditional” parameters,\n",
      "GridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the\n",
      "list is expanded into an independent grid. A possible grid search involving kernel and\n",
      "parameters could look like this:\n",
      "In[34]:\n",
      "param_grid = [{'kernel': ['rbf'],\n",
      "               'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
      "               'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
      "              {'kernel': ['linear'],\n",
      "               'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\n",
      "print(\"List of grids:\\n{}\".format(param_grid))\n",
      "Out[34]:\n",
      "List of grids:\n",
      "[{'kernel': ['rbf'], 'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
      "  'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
      " {'kernel': ['linear'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\n",
      "In the first grid, the kernel parameter is always set to 'rbf' (not that the entry for\n",
      "kernel is a list of length one), and both the C and gamma parameters are varied. In the\n",
      "second grid, the kernel parameter is always set to linear, and only C is varied. Now\n",
      "let’s apply this more complex parameter search:\n",
      "In[35]:\n",
      "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
      "grid_search.fit(X_train, y_train)\n",
      "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
      "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
      "Out[35]:\n",
      "Best parameters: {'C': 100, 'kernel': 'rbf', 'gamma': 0.01}\n",
      "Best cross-validation score: 0.97\n",
      "Grid Search | 271\n",
      "Let’s look at the cv_results_ again. As expected, if kernel is 'linear', then only C is\n",
      "varied:\n",
      "In[36]:\n",
      "results = pd.DataFrame(grid_search.cv_results_)\n",
      "# we display the transposed table so that it better fits on the page:\n",
      "display(results.T)\n",
      "Out[36]:\n",
      "0 1 2 3 … 38 39 40 41\n",
      "param_C 0.001 0.001 0.001 0.001 … 0.1 1 10 100\n",
      "param_gamma 0.001 0.01 0.1 1 … NaN NaN NaN NaN\n",
      "param_kernel rbf rbf rbf rbf … linear linear linear linear\n",
      "params {C: 0.001,\n",
      "kernel: rbf,\n",
      "gamma:\n",
      "0.001}\n",
      "{C: 0.001,\n",
      "kernel: rbf,\n",
      "gamma:\n",
      "0.01}\n",
      "{C: 0.001,\n",
      "kernel: rbf,\n",
      "gamma:\n",
      "0.1}\n",
      "{C: 0.001,\n",
      "kernel: rbf,\n",
      "gamma: 1}\n",
      "… {C: 0.1,\n",
      "kernel:\n",
      "linear}\n",
      "{C: 1,\n",
      "kernel:\n",
      "linear}\n",
      "{C: 10,\n",
      "kernel:\n",
      "linear}\n",
      "{C: 100,\n",
      "kernel:\n",
      "linear}\n",
      "mean_test_score 0.37 0.37 0.37 0.37 … 0.95 0.97 0.96 0.96\n",
      "rank_test_score 27 27 27 27 … 11 1 3 3\n",
      "split0_test_score 0.38 0.38 0.38 0.38 … 0.96 1 0.96 0.96\n",
      "split1_test_score 0.35 0.35 0.35 0.35 … 0.91 0.96 1 1\n",
      "split2_test_score 0.36 0.36 0.36 0.36 … 1 1 1 1\n",
      "split3_test_score 0.36 0.36 0.36 0.36 … 0.91 0.95 0.91 0.91\n",
      "split4_test_score 0.38 0.38 0.38 0.38 … 0.95 0.95 0.95 0.95\n",
      "std_test_score 0.011 0.011 0.011 0.011 … 0.033 0.022 0.034 0.034\n",
      "12 rows × 42 columns\n",
      "Using different cross-validation strategies with grid search\n",
      "Similarly to cross_val_score, GridSearchCV uses stratified k-fold cross-validation\n",
      "by default for classification, and k-fold cross-validation for regression. However, you\n",
      "can also pass any cross-validation splitter, as described in “More control over cross-\n",
      "validation” on page 256, as the cv parameter in GridSearchCV. In particular, to get\n",
      "only a single split into a training and a validation set, you can use ShuffleSplit or\n",
      "StratifiedShuffleSplit with n_iter=1. This might be helpful for very large data‐\n",
      "sets, or very slow models.\n",
      "Nested cross-validation\n",
      "In the preceding examples, we went from using a single split of the data into training,\n",
      "validation, and test sets to splitting the data into training and test sets and then per‐\n",
      "forming cross-validation on the training set. But when using GridSearchCV as\n",
      "272 | Chapter 5: Model Evaluation and Improvement\n",
      "described earlier, we still have a single split of the data into training and test sets,\n",
      "which might make our results unstable and make us depend too much on this single\n",
      "split of the data. We can go a step further, and instead of splitting the original data\n",
      "into training and test sets once, use multiple splits of cross-validation. This will result\n",
      "in what is called nested cross-validation. In nested cross-validation, there is an outer\n",
      "loop over splits of the data into training and test sets. For each of them, a grid search\n",
      "is run (which might result in different best parameters for each split in the outer\n",
      "loop). Then, for each outer split, the test set score using the best settings is reported.\n",
      "The result of this procedure is a list of scores—not a model, and not a parameter set‐\n",
      "ting. The scores tell us how well a model generalizes, given the best parameters found\n",
      "by the grid. As it doesn’t provide a model that can be used on new data, nested cross-\n",
      "validation is rarely used when looking for a predictive model to apply to future data.\n",
      "However, it can be useful for evaluating how well a given model works on a particular\n",
      "dataset.\n",
      "Implementing nested cross-validation in scikit-learn is straightforward. We call\n",
      "cross_val_score with an instance of GridSearchCV as the model:\n",
      "In[34]:\n",
      "scores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5),\n",
      "                         iris.data, iris.target, cv=5)\n",
      "print(\"Cross-validation scores: \", scores)\n",
      "print(\"Mean cross-validation score: \", scores.mean())\n",
      "Out[34]:\n",
      "Cross-validation scores:  [ 0.967  1.     0.967  0.967  1.   ]\n",
      "Mean cross-validation score:  0.98\n",
      "The result of our nested cross-validation can be summarized as “ SVC can achieve 98%\n",
      "mean cross-validation accuracy on the iris dataset”—nothing more and nothing\n",
      "less.\n",
      "Here, we used stratified five-fold cross-validation in both the inner and the outer\n",
      "loop. As our param_grid contains 36 combinations of parameters, this results in a\n",
      "whopping 36 * 5 * 5 = 900 models being built, making nested cross-validation a very\n",
      "expensive procedure. Here, we used the same cross-validation splitter in the inner\n",
      "and the outer loop; however, this is not necessary and you can use any combination\n",
      "of cross-validation strategies in the inner and outer loops. It can be a bit tricky to\n",
      "understand what is happening in the single line given above, and it can be helpful to\n",
      "visualize it as for loops, as done in the following simplified implementation:\n",
      "Grid Search | 273\n",
      "In[35]:\n",
      "def nested_cv(X, y, inner_cv, outer_cv, Classifier, parameter_grid):\n",
      "    outer_scores = []\n",
      "    # for each split of the data in the outer cross-validation\n",
      "    # (split method returns indices)\n",
      "    for training_samples, test_samples in outer_cv.split(X, y):\n",
      "        # find best parameter using inner cross-validation\n",
      "        best_parms = {}\n",
      "        best_score = -np.inf\n",
      "        # iterate over parameters\n",
      "        for parameters in parameter_grid:\n",
      "            # accumulate score over inner splits\n",
      "            cv_scores = []\n",
      "            # iterate over inner cross-validation\n",
      "            for inner_train, inner_test in inner_cv.split(\n",
      "                    X[training_samples], y[training_samples]):\n",
      "                # build classifier given parameters and training data\n",
      "                clf = Classifier(**parameters)\n",
      "                clf.fit(X[inner_train], y[inner_train])\n",
      "                # evaluate on inner test set\n",
      "                score = clf.score(X[inner_test], y[inner_test])\n",
      "                cv_scores.append(score)\n",
      "            # compute mean score over inner folds\n",
      "            mean_score = np.mean(cv_scores)\n",
      "            if mean_score > best_score:\n",
      "                # if better than so far, remember parameters\n",
      "                best_score = mean_score\n",
      "                best_params = parameters\n",
      "        # build classifier on best parameters using outer training set\n",
      "        clf = Classifier(**best_params)\n",
      "        clf.fit(X[training_samples], y[training_samples])\n",
      "        # evaluate\n",
      "        outer_scores.append(clf.score(X[test_samples], y[test_samples]))\n",
      "    return np.array(outer_scores)\n",
      "Now, let’s run this function on the iris dataset:\n",
      "In[36]:\n",
      "from sklearn.model_selection import ParameterGrid, StratifiedKFold\n",
      "scores = nested_cv(iris.data, iris.target, StratifiedKFold(5),\n",
      "          StratifiedKFold(5), SVC, ParameterGrid(param_grid))\n",
      "print(\"Cross-validation scores: {}\".format(scores))\n",
      "Out[36]:\n",
      "Cross-validation scores: [ 0.967  1.     0.967  0.967  1.   ]\n",
      "Parallelizing cross-validation and grid search\n",
      "While running a grid search over many parameters and on large datasets can be com‐\n",
      "putationally challenging, it is also embarrassingly parallel. This means that building a\n",
      "274 | Chapter 5: Model Evaluation and Improvement\n",
      "model using a particular parameter setting on a particular cross-validation split can\n",
      "be done completely independently from the other parameter settings and models.\n",
      "This makes grid search and cross-validation ideal candidates for parallelization over\n",
      "multiple CPU cores or over a cluster. Y ou can make use of multiple cores in Grid\n",
      "SearchCV and cross_val_score by setting the n_jobs parameter to the number of\n",
      "CPU cores you want to use. Y ou can set n_jobs=-1 to use all available cores.\n",
      "Y ou should be aware that scikit-learn does not allow nesting of parallel operations .\n",
      "So, if you are using the n_jobs option on your model (for example, a random forest),\n",
      "you cannot use it in GridSearchCV to search over this model. If your dataset and\n",
      "model are very large, it might be that using many cores uses up too much memory,\n",
      "and you should monitor your memory usage when building large models in parallel.\n",
      "It is also possible to parallelize grid search and cross-validation over multiple\n",
      "machines in a cluster, although at the time of writing this is not supported within\n",
      "scikit-learn. It is, however, possible to use the IPython parallel framework for par‐\n",
      "allel grid searches, if you don’t mind writing the for loop over parameters as we did\n",
      "in “Simple Grid Search” on page 261.\n",
      "For Spark users, there is also the recently developed spark-sklearn package, which\n",
      "allows running a grid search over an already established Spark cluster.\n",
      "Evaluation Metrics and Scoring\n",
      "So far, we have evaluated classification performance using accuracy (the fraction of\n",
      "correctly classified samples) and regression performance using R2. However, these are\n",
      "only two of the many possible ways to summarize how well a supervised model per‐\n",
      "forms on a given dataset. In practice, these evaluation metrics might not be appropri‐\n",
      "ate for your application, and it is important to choose the right metric when selecting\n",
      "between models and adjusting parameters.\n",
      "Keep the End Goal in Mind\n",
      "When selecting a metric, you should always have the end goal of the machine learn‐\n",
      "ing application in mind. In practice, we are usually interested not just in making\n",
      "accurate predictions, but in using these predictions as part of a larger decision-\n",
      "making process. Before picking a machine learning metric, you should think about\n",
      "the high-level goal of the application, often called the business metric. The conse‐\n",
      "quences of choosing a particular algorithm for a machine learning application are\n",
      "Evaluation Metrics and Scoring | 275\n",
      "2 We ask scientifically minded readers to excuse the commercial language in this section. Not losing track of the\n",
      "end goal is equally important in science, though the authors are not aware of a similar phrase to “business\n",
      "impact” being used in that realm.\n",
      "called the business impact.2 Maybe the high-level goal is avoiding traffic accidents, or\n",
      "decreasing the number of hospital admissions. It could also be getting more users for\n",
      "your website, or having users spend more money in your shop. When choosing a\n",
      "model or adjusting parameters, you should pick the model or parameter values that\n",
      "have the most positive influence on the business metric. Often this is hard, as assess‐\n",
      "ing the business impact of a particular model might require putting it in production\n",
      "in a real-life system.\n",
      "In the early stages of development, and for adjusting parameters, it is often infeasible\n",
      "to put models into production just for testing purposes, because of the high business\n",
      "or personal risks that can be involved. Imagine evaluating the pedestrian avoidance\n",
      "capabilities of a self-driving car by just letting it drive around, without verifying it\n",
      "first; if your model is bad, pedestrians will be in trouble! Therefore we often need to\n",
      "find some surrogate evaluation procedure, using an evaluation metric that is easier to\n",
      "compute. For example, we could test classifying images of pedestrians against non-\n",
      "pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\n",
      "pays off to find the closest metric to the original business goal that is feasible to evalu‐\n",
      "ate. This closest metric should be used whenever possible for model evaluation and\n",
      "selection. The result of this evaluation might not be a single number—the conse‐\n",
      "quence of your algorithm could be that you have 10% more customers, but each cus‐\n",
      "tomer will spend 15% less—but it should capture the expected business impact of\n",
      "choosing one model over another.\n",
      "In this section, we will first discuss metrics for the important special case of binary\n",
      "classification, then turn to multiclass classification and finally regression.\n",
      "Metrics for Binary Classification\n",
      "Binary classification is arguably the most common and conceptually simple applica‐\n",
      "tion of machine learning in practice. However, there are still a number of caveats in\n",
      "evaluating even this simple task. Before we dive into alternative metrics, let’s have a\n",
      "look at the ways in which measuring accuracy might be misleading. Remember that\n",
      "for binary classification, we often speak of a positive class and a negative class, with\n",
      "the understanding that the positive class is the one we are looking for.\n",
      "Kinds of errors\n",
      "Often, accuracy is not a good measure of predictive performance, as the number of\n",
      "mistakes we make does not contain all the information we are interested in. Imagine\n",
      "an application to screen for the early detection of cancer using an automated test. If\n",
      "276 | Chapter 5: Model Evaluation and Improvement\n",
      "the test is negative, the patient will be assumed healthy, while if the test is positive, the\n",
      "patient will undergo additional screening. Here, we would call a positive test (an indi‐\n",
      "cation of cancer) the positive class, and a negative test the negative class. We can’t\n",
      "assume that our model will always work perfectly, and it will make mistakes. For any\n",
      "application, we need to ask ourselves what the consequences of these mistakes might\n",
      "be in the real world.\n",
      "One possible mistake is that a healthy patient will be classified as positive, leading to\n",
      "additional testing. This leads to some costs and an inconvenience for the patient (and\n",
      "possibly some mental distress). An incorrect positive prediction is called a false posi‐\n",
      "tive. The other possible mistake is that a sick patient will be classified as negative, and\n",
      "will not receive further tests and treatment. The undiagnosed cancer might lead to\n",
      "serious health issues, and could even be fatal. A mistake of this kind—an incorrect\n",
      "negative prediction—is called a false negative . In statistics, a false positive is also\n",
      "known as type I error, and a false negative as type II error. We will stick to “false nega‐\n",
      "tive” and “false positive, ” as they are more explicit and easier to remember. In the can‐\n",
      "cer diagnosis example, it is clear that we want to avoid false negatives as much as\n",
      "possible, while false positives can be viewed as more of a minor nuisance.\n",
      "While this is a particularly drastic example, the consequence of false positives and\n",
      "false negatives are rarely the same. In commercial applications, it might be possible to\n",
      "assign dollar values to both kinds of mistakes, which would allow measuring the error\n",
      "of a particular prediction in dollars, instead of accuracy. This might be much more\n",
      "meaningful for making business decisions on which model to use.\n",
      "Imbalanced datasets\n",
      "Types of errors play an important role when one of two classes is much more frequent\n",
      "than the other one. This is very common in practice; a good example is click-through\n",
      "prediction, where each data point represents an “impression, ” an item that was shown\n",
      "to a user. This item might be an ad, or a related story, or a related person to follow on\n",
      "a social media site. The goal is to predict whether, if shown a particular item, a user\n",
      "will click on it (indicating they are interested). Most things users are shown on the\n",
      "Internet (in particular, ads) will not result in a click. Y ou might need to show a user\n",
      "100 ads or articles before they find something interesting enough to click on. This\n",
      "results in a dataset where for each 99 “no click” data points, there is 1 “clicked” data\n",
      "point; in other words, 99% of the samples belong to the “no click” class. Datasets in\n",
      "which one class is much more frequent than the other are often called imbalanced\n",
      "datasets, or datasets with imbalanced classes. In reality, imbalanced data is the norm,\n",
      "and it is rare that the events of interest have equal or even similar frequency in the\n",
      "data.\n",
      "Now let’s say you build a classifier that is 99% accurate on the click prediction task.\n",
      "What does that tell you? 99% accuracy sounds impressive, but this doesn’t take the\n",
      "Evaluation Metrics and Scoring | 277\n",
      "class imbalance into account. Y ou can achieve 99% accuracy without building a\n",
      "machine learning model, by always predicting “no click. ” On the other hand, even\n",
      "with imbalanced data, a 99% accurate model could in fact be quite good. However,\n",
      "accuracy doesn’t allow us to distinguish the constant “no click” model from a poten‐\n",
      "tially good model.\n",
      "To illustrate, we’ll create a 9:1 imbalanced dataset from the digits dataset, by classify‐\n",
      "ing the digit 9 against the nine other classes:\n",
      "In[37]:\n",
      "from sklearn.datasets import load_digits\n",
      "digits = load_digits()\n",
      "y = digits.target == 9\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    digits.data, y, random_state=0)\n",
      "We can use the DummyClassifier to always predict the majority class (here\n",
      "“not nine”) to see how uninformative accuracy can be:\n",
      "In[38]:\n",
      "from sklearn.dummy import DummyClassifier\n",
      "dummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
      "pred_most_frequent = dummy_majority.predict(X_test)\n",
      "print(\"Unique predicted labels: {}\".format(np.unique(pred_most_frequent)))\n",
      "print(\"Test score: {:.2f}\".format(dummy_majority.score(X_test, y_test)))\n",
      "Out[38]:\n",
      "Unique predicted labels: [False]\n",
      "Test score: 0.90\n",
      "We obtained close to 90% accuracy without learning anything. This might seem strik‐\n",
      "ing, but think about it for a minute. Imagine someone telling you their model is 90%\n",
      "accurate. Y ou might think they did a very good job. But depending on the problem,\n",
      "that might be possible by just predicting one class! Let’s compare this against using an\n",
      "actual classifier:\n",
      "In[39]:\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "tree = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\n",
      "pred_tree = tree.predict(X_test)\n",
      "print(\"Test score: {:.2f}\".format(tree.score(X_test, y_test)))\n",
      "Out[39]:\n",
      "Test score: 0.92\n",
      "278 | Chapter 5: Model Evaluation and Improvement\n",
      "According to accuracy, the DecisionTreeClassifier is only slightly better than the\n",
      "constant predictor. This could indicate either that something is wrong with how we\n",
      "used DecisionTreeClassifier, or that accuracy is in fact not a good measure here.\n",
      "For comparison purposes, let’s evaluate two more classifiers, LogisticRegression\n",
      "and the default DummyClassifier, which makes random predictions but produces\n",
      "classes with the same proportions as in the training set:\n",
      "In[40]:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "dummy = DummyClassifier().fit(X_train, y_train)\n",
      "pred_dummy = dummy.predict(X_test)\n",
      "print(\"dummy score: {:.2f}\".format(dummy.score(X_test, y_test)))\n",
      "logreg = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
      "pred_logreg = logreg.predict(X_test)\n",
      "print(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\n",
      "Out[40]:\n",
      "dummy score: 0.80\n",
      "logreg score: 0.98\n",
      "The dummy classifier that produces random output is clearly the worst of the lot\n",
      "(according to accuracy), while LogisticRegression produces very good results.\n",
      "However, even the random classifier yields over 80% accuracy. This makes it very\n",
      "hard to judge which of these results is actually helpful. The problem here is that accu‐\n",
      "racy is an inadequate measure for quantifying predictive performance in this imbal‐\n",
      "anced setting. For the rest of this chapter, we will explore alternative metrics that\n",
      "provide better guidance in selecting models. In particular, we would like to have met‐\n",
      "rics that tell us how much better a model is than making “most frequent” predictions\n",
      "or random predictions, as they are computed in pred_most_frequent and\n",
      "pred_dummy. If we use a metric to assess our models, it should definitely be able to\n",
      "weed out these nonsense predictions.\n",
      "Confusion matrices\n",
      "One of the most comprehensive ways to represent the result of evaluating binary clas‐\n",
      "sification is using confusion matrices. Let’s inspect the predictions of LogisticRegres\n",
      "sion from the previous section using the confusion_matrix function. We already\n",
      "stored the predictions on the test set in pred_logreg:\n",
      "Evaluation Metrics and Scoring | 279\n",
      "In[41]:\n",
      "from sklearn.metrics import confusion_matrix\n",
      "confusion = confusion_matrix(y_test, pred_logreg)\n",
      "print(\"Confusion matrix:\\n{}\".format(confusion))\n",
      "Out[41]:\n",
      "Confusion matrix:\n",
      "[[401   2]\n",
      " [  8  39]]\n",
      "The output of confusion_matrix is a two-by-two array, where the rows correspond\n",
      "to the true classes and the columns correspond to the predicted classes. Each entry\n",
      "counts how often a sample that belongs to the class corresponding to the row (here,\n",
      "“not nine” and “nine”) was classified as the class corresponding to the column. The\n",
      "following plot (Figure 5-10) illustrates this meaning:\n",
      "In[42]:\n",
      "mglearn.plots.plot_confusion_matrix_illustration()\n",
      "Figure 5-10. Confusion matrix of the “nine vs. rest” classification task\n",
      "280 | Chapter 5: Model Evaluation and Improvement\n",
      "3 The main diagonal of a two-dimensional array or matrix A is A[i, i].\n",
      "Entries on the main diagonal 3 of the confusion matrix correspond to correct classifi‐\n",
      "cations, while other entries tell us how many samples of one class got mistakenly clas‐\n",
      "sified as another class.\n",
      "If we declare “a nine” the positive class, we can relate the entries of the confusion\n",
      "matrix with the terms false positive and false negative that we introduced earlier. To\n",
      "complete the picture, we call correctly classified samples belonging to the positive\n",
      "class true positives and correctly classified samples belonging to the negative class true\n",
      "negatives. These terms are usually abbreviated FP , FN, TP , and TN and lead to the fol‐\n",
      "lowing interpretation for the confusion matrix (Figure 5-11):\n",
      "In[43]:\n",
      "mglearn.plots.plot_binary_confusion_matrix()\n",
      "Figure 5-11. Confusion matrix for binary classification\n",
      "Now let’s use the confusion matrix to compare the models we fitted earlier (the two\n",
      "dummy models, the decision tree, and the logistic regression):\n",
      "In[44]:\n",
      "print(\"Most frequent class:\")\n",
      "print(confusion_matrix(y_test, pred_most_frequent))\n",
      "print(\"\\nDummy model:\")\n",
      "print(confusion_matrix(y_test, pred_dummy))\n",
      "print(\"\\nDecision tree:\")\n",
      "print(confusion_matrix(y_test, pred_tree))\n",
      "print(\"\\nLogistic Regression\")\n",
      "print(confusion_matrix(y_test, pred_logreg))\n",
      "Evaluation Metrics and Scoring | 281\n",
      "Out[44]:\n",
      "Most frequent class:\n",
      "[[403   0]\n",
      " [ 47   0]]\n",
      "Dummy model:\n",
      "[[361  42]\n",
      " [ 43   4]]\n",
      "Decision tree:\n",
      "[[390  13]\n",
      " [ 24  23]]\n",
      "Logistic Regression\n",
      "[[401   2]\n",
      " [  8  39]]\n",
      "Looking at the confusion matrix, it is quite clear that something is wrong with\n",
      "pred_most_frequent, because it always predicts the same class. pred_dummy, on the\n",
      "other hand, has a very small number of true positives (4), particularly compared to\n",
      "the number of false negatives and false positives—there are many more false positives\n",
      "than true positives! The predictions made by the decision tree make much more\n",
      "sense than the dummy predictions, even though the accuracy was nearly the same.\n",
      "Finally, we can see that logistic regression does better than pred_tree in all aspects: it\n",
      "has more true positives and true negatives while having fewer false positives and false\n",
      "negatives. From this comparison, it is clear that only the decision tree and the logistic\n",
      "regression give reasonable results, and that the logistic regression works better than\n",
      "the tree on all accounts. However, inspecting the full confusion matrix is a bit cum‐\n",
      "bersome, and while we gained a lot of insight from looking at all aspects of the\n",
      "matrix, the process was very manual and qualitative. There are several ways to sum‐\n",
      "marize the information in the confusion matrix, which we will discuss next.\n",
      "Relation to accuracy.    We already saw one way to summarize the result in the confu‐\n",
      "sion matrix—by computing accuracy, which can be expressed as:\n",
      "Accuracy = TP+TN\n",
      "TP+TN + FP + FN\n",
      "In other words, accuracy is the number of correct predictions (TP and TN) divided\n",
      "by the number of all samples (all entries of the confusion matrix summed up).\n",
      "Precision, recall, and f-score.    There are several other ways to summarize the confusion\n",
      "matrix, with the most common ones being precision and recall. Precision measures\n",
      "how many of the samples predicted as positive are actually positive:\n",
      "282 | Chapter 5: Model Evaluation and Improvement\n",
      "Precision = TP\n",
      "TP+FP\n",
      "Precision is used as a performance metric when the goal is to limit the number of\n",
      "false positives. As an example, imagine a model for predicting whether a new drug\n",
      "will be effective in treating a disease in clinical trials. Clinical trials are notoriously\n",
      "expensive, and a pharmaceutical company will only want to run an experiment if it is\n",
      "very sure that the drug will actually work. Therefore, it is important that the model\n",
      "does not produce many false positives—in other words, that it has a high precision.\n",
      "Precision is also known as positive predictive value (PPV).\n",
      "Recall, on the other hand, measures how many of the positive samples are captured\n",
      "by the positive predictions:\n",
      "Recall = TP\n",
      "TP+FN\n",
      "Recall is used as performance metric when we need to identify all positive samples;\n",
      "that is, when it is important to avoid false negatives. The cancer diagnosis example\n",
      "from earlier in this chapter is a good example for this: it is important to find all peo‐\n",
      "ple that are sick, possibly including healthy patients in the prediction. Other names\n",
      "for recall are sensitivity, hit rate, or true positive rate (TPR).\n",
      "There is a trade-off between optimizing recall and optimizing precision. Y ou can triv‐\n",
      "ially obtain a perfect recall if you predict all samples to belong to the positive class—\n",
      "there will be no false negatives, and no true negatives either. However, predicting all\n",
      "samples as positive will result in many false positives, and therefore the precision will\n",
      "be very low. On the other hand, if you find a model that predicts only the single data\n",
      "point it is most sure about as positive and the rest as negative, then precision will be\n",
      "perfect (assuming this data point is in fact positive), but recall will be very bad.\n",
      "Precision and recall are only two of many classification measures\n",
      "derived from TP , FP , TN, and FN. Y ou can find a great summary of\n",
      "all the measures on Wikipedia. In the machine learning commu‐\n",
      "nity, precision and recall are arguably the most commonly used\n",
      "measures for binary classification, but other communities might\n",
      "use other related metrics.\n",
      "So, while precision and recall are very important measures, looking at only one of\n",
      "them will not provide you with the full picture. One way to summarize them is the\n",
      "f-score or f-measure, which is with the harmonic mean of precision and recall:\n",
      "F = 2 · precision·recall\n",
      "precision+recall\n",
      "Evaluation Metrics and Scoring | 283\n",
      "This particular variant is also known as the f1-score. As it takes precision and recall\n",
      "into account, it can be a better measure than accuracy on imbalanced binary classifi‐\n",
      "cation datasets. Let’s run it on the predictions for the “nine vs. rest” dataset that we\n",
      "computed earlier. Here, we will assume that the “nine” class is the positive class (it is\n",
      "labeled as True while the rest is labeled as False), so the positive class is the minority\n",
      "class:\n",
      "In[45]:\n",
      "from sklearn.metrics import f1_score\n",
      "print(\"f1 score most frequent: {:.2f}\".format(\n",
      "        f1_score(y_test, pred_most_frequent)))\n",
      "print(\"f1 score dummy: {:.2f}\".format(f1_score(y_test, pred_dummy)))\n",
      "print(\"f1 score tree: {:.2f}\".format(f1_score(y_test, pred_tree)))\n",
      "print(\"f1 score logistic regression: {:.2f}\".format(\n",
      "        f1_score(y_test, pred_logreg)))\n",
      "Out[45]:\n",
      "f1 score most frequent: 0.00\n",
      "f1 score dummy: 0.10\n",
      "f1 score tree: 0.55\n",
      "f1 score logistic regression: 0.89\n",
      "We can note two things here. First, we get an error message for the most_frequent\n",
      "prediction, as there were no predictions of the positive class (which makes the\n",
      "denominator in the f-score zero). Also, we can see a pretty strong distinction between\n",
      "the dummy predictions and the tree predictions, which wasn’t clear when looking at\n",
      "accuracy alone. Using the f-score for evaluation, we summarized the predictive per‐\n",
      "formance again in one number. However, the f-score seems to capture our intuition\n",
      "of what makes a good model much better than accuracy did. A disadvantage of the\n",
      "f-score, however, is that it is harder to interpret and explain than accuracy.\n",
      "If we want a more comprehensive summary of precision, recall, and f1-score, we can\n",
      "use the classification_report convenience function to compute all three at once,\n",
      "and print them in a nice format:\n",
      "In[46]:\n",
      "from sklearn.metrics import classification_report\n",
      "print(classification_report(y_test, pred_most_frequent,\n",
      "                            target_names=[\"not nine\", \"nine\"]))\n",
      "284 | Chapter 5: Model Evaluation and Improvement\n",
      "Out[46]:\n",
      "             precision    recall  f1-score   support\n",
      "   not nine       0.90      1.00      0.94       403\n",
      "       nine       0.00      0.00      0.00        47\n",
      "avg / total       0.80      0.90      0.85       450\n",
      "The classification_report function produces one line per class (here, True and\n",
      "False) and reports precision, recall, and f-score with this class as the positive class.\n",
      "Before, we assumed the minority “nine” class was the positive class. If we change the\n",
      "positive class to “not nine, ” we can see from the output of classification_report\n",
      "that we obtain an f-score of 0.94 with the most_frequent model. Furthermore, for the\n",
      "“not nine” class we have a recall of 1, as we classified all samples as “not nine. ” The\n",
      "last column next to the f-score provides the support of each class, which simply means\n",
      "the number of samples in this class according to the ground truth.\n",
      "The last row in the classification report shows a weighted (by the number of samples\n",
      "in the class) average of the numbers for each class. Here are two more reports, one for\n",
      "the dummy classifier and one for the logistic regression:\n",
      "In[47]:\n",
      "print(classification_report(y_test, pred_dummy,\n",
      "                            target_names=[\"not nine\", \"nine\"]))\n",
      "Out[47]:\n",
      "             precision    recall  f1-score   support\n",
      "   not nine       0.90      0.92      0.91       403\n",
      "       nine       0.11      0.09      0.10        47\n",
      "avg / total       0.81      0.83      0.82       450\n",
      "In[48]:\n",
      "print(classification_report(y_test, pred_logreg,\n",
      "                            target_names=[\"not nine\", \"nine\"]))\n",
      "Out[48]:\n",
      "             precision    recall  f1-score   support\n",
      "   not nine       0.98      1.00      0.99       403\n",
      "       nine       0.95      0.83      0.89        47\n",
      "avg / total       0.98      0.98      0.98       450\n",
      "Evaluation Metrics and Scoring | 285\n",
      "As you may notice when looking at the reports, the differences between the dummy\n",
      "models and a very good model are not as clear any more. Picking which class is\n",
      "declared the positive class has a big impact on the metrics. While the f-score for the\n",
      "dummy classification is 0.13 (vs. 0.89 for the logistic regression) on the “nine” class,\n",
      "for the “not nine” class it is 0.90 vs. 0.99, which both seem like reasonable results.\n",
      "Looking at all the numbers together paints a pretty accurate picture, though, and we\n",
      "can clearly see the superiority of the logistic regression model.\n",
      "Taking uncertainty into account\n",
      "The confusion matrix and the classification report provide a very detailed analysis of\n",
      "a particular set of predictions. However, the predictions themselves already threw\n",
      "away a lot of information that is contained in the model. As we discussed in Chap‐\n",
      "ter 2, most classifiers provide a decision_function or a predict_proba method to\n",
      "assess degrees of certainty about predictions. Making predictions can be seen as\n",
      "thresholding the output of decision_function or predict_proba at a certain fixed\n",
      "point—in binary classification we use 0 for the decision function and 0.5 for\n",
      "predict_proba.\n",
      "The following is an example of an imbalanced binary classification task, with 400\n",
      "points in the negative class classified against 50 points in the positive class. The train‐\n",
      "ing data is shown on the left in Figure 5-12. We train a kernel SVM model on this\n",
      "data, and the plots to the right of the training data illustrate the values of the decision\n",
      "function as a heat map. Y ou can see a black circle in the plot in the top center, which\n",
      "denotes the threshold of the decision_function being exactly zero. Points inside this\n",
      "circle will be classified as the positive class, and points outside as the negative class:\n",
      "In[49]:\n",
      "from mglearn.datasets import make_blobs\n",
      "X, y = make_blobs(n_samples=(400, 50), centers=2, cluster_std=[7.0, 2],\n",
      "                  random_state=22)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
      "svc = SVC(gamma=.05).fit(X_train, y_train)\n",
      "In[50]:\n",
      "mglearn.plots.plot_decision_threshold()\n",
      "286 | Chapter 5: Model Evaluation and Improvement\n",
      "Figure 5-12. Heatmap of the decision function and the impact of changing the decision\n",
      "threshold\n",
      "We can use the classification_report function to evaluate precision and recall for\n",
      "both classes:\n",
      "In[51]:\n",
      "print(classification_report(y_test, svc.predict(X_test)))\n",
      "Out[51]:\n",
      "             precision    recall  f1-score   support\n",
      "          0       0.97      0.89      0.93       104\n",
      "          1       0.35      0.67      0.46         9\n",
      "avg / total       0.92      0.88      0.89       113\n",
      "For class 1, we get a fairly small recall, and precision is mixed. Because class 0 is so\n",
      "much larger, the classifier focuses on getting class 0 right, and not the smaller class 1.\n",
      "Let’s assume in our application it is more important to have a high recall for class 1, as\n",
      "in the cancer screening example earlier. This means we are willing to risk more false\n",
      "positives (false class 1) in exchange for more true positives (which will increase the\n",
      "recall). The predictions generated by svc.predict really do not fulfill this require‐\n",
      "ment, but we can adjust the predictions to focus on a higher recall of class 1 by\n",
      "changing the decision threshold away from 0. By default, points with a deci\n",
      "sion_function value greater than 0 will be classified as class 1. We want more points\n",
      "to be classified as class 1, so we need to decrease the threshold:\n",
      "Evaluation Metrics and Scoring | 287\n",
      "In[52]:\n",
      "y_pred_lower_threshold = svc.decision_function(X_test) > -.8\n",
      "Let’s look at the classification report for this prediction:\n",
      "In[53]:\n",
      "print(classification_report(y_test, y_pred_lower_threshold))\n",
      "Out[53]:\n",
      "             precision    recall  f1-score   support\n",
      "          0       1.00      0.82      0.90       104\n",
      "          1       0.32      1.00      0.49         9\n",
      "avg / total       0.95      0.83      0.87       113\n",
      "As expected, the recall of class 1 went up, and the precision went down. We are now\n",
      "classifying a larger region of space as class 1, as illustrated in the top-right panel of\n",
      "Figure 5-12. If you value precision over recall or the other way around, or your data is\n",
      "heavily imbalanced, changing the decision threshold is the easiest way to obtain bet‐\n",
      "ter results. As the decision_function can have arbitrary ranges, it is hard to provide\n",
      "a rule of thumb regarding how to pick a threshold.\n",
      "If you do set a threshold, you need to be careful not to do so using\n",
      "the test set. As with any other parameter, setting a decision thresh‐\n",
      "old on the test set is likely to yield overly optimistic results. Use a\n",
      "validation set or cross-validation instead.\n",
      "Picking a threshold for models that implement the predict_proba method can be\n",
      "easier, as the output of predict_proba is on a fixed 0 to 1 scale, and models probabil‐\n",
      "ities. By default, the threshold of 0.5 means that if the model is more than 50% “sure”\n",
      "that a point is of the positive class, it will be classified as such. Increasing the thresh‐\n",
      "old means that the model needs to be more confident to make a positive decision\n",
      "(and less confident to make a negative decision). While working with probabilities\n",
      "may be more intuitive than working with arbitrary thresholds, not all models provide\n",
      "realistic models of uncertainty (a DecisionTree that is grown to its full depth is\n",
      "always 100% sure of its decisions, even though it might often be wrong). This relates\n",
      "to the concept of calibration: a calibrated model is a model that provides an accurate\n",
      "measure of its uncertainty. Discussing calibration in detail is beyond the scope of this\n",
      "book, but you can find more details in the paper “Predicting Good Probabilities with\n",
      "Supervised Learning” by Alexandru Niculescu-Mizil and Rich Caruana.\n",
      "288 | Chapter 5: Model Evaluation and Improvement\n",
      "Precision-recall curves and ROC curves\n",
      "As we just discussed, changing the threshold that is used to make a classification deci‐\n",
      "sion in a model is a way to adjust the trade-off of precision and recall for a given clas‐\n",
      "sifier. Maybe you want to miss less than 10% of positive samples, meaning a desired\n",
      "recall of 90%. This decision depends on the application, and it should be driven by\n",
      "business goals. Once a particular goal is set—say, a particular recall or precision value\n",
      "for a class—a threshold can be set appropriately. It is always possible to set a thresh‐\n",
      "old to fulfill a particular target, like 90% recall. The hard part is to develop a model\n",
      "that still has reasonable precision with this threshold—if you classify everything as\n",
      "positive, you will have 100% recall, but your model will be useless.\n",
      "Setting a requirement on a classifier like 90% recall is often called setting the operat‐\n",
      "ing point. Fixing an operating point is often helpful in business settings to make per‐\n",
      "formance guarantees to customers or other groups inside your organization.\n",
      "Often, when developing a new model, it is not entirely clear what the operating point\n",
      "will be. For this reason, and to understand a modeling problem better, it is instructive\n",
      "to look at all possible thresholds, or all possible trade-offs of precision and recalls at\n",
      "once. This is possible using a tool called the precision-recall curve. Y ou can find the\n",
      "function to compute the precision-recall curve in the sklearn.metrics module. It\n",
      "needs the ground truth labeling and predicted uncertainties, created via either\n",
      "decision_function or predict_proba:\n",
      "In[54]:\n",
      "from sklearn.metrics import precision_recall_curve\n",
      "precision, recall, thresholds = precision_recall_curve(\n",
      "    y_test, svc.decision_function(X_test))\n",
      "The precision_recall_curve function returns a list of precision and recall values\n",
      "for all possible thresholds (all values that appear in the decision function) in sorted\n",
      "order, so we can plot a curve, as seen in Figure 5-13:\n",
      "In[55]:\n",
      "# Use more data points for a smoother curve\n",
      "X, y = make_blobs(n_samples=(4000, 500), centers=2, cluster_std=[7.0, 2],\n",
      "                  random_state=22)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
      "svc = SVC(gamma=.05).fit(X_train, y_train)\n",
      "precision, recall, thresholds = precision_recall_curve(\n",
      "    y_test, svc.decision_function(X_test))\n",
      "# find threshold closest to zero\n",
      "close_zero = np.argmin(np.abs(thresholds))\n",
      "plt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\n",
      "         label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n",
      "plt.plot(precision, recall, label=\"precision recall curve\")\n",
      "plt.xlabel(\"Precision\")\n",
      "plt.ylabel(\"Recall\")\n",
      "Evaluation Metrics and Scoring | 289\n",
      "Figure 5-13. Precision recall curve for SVC(gamma=0.05)\n",
      "Each point along the curve in Figure 5-13 corresponds to a possible threshold of the\n",
      "decision_function. We can see, for example, that we can achieve a recall of 0.4 at a\n",
      "precision of about 0.75. The black circle marks the point that corresponds to a thresh‐\n",
      "old of 0, the default threshold for decision_function. This point is the trade-off that\n",
      "is chosen when calling the predict method.\n",
      "The closer a curve stays to the upper-right corner, the better the classifier. A point at\n",
      "the upper right means high precision and high recall for the same threshold. The\n",
      "curve starts at the top-left corner, corresponding to a very low threshold, classifying\n",
      "everything as the positive class. Raising the threshold moves the curve toward higher\n",
      "precision, but also lower recall. Raising the threshold more and more, we get to a sit‐\n",
      "uation where most of the points classified as being positive are true positives, leading\n",
      "to a very high precision but lower recall. The more the model keeps recall high as\n",
      "precision goes up, the better.\n",
      "Looking at this particular curve a bit more, we can see that with this model it is possi‐\n",
      "ble to get a precision of up to around 0.5 with very high recall. If we want a much\n",
      "higher precision, we have to sacrifice a lot of recall. In other words, on the left the\n",
      "curve is relatively flat, meaning that recall does not go down a lot when we require\n",
      "increased precision. For precision greater than 0.5, each gain in precision costs us a\n",
      "lot of recall.\n",
      "Different classifiers can work well in different parts of the curve—that is, at different\n",
      "operating points. Let’s compare the SVM we trained to a random forest trained on the\n",
      "same dataset. The RandomForestClassifier doesn’t have a decision_function, only\n",
      "predict_proba. The precision_recall_curve function expects as its second argu‐\n",
      "ment a certainty measure for the positive class (class 1), so we pass the probability of\n",
      "a sample being class 1—that is, rf.predict_proba(X_test)[:, 1] . The default\n",
      "threshold for predict_proba in binary classification is 0.5, so this is the point we\n",
      "marked on the curve (see Figure 5-14):\n",
      "290 | Chapter 5: Model Evaluation and Improvement\n",
      "In[56]:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "rf = RandomForestClassifier(n_estimators=100, random_state=0, max_features=2)\n",
      "rf.fit(X_train, y_train)\n",
      "# RandomForestClassifier has predict_proba, but not decision_function\n",
      "precision_rf, recall_rf, thresholds_rf = precision_recall_curve(\n",
      "    y_test, rf.predict_proba(X_test)[:, 1])\n",
      "plt.plot(precision, recall, label=\"svc\")\n",
      "plt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\n",
      "         label=\"threshold zero svc\", fillstyle=\"none\", c='k', mew=2)\n",
      "plt.plot(precision_rf, recall_rf, label=\"rf\")\n",
      "close_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\n",
      "plt.plot(precision_rf[close_default_rf], recall_rf[close_default_rf], '^', c='k',\n",
      "         markersize=10, label=\"threshold 0.5 rf\", fillstyle=\"none\", mew=2)\n",
      "plt.xlabel(\"Precision\")\n",
      "plt.ylabel(\"Recall\")\n",
      "plt.legend(loc=\"best\")\n",
      "Figure 5-14. Comparing precision recall curves of SVM and random forest\n",
      "From the comparison plot we can see that the random forest performs better at the\n",
      "extremes, for very high recall or very high precision requirements. Around the mid‐\n",
      "dle (approximately precision=0.7), the SVM performs better. If we only looked at the\n",
      "f1-score to compare overall performance, we would have missed these subtleties. The\n",
      "f1-score only captures one point on the precision-recall curve, the one given by the\n",
      "default threshold:\n",
      "Evaluation Metrics and Scoring | 291\n",
      "4 There are some minor technical differences between the area under the precision-recall curve and average\n",
      "precision. However, this explanation conveys the general idea.\n",
      "In[57]:\n",
      "print(\"f1_score of random forest: {:.3f}\".format(\n",
      "    f1_score(y_test, rf.predict(X_test))))\n",
      "print(\"f1_score of svc: {:.3f}\".format(f1_score(y_test, svc.predict(X_test))))\n",
      "Out[57]:\n",
      "f1_score of random forest: 0.610\n",
      "f1_score of svc: 0.656\n",
      "Comparing two precision-recall curves provides a lot of detailed insight, but is a fairly\n",
      "manual process. For automatic model comparison, we might want to summarize the\n",
      "information contained in the curve, without limiting ourselves to a particular thresh‐\n",
      "old or operating point. One particular way to summarize the precision-recall curve is\n",
      "by computing the integral or area under the curve of the precision-recall curve, also\n",
      "known as the average precision.4 Y ou can use the average_precision_score function\n",
      "to compute the average precision. Because we need to compute the ROC curve and\n",
      "consider multiple thresholds, the result of decision_function or predict_proba\n",
      "needs to be passed to average_precision_score, not the result of predict:\n",
      "In[58]:\n",
      "from sklearn.metrics import average_precision_score\n",
      "ap_rf = average_precision_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
      "ap_svc = average_precision_score(y_test, svc.decision_function(X_test))\n",
      "print(\"Average precision of random forest: {:.3f}\".format(ap_rf))\n",
      "print(\"Average precision of svc: {:.3f}\".format(ap_svc))\n",
      "Out[58]:\n",
      "Average precision of random forest: 0.666\n",
      "Average precision of svc: 0.663\n",
      "When averaging over all possible thresholds, we see that the random forest and SVC\n",
      "perform similarly well, with the random forest even slightly ahead. This is quite dif‐\n",
      "ferent from the result we got from f1_score earlier. Because average precision is the\n",
      "area under a curve that goes from 0 to 1, average precision always returns a value\n",
      "between 0 (worst) and 1 (best). The average precision of a classifier that assigns\n",
      "decision_function at random is the fraction of positive samples in the dataset.\n",
      "Receiver operating characteristics (ROC) and AUC\n",
      "There is another tool that is commonly used to analyze the behavior of classifiers at\n",
      "different thresholds: the receiver operating characteristics curve , or ROC curve  for\n",
      "short. Similar to the precision-recall curve, the ROC curve considers all possible\n",
      "292 | Chapter 5: Model Evaluation and Improvement\n",
      "thresholds for a given classifier, but instead of reporting precision and recall, it shows\n",
      "the false positive rate (FPR) against the true positive rate (TPR). Recall that the true\n",
      "positive rate is simply another name for recall, while the false positive rate is the frac‐\n",
      "tion of false positives out of all negative samples:\n",
      "FPR = FP\n",
      "FP+TN\n",
      "The ROC curve can be computed using the roc_curve function (see Figure 5-15):\n",
      "In[59]:\n",
      "from sklearn.metrics import roc_curve\n",
      "fpr, tpr, thresholds = roc_curve(y_test, svc.decision_function(X_test))\n",
      "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
      "plt.xlabel(\"FPR\")\n",
      "plt.ylabel(\"TPR (recall)\")\n",
      "# find threshold closest to zero\n",
      "close_zero = np.argmin(np.abs(thresholds))\n",
      "plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10,\n",
      "         label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n",
      "plt.legend(loc=4)\n",
      "Figure 5-15. ROC curve for SVM\n",
      "For the ROC curve, the ideal curve is close to the top left: you want a classifier that\n",
      "produces a high recall while keeping a low false positive rate. Compared to the default\n",
      "threshold of 0, the curve shows that we can achieve a significantly higher recall\n",
      "(around 0.9) while only increasing the FPR slightly. The point closest to the top left\n",
      "might be a better operating point than the one chosen by default. Again, be aware that\n",
      "choosing a threshold should not be done on the test set, but on a separate validation\n",
      "set.\n",
      "Evaluation Metrics and Scoring | 293\n",
      "Y ou can find a comparison of the random forest and the SVM using ROC curves in\n",
      "Figure 5-16:\n",
      "In[60]:\n",
      "from sklearn.metrics import roc_curve\n",
      "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf.predict_proba(X_test)[:, 1])\n",
      "plt.plot(fpr, tpr, label=\"ROC Curve SVC\")\n",
      "plt.plot(fpr_rf, tpr_rf, label=\"ROC Curve RF\")\n",
      "plt.xlabel(\"FPR\")\n",
      "plt.ylabel(\"TPR (recall)\")\n",
      "plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10,\n",
      "         label=\"threshold zero SVC\", fillstyle=\"none\", c='k', mew=2)\n",
      "close_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\n",
      "plt.plot(fpr_rf[close_default_rf], tpr[close_default_rf], '^', markersize=10,\n",
      "         label=\"threshold 0.5 RF\", fillstyle=\"none\", c='k', mew=2)\n",
      "plt.legend(loc=4)\n",
      "Figure 5-16. Comparing ROC curves for SVM and random forest\n",
      "As for the precision-recall curve, we often want to summarize the ROC curve using a\n",
      "single number, the area under the curve (this is commonly just referred to as the\n",
      "AUC, and it is understood that the curve in question is the ROC curve). We can com‐\n",
      "pute the area under the ROC curve using the roc_auc_score function:\n",
      "294 | Chapter 5: Model Evaluation and Improvement\n",
      "In[61]:\n",
      "from sklearn.metrics import roc_auc_score\n",
      "rf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
      "svc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\n",
      "print(\"AUC for Random Forest: {:.3f}\".format(rf_auc))\n",
      "print(\"AUC for SVC: {:.3f}\".format(svc_auc))\n",
      "Out[61]:\n",
      "AUC for Random Forest: 0.937\n",
      "AUC for SVC: 0.916\n",
      "Comparing the random forest and SVM using the AUC score, we find that the ran‐\n",
      "dom forest performs quite a bit better than the SVM. Recall that because average pre‐\n",
      "cision is the area under a curve that goes from 0 to 1, average precision always returns\n",
      "a value between 0 (worst) and 1 (best). Predicting randomly always produces an AUC\n",
      "of 0.5, no matter how imbalanced the classes in a dataset are. This makes AUC a\n",
      "much better metric for imbalanced classification problems than accuracy. The AUC\n",
      "can be interpreted as evaluating the ranking of positive samples. It’s equivalent to the\n",
      "probability that a randomly picked point of the positive class will have a higher score\n",
      "according to the classifier than a randomly picked point from the negative class. So, a\n",
      "perfect AUC of 1 means that all positive points have a higher score than all negative\n",
      "points. For classification problems with imbalanced classes, using AUC for model\n",
      "selection is often much more meaningful than using accuracy.\n",
      "Let’s go back to the problem we studied earlier of classifying all nines in the digits\n",
      "dataset versus all other digits. We will classify the dataset with an SVM with three dif‐\n",
      "ferent settings of the kernel bandwidth, gamma (see Figure 5-17):\n",
      "In[62]:\n",
      "y = digits.target == 9\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    digits.data, y, random_state=0)\n",
      "plt.figure()\n",
      "for gamma in [1, 0.05, 0.01]:\n",
      "    svc = SVC(gamma=gamma).fit(X_train, y_train)\n",
      "    accuracy = svc.score(X_test, y_test)\n",
      "    auc = roc_auc_score(y_test, svc.decision_function(X_test))\n",
      "    fpr, tpr, _ = roc_curve(y_test , svc.decision_function(X_test))\n",
      "    print(\"gamma = {:.2f}  accuracy = {:.2f}  AUC = {:.2f}\".format(\n",
      "    gamma, accuracy, auc))\n",
      "    plt.plot(fpr, tpr, label=\"gamma={:.3f}\".format(gamma))\n",
      "plt.xlabel(\"FPR\")\n",
      "plt.ylabel(\"TPR\")\n",
      "plt.xlim(-0.01, 1)\n",
      "plt.ylim(0, 1.02)\n",
      "plt.legend(loc=\"best\")\n",
      "Evaluation Metrics and Scoring | 295\n",
      "5 Looking at the curve for gamma=0.01 in detail, you can see a small kink close to the top left. That means that at\n",
      "least one point was not ranked correctly. The AUC of 1.0 is a consequence of rounding to the second decimal\n",
      "point.\n",
      "Out[62]:\n",
      "gamma = 1.00  accuracy = 0.90  AUC = 0.50\n",
      "gamma = 0.05  accuracy = 0.90  AUC = 0.90\n",
      "gamma = 0.01  accuracy = 0.90  AUC = 1.00\n",
      "Figure 5-17. Comparing ROC curves of SVMs with different settings of gamma\n",
      "The accuracy of all three settings of gamma is the same, 90%. This might be the same\n",
      "as chance performance, or it might not. Looking at the AUC and the corresponding\n",
      "curve, however, we see a clear distinction between the three models. With gamma=1.0,\n",
      "the AUC is actually at chance level, meaning that the output of the decision_func\n",
      "tion is as good as random. With gamma=0.05, performance drastically improves to an\n",
      "AUC of 0.5. Finally, with gamma=0.01, we get a perfect AUC of 1.0. That means that\n",
      "all positive points are ranked higher than all negative points according to the decision\n",
      "function. In other words, with the right threshold, this model can classify the data\n",
      "perfectly!5 Knowing this, we can adjust the threshold on this model and obtain great\n",
      "predictions. If we had only used accuracy, we would never have discovered this.\n",
      "For this reason, we highly recommend using AUC when evaluating models on imbal‐\n",
      "anced data. Keep in mind that AUC does not make use of the default threshold,\n",
      "though, so adjusting the decision threshold might be necessary to obtain useful classi‐\n",
      "fication results from a model with a high AUC.\n",
      "Metrics for Multiclass Classification\n",
      "Now that we have discussed evaluation of binary classification tasks in depth, let’s\n",
      "move on to metrics to evaluate multiclass classification. Basically, all metrics for\n",
      "multiclass classification are derived from binary classification metrics, but averaged\n",
      "296 | Chapter 5: Model Evaluation and Improvement\n",
      "over all classes. Accuracy for multiclass classification is again defined as the fraction\n",
      "of correctly classified examples. And again, when classes are imbalanced, accuracy is\n",
      "not a great evaluation measure. Imagine a three-class classification problem with 85%\n",
      "of points belonging to class A, 10% belonging to class B, and 5% belonging to class C.\n",
      "What does being 85% accurate mean on this dataset? In general, multiclass classifica‐\n",
      "tion results are harder to understand than binary classification results. Apart from\n",
      "accuracy, common tools are the confusion matrix and the classification report we saw\n",
      "in the binary case in the previous section. Let’s apply these two detailed evaluation\n",
      "methods on the task of classifying the 10 different handwritten digits in the digits\n",
      "dataset:\n",
      "In[63]:\n",
      "from sklearn.metrics import accuracy_score\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    digits.data, digits.target, random_state=0)\n",
      "lr = LogisticRegression().fit(X_train, y_train)\n",
      "pred = lr.predict(X_test)\n",
      "print(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, pred)))\n",
      "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, pred)))\n",
      "Out[63]:\n",
      "Accuracy: 0.953\n",
      "Confusion matrix:\n",
      "[[37  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 39  0  0  0  0  2  0  2  0]\n",
      " [ 0  0 41  3  0  0  0  0  0  0]\n",
      " [ 0  0  1 43  0  0  0  0  0  1]\n",
      " [ 0  0  0  0 38  0  0  0  0  0]\n",
      " [ 0  1  0  0  0 47  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 52  0  0  0]\n",
      " [ 0  1  0  1  1  0  0 45  0  0]\n",
      " [ 0  3  1  0  0  0  0  0 43  1]\n",
      " [ 0  0  0  1  0  1  0  0  1 44]]\n",
      "The model has an accuracy of 95.3%, which already tells us that we are doing pretty\n",
      "well. The confusion matrix provides us with some more detail. As for the binary case,\n",
      "each row corresponds to a true label, and each column corresponds to a predicted\n",
      "label. Y ou can find a visually more appealing plot in Figure 5-18:\n",
      "In[64]:\n",
      "scores_image = mglearn.tools.heatmap(\n",
      "    confusion_matrix(y_test, pred), xlabel='Predicted label',\n",
      "    ylabel='True label', xticklabels=digits.target_names,\n",
      "    yticklabels=digits.target_names, cmap=plt.cm.gray_r, fmt=\"%d\")\n",
      "plt.title(\"Confusion matrix\")\n",
      "plt.gca().invert_yaxis()\n",
      "Evaluation Metrics and Scoring | 297\n",
      "Figure 5-18. Confusion matrix for the 10-digit classification task\n",
      "For the first class, the digit 0, there are 37 samples in the class, and all of these sam‐\n",
      "ples were classified as class 0 (there are no false negatives for class 0). We can see that\n",
      "because all other entries in the first row of the confusion matrix are 0. We can also see\n",
      "that no other digits were mistakenly classified as 0, because all other entries in the\n",
      "first column of the confusion matrix are 0 (there are no false positives for class 0).\n",
      "Some digits were confused with others, though—for example, the digit 2 (third row),\n",
      "three of which were classified as the digit 3 (fourth column). There was also one digit\n",
      "3 that was classified as 2 (third column, fourth row) and one digit 8 that was classified\n",
      "as 2 (thrid column, fourth row).\n",
      "With the classification_report function, we can compute the precision, recall,\n",
      "and f-score for each class:\n",
      "In[65]:\n",
      "print(classification_report(y_test, pred))\n",
      "Out[65]:\n",
      "             precision    recall  f1-score   support\n",
      "          0       1.00      1.00      1.00        37\n",
      "          1       0.89      0.91      0.90        43\n",
      "          2       0.95      0.93      0.94        44\n",
      "          3       0.90      0.96      0.92        45\n",
      "          4       0.97      1.00      0.99        38\n",
      "          5       0.98      0.98      0.98        48\n",
      "          6       0.96      1.00      0.98        52\n",
      "          7       1.00      0.94      0.97        48\n",
      "          8       0.93      0.90      0.91        48\n",
      "          9       0.96      0.94      0.95        47\n",
      "avg / total       0.95      0.95      0.95       450\n",
      "298 | Chapter 5: Model Evaluation and Improvement\n",
      "Unsurprisingly, precision and recall are a perfect 1 for class 0, as there are no confu‐\n",
      "sions with this class. For class 7, on the other hand, precision is 1 because no other\n",
      "class was mistakenly classified as 7, while for class 6, there are no false negatives, so\n",
      "the recall is 1. We can also see that the model has particular difficulties with classes 8\n",
      "and 3.\n",
      "The most commonly used metric for imbalanced datasets in the multiclass setting is\n",
      "the multiclass version of the f-score. The idea behind the multiclass f-score is to com‐\n",
      "pute one binary f-score per class, with that class being the positive class and the other\n",
      "classes making up the negative classes. Then, these per-class f-scores are averaged\n",
      "using one of the following strategies:\n",
      "• \"macro\" averaging computes the unweighted per-class f-scores. This gives equal\n",
      "weight to all classes, no matter what their size is.\n",
      "• \"weighted\" averaging computes the mean of the per-class f-scores, weighted by\n",
      "their support. This is what is reported in the classification report.\n",
      "• \"micro\" averaging computes the total number of false positives, false negatives,\n",
      "and true positives over all classes, and then computes precision, recall, and f-\n",
      "score using these counts.\n",
      "If you care about each sample equally much, it is recommended to use the \"micro\"\n",
      "average f1-score; if you care about each class equally much, it is recommended to use\n",
      "the \"macro\" average f1-score:\n",
      "In[66]:\n",
      "print(\"Micro average f1 score: {:.3f}\".format\n",
      "       (f1_score(y_test, pred, average=\"micro\")))\n",
      "print(\"Macro average f1 score: {:.3f}\".format\n",
      "       (f1_score(y_test, pred, average=\"macro\")))\n",
      "Out[66]:\n",
      "Micro average f1 score: 0.953\n",
      "Macro average f1 score: 0.954\n",
      "Regression Metrics\n",
      "Evaluation for regression can be done in similar detail as we did for classification—\n",
      "for example, by analyzing overpredicting the target versus underpredicting the target.\n",
      "However, in most applications we’ve seen, using the default R2 used in the score\n",
      "method of all regressors is enough. Sometimes business decisions are made on the\n",
      "basis of mean squared error or mean absolute error, which might give incentive to\n",
      "tune models using these metrics. In general, though, we have found R2 to be a more\n",
      "intuitive metric to evaluate regression models.\n",
      "Evaluation Metrics and Scoring | 299\n",
      "Using Evaluation Metrics in Model Selection\n",
      "We have discussed many evaluation methods in detail, and how to apply them given\n",
      "the ground truth and a model. However, we often want to use metrics like AUC in\n",
      "model selection using GridSearchCV or cross_val_score. Luckily scikit-learn\n",
      "provides a very simple way to achieve this, via the scoring argument that can be used\n",
      "in both GridSearchCV and cross_val_score. Y ou can simply provide a string\n",
      "describing the evaluation metric you want to use. Say, for example, we want to evalu‐\n",
      "ate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC\n",
      "score. Changing the score from the default (accuracy) to AUC can be done by provid‐\n",
      "ing \"roc_auc\" as the scoring parameter:\n",
      "In[67]:\n",
      "# default scoring for classification is accuracy\n",
      "print(\"Default scoring: {}\".format(\n",
      "    cross_val_score(SVC(), digits.data, digits.target == 9)))\n",
      "# providing scoring=\"accuracy\" doesn't change the results\n",
      "explicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\n",
      "                                     scoring=\"accuracy\")\n",
      "print(\"Explicit accuracy scoring: {}\".format(explicit_accuracy))\n",
      "roc_auc =  cross_val_score(SVC(), digits.data, digits.target == 9,\n",
      "                           scoring=\"roc_auc\")\n",
      "print(\"AUC scoring: {}\".format(roc_auc))\n",
      "Out[67]:\n",
      "Default scoring: [ 0.9  0.9  0.9]\n",
      "Explicit accuracy scoring: [ 0.9  0.9  0.9]\n",
      "AUC scoring: [ 0.994  0.99   0.996]\n",
      "Similarly, we can change the metric used to pick the best parameters in Grid\n",
      "SearchCV:\n",
      "In[68]:\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    digits.data, digits.target == 9, random_state=0)\n",
      "# we provide a somewhat bad grid to illustrate the point:\n",
      "param_grid = {'gamma': [0.0001, 0.01, 0.1, 1, 10]}\n",
      "# using the default scoring of accuracy:\n",
      "grid = GridSearchCV(SVC(), param_grid=param_grid)\n",
      "grid.fit(X_train, y_train)\n",
      "print(\"Grid-Search with accuracy\")\n",
      "print(\"Best parameters:\", grid.best_params_)\n",
      "print(\"Best cross-validation score (accuracy)): {:.3f}\".format(grid.best_score_))\n",
      "print(\"Test set AUC: {:.3f}\".format(\n",
      "    roc_auc_score(y_test, grid.decision_function(X_test))))\n",
      "print(\"Test set accuracy: {:.3f}\".format(grid.score(X_test, y_test)))\n",
      "300 | Chapter 5: Model Evaluation and Improvement\n",
      "6 Finding a higher-accuracy solution using AUC is likely a consequence of accuracy being a bad measure of\n",
      "model performance on imbalanced data.\n",
      "Out[68]:\n",
      "Grid-Search with accuracy\n",
      "Best parameters: {'gamma': 0.0001}\n",
      "Best cross-validation score (accuracy)): 0.970\n",
      "Test set AUC: 0.992\n",
      "Test set accuracy: 0.973\n",
      "In[69]:\n",
      "# using AUC scoring instead:\n",
      "grid = GridSearchCV(SVC(), param_grid=param_grid, scoring=\"roc_auc\")\n",
      "grid.fit(X_train, y_train)\n",
      "print(\"\\nGrid-Search with AUC\")\n",
      "print(\"Best parameters:\", grid.best_params_)\n",
      "print(\"Best cross-validation score (AUC): {:.3f}\".format(grid.best_score_))\n",
      "print(\"Test set AUC: {:.3f}\".format(\n",
      "    roc_auc_score(y_test, grid.decision_function(X_test))))\n",
      "print(\"Test set accuracy: {:.3f}\".format(grid.score(X_test, y_test)))\n",
      "Out[69]:\n",
      "Grid-Search with AUC\n",
      "Best parameters: {'gamma': 0.01}\n",
      "Best cross-validation score (AUC): 0.997\n",
      "Test set AUC: 1.000\n",
      "Test set accuracy: 1.000\n",
      "When using accuracy, the parameter gamma=0.0001 is selected, while gamma=0.01 is\n",
      "selected when using AUC. The cross-validation accuracy is consistent with the test set\n",
      "accuracy in both cases. However, using AUC found a better parameter setting in\n",
      "terms of AUC and even in terms of accuracy.6\n",
      "The most important values for the scoring parameter for classification are accuracy\n",
      "(the default); roc_auc for the area under the ROC curve; average_precision for the\n",
      "area under the precision-recall curve; f1, f1_macro, f1_micro, and f1_weighted for\n",
      "the binary f1-score and the different weighted variants. For regression, the most com‐\n",
      "monly used values are r2 for the R2 score, mean_squared_error for mean squared\n",
      "error, and mean_absolute_error for mean absolute error. Y ou can find a full list of\n",
      "supported arguments in the documentation or by looking at the SCORER dictionary\n",
      "defined in the metrics.scorer module:\n",
      "Evaluation Metrics and Scoring | 301\n",
      "7 We highly recommend Foster Provost and Tom Fawcett’s book Data Science for Business (O’Reilly) for more\n",
      "information on this topic.\n",
      "In[70]:\n",
      "from sklearn.metrics.scorer import SCORERS\n",
      "print(\"Available scorers:\\n{}\".format(sorted(SCORERS.keys())))\n",
      "Out[70]:\n",
      "Available scorers:\n",
      "['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro',\n",
      " 'f1_micro', 'f1_samples', 'f1_weighted', 'log_loss', 'mean_absolute_error',\n",
      " 'mean_squared_error', 'median_absolute_error', 'precision', 'precision_macro',\n",
      " 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall',\n",
      " 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']\n",
      "Summary and Outlook\n",
      "In this chapter we discussed cross-validation, grid search, and evaluation metrics, the\n",
      "cornerstones of evaluating and improving machine learning algorithms. The tools\n",
      "described in this chapter, together with the algorithms described in Chapters 2 and 3,\n",
      "are the bread and butter of every machine learning practitioner.\n",
      "There are two particular points that we made in this chapter that warrant repeating,\n",
      "because they are often overlooked by new practitioners. The first has to do with\n",
      "cross-validation. Cross-validation or the use of a test set allow us to evaluate a\n",
      "machine learning model as it will perform in the future. However, if we use the test\n",
      "set or cross-validation to select a model or select model parameters, we “use up” the\n",
      "test data, and using the same data to evaluate how well our model will do in the future\n",
      "will lead to overly optimistic estimates. We therefore need to resort to a split into\n",
      "training data for model building, validation data for model and parameter selection,\n",
      "and test data for model evaluation. Instead of a simple split, we can replace each of\n",
      "these splits with cross-validation. The most commonly used form (as described ear‐\n",
      "lier) is a training/test split for evaluation, and using cross-validation on the training\n",
      "set for model and parameter selection.\n",
      "The second point has to do with the importance of the evaluation metric or scoring\n",
      "function used for model selection and model evaluation. The theory of how to make\n",
      "business decisions from the predictions of a machine learning model is somewhat\n",
      "beyond the scope of this book. 7 However, it is rarely the case that the end goal of a\n",
      "machine learning task is building a model with a high accuracy. Make sure that the\n",
      "metric you choose to evaluate and select a model for is a good stand-in for what the\n",
      "model will actually be used for. In reality, classification problems rarely have balanced\n",
      "classes, and often false positives and false negatives have very different consequences.\n",
      "302 | Chapter 5: Model Evaluation and Improvement\n",
      "Make sure you understand what these consequences are, and pick an evaluation met‐\n",
      "ric accordingly.\n",
      "The model evaluation and selection techniques we have described so far are the most\n",
      "important tools in a data scientist’s toolbox. Grid search and cross-validation as we’ve\n",
      "described them in this chapter can only be applied to a single supervised model. We\n",
      "have seen before, however, that many models require preprocessing, and that in some\n",
      "applications, like the face recognition example in Chapter 3 , extracting a different\n",
      "representation of the data can be useful. In the next chapter, we will introduce the\n",
      "Pipeline class, which allows us to use grid search and cross-validation on these com‐\n",
      "plex chains of algorithms.\n",
      "Summary and Outlook | 303\n",
      "\n",
      "CHAPTER 6\n",
      "Algorithm Chains and Pipelines\n",
      "For many machine learning algorithms, the particular representation of the data that\n",
      "you provide is very important, as we discussed in Chapter 4. This starts with scaling\n",
      "the data and combining features by hand and goes all the way to learning features\n",
      "using unsupervised machine learning, as we saw in Chapter 3. Consequently, most\n",
      "machine learning applications require not only the application of a single algorithm,\n",
      "but the chaining together of many different processing steps and machine learning\n",
      "models. In this chapter, we will cover how to use the Pipeline class to simplify the\n",
      "process of building chains of transformations and models. In particular, we will see\n",
      "how we can combine Pipeline and GridSearchCV to search over parameters for all\n",
      "processing steps at once.\n",
      "As an example of the importance of chaining models, we noticed that we can greatly\n",
      "improve the performance of a kernel SVM on the cancer dataset by using the Min\n",
      "MaxScaler for preprocessing. Here’s code for splitting the data, computing the mini‐\n",
      "mum and maximum, scaling the data, and training the SVM:\n",
      "In[1]:\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.datasets import load_breast_cancer\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "# load and split the data\n",
      "cancer = load_breast_cancer()\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    cancer.data, cancer.target, random_state=0)\n",
      "# compute minimum and maximum on the training data\n",
      "scaler = MinMaxScaler().fit(X_train)\n",
      "305\n",
      "In[2]:\n",
      "# rescale the training data\n",
      "X_train_scaled = scaler.transform(X_train)\n",
      "svm = SVC()\n",
      "# learn an SVM on the scaled training data\n",
      "svm.fit(X_train_scaled, y_train)\n",
      "# scale the test data and score the scaled data\n",
      "X_test_scaled = scaler.transform(X_test)\n",
      "print(\"Test score: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\n",
      "Out[2]:\n",
      "Test score: 0.95\n",
      "Parameter Selection with Preprocessing\n",
      "Now let’s say we want to find better parameters for SVC using GridSearchCV, as dis‐\n",
      "cussed in Chapter 5. How should we go about doing this? A naive approach might\n",
      "look like this:\n",
      "In[3]:\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "# for illustration purposes only, don't use this code!\n",
      "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
      "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
      "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\n",
      "grid.fit(X_train_scaled, y_train)\n",
      "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\n",
      "print(\"Best set score: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\n",
      "print(\"Best parameters: \", grid.best_params_)\n",
      "Out[3]:\n",
      "Best cross-validation accuracy: 0.98\n",
      "Best set score: 0.97\n",
      "Best parameters:  {'gamma': 1, 'C': 1}\n",
      "Here, we ran the grid search over the parameters of SVC using the scaled data. How‐\n",
      "ever, there is a subtle catch in what we just did. When scaling the data, we used all the\n",
      "data in the training set to find out how to train it. We then use the scaled training data\n",
      "to run our grid search using cross-validation. For each split in the cross-validation,\n",
      "some part of the original training set will be declared the training part of the split,\n",
      "and some the test part of the split. The test part is used to measure what new data will\n",
      "look like to a model trained on the training part. However, we already used the infor‐\n",
      "mation contained in the test part of the split, when scaling the data. Remember that\n",
      "the test part in each split in the cross-validation is part of the training set, and we\n",
      "used the information from the entire training set to find the right scaling of the data.\n",
      "306 | Chapter 6: Algorithm Chains and Pipelines\n",
      "This is fundamentally different from how new data looks to the model.  If we observe\n",
      "new data (say, in form of our test set), this data will not have been used to scale the\n",
      "training data, and it might have a different minimum and maximum than the train‐\n",
      "ing data. The following example ( Figure 6-1) shows how the data processing during\n",
      "cross-validation and the final evaluation differ:\n",
      "In[4]:\n",
      "mglearn.plots.plot_improper_processing()\n",
      "Figure 6-1. Data usage when preprocessing outside the cross-validation loop\n",
      "So, the splits in the cross-validation no longer correctly mirror how new data will\n",
      "look to the modeling process. We already leaked information from these parts of the\n",
      "data into our modeling process. This will lead to overly optimistic results during\n",
      "cross-validation, and possibly the selection of suboptimal parameters.\n",
      "To get around this problem, the splitting of the dataset during cross-validation should\n",
      "be done before doing any preprocessing. Any process that extracts knowledge from the\n",
      "dataset should only ever be applied to the training portion of the dataset, so any\n",
      "cross-validation should be the “outermost loop” in your processing.\n",
      "To achieve this in scikit-learn with the cross_val_score function and the Grid\n",
      "SearchCV function, we can use the Pipeline class. The Pipeline class is a class that\n",
      "allows “gluing” together multiple processing steps into a single scikit-learn estima‐\n",
      "Parameter Selection with Preprocessing | 307\n",
      "1 With one exception: the name can’t contain a double underscore, __.\n",
      "tor. The Pipeline class itself has fit, predict, and score methods and behaves just\n",
      "like any other model in scikit-learn. The most common use case of the Pipeline\n",
      "class is in chaining preprocessing steps (like scaling of the data) together with a\n",
      "supervised model like a classifier.\n",
      "Building Pipelines\n",
      "Let’s look at how we can use the Pipeline class to express the workflow for training\n",
      "an SVM after scaling the data with MinMaxScaler (for now without the grid search).\n",
      "First, we build a pipeline object by providing it with a list of steps. Each step is a tuple\n",
      "containing a name (any string of your choosing1) and an instance of an estimator:\n",
      "In[5]:\n",
      "from sklearn.pipeline import Pipeline\n",
      "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\n",
      "Here, we created two steps: the first, called \"scaler\", is an instance of MinMaxScaler,\n",
      "and the second, called \"svm\", is an instance of SVC. Now, we can fit the pipeline, like\n",
      "any other scikit-learn estimator:\n",
      "In[6]:\n",
      "pipe.fit(X_train, y_train)\n",
      "Here, pipe.fit first calls fit on the first step (the scaler), then transforms the train‐\n",
      "ing data using the scaler, and finally fits the SVM with the scaled data. To evaluate on\n",
      "the test data, we simply call pipe.score:\n",
      "In[7]:\n",
      "print(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))\n",
      "Out[7]:\n",
      "Test score: 0.95\n",
      "Calling the score method on the pipeline first transforms the test data using the\n",
      "scaler, and then calls the score method on the SVM using the scaled test data. As you\n",
      "can see, the result is identical to the one we got from the code at the beginning of the\n",
      "chapter, when doing the transformations by hand. Using the pipeline, we reduced the\n",
      "code needed for our “preprocessing + classification” process. The main benefit of\n",
      "using the pipeline, however, is that we can now use this single estimator in\n",
      "cross_val_score or GridSearchCV.\n",
      "308 | Chapter 6: Algorithm Chains and Pipelines\n",
      "Using Pipelines in Grid Searches\n",
      "Using a pipeline in a grid search works the same way as using any other estimator. We\n",
      "define a parameter grid to search over, and construct a GridSearchCV from the pipe‐\n",
      "line and the parameter grid. When specifying the parameter grid, there is a slight\n",
      "change, though. We need to specify for each parameter which step of the pipeline it\n",
      "belongs to. Both parameters that we want to adjust, C and gamma, are parameters of\n",
      "SVC, the second step. We gave this step the name \"svm\". The syntax to define a param‐\n",
      "eter grid for a pipeline is to specify for each parameter the step name, followed by __\n",
      "(a double underscore), followed by the parameter name. To search over the C param‐\n",
      "eter of SVC we therefore have to use \"svm__C\" as the key in the parameter grid dictio‐\n",
      "nary, and similarly for gamma:\n",
      "In[8]:\n",
      "param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
      "              'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
      "With this parameter grid we can use GridSearchCV as usual:\n",
      "In[9]:\n",
      "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
      "grid.fit(X_train, y_train)\n",
      "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\n",
      "print(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
      "print(\"Best parameters: {}\".format(grid.best_params_))\n",
      "Out[9]:\n",
      "Best cross-validation accuracy: 0.98\n",
      "Test set score: 0.97\n",
      "Best parameters: {'svm__C': 1, 'svm__gamma': 1}\n",
      "In contrast to the grid search we did before, now for each split in the cross-validation,\n",
      "the MinMaxScaler is refit with only the training splits and no information is leaked\n",
      "from the test split into the parameter search. Compare this ( Figure 6-2 ) with\n",
      "Figure 6-1 earlier in this chapter:\n",
      "In[10]:\n",
      "mglearn.plots.plot_proper_processing()\n",
      "Using Pipelines in Grid Searches | 309\n",
      "Figure 6-2. Data usage when preprocessing inside the cross-validation loop with a\n",
      "pipeline\n",
      "The impact of leaking information in the cross-validation varies depending on the\n",
      "nature of the preprocessing step. Estimating the scale of the data using the test fold\n",
      "usually doesn’t have a terrible impact, while using the test fold in feature extraction\n",
      "and feature selection can lead to substantial differences in outcomes.\n",
      "Illustrating Information Leakage\n",
      "A great example of leaking information in cross-validation is given in Hastie, Tibshir‐\n",
      "ani, and Friedman’s book The Elements of Statistical Learning , and we reproduce an\n",
      "adapted version here. Let’s consider a synthetic regression task with 100 samples and\n",
      "1,000 features that are sampled independently from a Gaussian distribution. We also\n",
      "sample the response from a Gaussian distribution:\n",
      "In[11]:\n",
      "rnd = np.random.RandomState(seed=0)\n",
      "X = rnd.normal(size=(100, 10000))\n",
      "y = rnd.normal(size=(100,))\n",
      "Given the way we created the dataset, there is no relation between the data, X, and the\n",
      "target, y (they are independent), so it should not be possible to learn anything from\n",
      "this dataset. We will now do the following. First, select the most informative of the 10\n",
      "features using SelectPercentile feature selection, and then we evaluate a Ridge\n",
      "regressor using cross-validation:\n",
      "310 | Chapter 6: Algorithm Chains and Pipelines\n",
      "In[12]:\n",
      "from sklearn.feature_selection import SelectPercentile, f_regression\n",
      "select = SelectPercentile(score_func=f_regression, percentile=5).fit(X, y)\n",
      "X_selected = select.transform(X)\n",
      "print(\"X_selected.shape: {}\".format(X_selected.shape))\n",
      "Out[12]:\n",
      "X_selected.shape: (100, 500)\n",
      "In[13]:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.linear_model import Ridge\n",
      "print(\"Cross-validation accuracy (cv only on ridge): {:.2f}\".format(\n",
      "      np.mean(cross_val_score(Ridge(), X_selected, y, cv=5))))\n",
      "Out[13]:\n",
      "Cross-validation accuracy (cv only on ridge): 0.91\n",
      "The mean R2 computed by cross-validation is 0.91, indicating a very good model.\n",
      "This clearly cannot be right, as our data is entirely random. What happened here is\n",
      "that our feature selection picked out some features among the 10,000 random features\n",
      "that are (by chance) very well correlated with the target. Because we fit the feature\n",
      "selection outside of the cross-validation, it could find features that are correlated both\n",
      "on the training and the test folds. The information we leaked from the test folds was\n",
      "very informative, leading to highly unrealistic results. Let’s compare this to a proper\n",
      "cross-validation using a pipeline:\n",
      "In[14]:\n",
      "pipe = Pipeline([(\"select\", SelectPercentile(score_func=f_regression,\n",
      "                                             percentile=5)),\n",
      "                 (\"ridge\", Ridge())])\n",
      "print(\"Cross-validation accuracy (pipeline): {:.2f}\".format(\n",
      "      np.mean(cross_val_score(pipe, X, y, cv=5))))\n",
      "Out[14]:\n",
      "Cross-validation accuracy (pipeline): -0.25\n",
      "This time, we get a negative R2 score, indicating a very poor model. Using the pipe‐\n",
      "line, the feature selection is now inside the cross-validation loop. This means features\n",
      "can only be selected using the training folds of the data, not the test fold. The feature\n",
      "selection finds features that are correlated with the target on the training set, but\n",
      "because the data is entirely random, these features are not correlated with the target\n",
      "on the test set. In this example, rectifying the data leakage issue in the feature selec‐\n",
      "tion makes the difference between concluding that a model works very well and con‐\n",
      "cluding that a model works not at all.\n",
      "Using Pipelines in Grid Searches | 311\n",
      "2 Or just fit_transform.\n",
      "The General Pipeline Interface\n",
      "The Pipeline class is not restricted to preprocessing and classification, but can in\n",
      "fact join any number of estimators together. For example, you could build a pipeline\n",
      "containing feature extraction, feature selection, scaling, and classification, for a total\n",
      "of four steps. Similarly, the last step could be regression or clustering instead of classi‐\n",
      "fication.\n",
      "The only requirement for estimators in a pipeline is that all but the last step need to\n",
      "have a transform method, so they can produce a new representation of the data that\n",
      "can be used in the next step.\n",
      "Internally, during the call to Pipeline.fit, the pipeline calls fit and then transform\n",
      "on each step in turn, 2 with the input given by the output of the transform method of\n",
      "the previous step. For the last step in the pipeline, just fit is called.\n",
      "Brushing over some finer details, this is implemented as follows. Remember that pipe\n",
      "line.steps is a list of tuples, so pipeline.steps[0][1] is the first estimator, pipe\n",
      "line.steps[1][1] is the second estimator, and so on:\n",
      "In[15]:\n",
      "def fit(self, X, y):\n",
      "    X_transformed = X\n",
      "    for name, estimator in self.steps[:-1]:\n",
      "        # iterate over all but the final step\n",
      "        # fit and transform the data\n",
      "        X_transformed = estimator.fit_transform(X_transformed, y)\n",
      "    # fit the last step\n",
      "    self.steps[-1][1].fit(X_transformed, y)\n",
      "    return self\n",
      "When predicting using Pipeline, we similarly transform the data using all but the\n",
      "last step, and then call predict on the last step:\n",
      "In[16]:\n",
      "def predict(self, X):\n",
      "    X_transformed = X\n",
      "    for step in self.steps[:-1]:\n",
      "        # iterate over all but the final step\n",
      "        # transform the data\n",
      "        X_transformed = step[1].transform(X_transformed)\n",
      "    # fit the last step\n",
      "    return self.steps[-1][1].predict(X_transformed)\n",
      "312 | Chapter 6: Algorithm Chains and Pipelines\n",
      "The process is illustrated in Figure 6-3  for two transformers, T1 and T2, and a\n",
      "classifier (called Classifier).\n",
      "Figure 6-3. Overview of the pipeline training and prediction process\n",
      "The pipeline is actually even more general than this. There is no requirement for the\n",
      "last step in a pipeline to have a predict function, and we could create a pipeline just\n",
      "containing, for example, a scaler and PCA. Then, because the last step ( PCA) has a\n",
      "transform method, we could call transform on the pipeline to get the output of\n",
      "PCA.transform applied to the data that was processed by the previous step. The last\n",
      "step of a pipeline is only required to have a fit method.\n",
      "Convenient Pipeline Creation with make_pipeline\n",
      "Creating a pipeline using the syntax described earlier is sometimes a bit cumbersome,\n",
      "and we often don’t need user-specified names for each step. There is a convenience\n",
      "function, make_pipeline, that will create a pipeline for us and automatically name\n",
      "each step based on its class. The syntax for make_pipeline is as follows:\n",
      "In[17]:\n",
      "from sklearn.pipeline import make_pipeline\n",
      "# standard syntax\n",
      "pipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n",
      "# abbreviated syntax\n",
      "pipe_short = make_pipeline(MinMaxScaler(), SVC(C=100))\n",
      "The General Pipeline Interface | 313\n",
      "The pipeline objects pipe_long and pipe_short do exactly the same thing, but\n",
      "pipe_short has steps that were automatically named. We can see the names of the\n",
      "steps by looking at the steps attribute:\n",
      "In[18]:\n",
      "print(\"Pipeline steps:\\n{}\".format(pipe_short.steps))\n",
      "Out[18]:\n",
      "Pipeline steps:\n",
      "[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
      " ('svc', SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
      "      decision_function_shape=None, degree=3, gamma='auto',\n",
      "             kernel='rbf', max_iter=-1, probability=False,\n",
      "             random_state=None, shrinking=True, tol=0.001,\n",
      "             verbose=False))]\n",
      "The steps are named minmaxscaler and svc. In general, the step names are just low‐\n",
      "ercase versions of the class names. If multiple steps have the same class, a number is\n",
      "appended:\n",
      "In[19]:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "pipe = make_pipeline(StandardScaler(), PCA(n_components=2), StandardScaler())\n",
      "print(\"Pipeline steps:\\n{}\".format(pipe.steps))\n",
      "Out[19]:\n",
      "Pipeline steps:\n",
      "[('standardscaler-1', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      " ('pca', PCA(copy=True, iterated_power=4, n_components=2, random_state=None,\n",
      "             svd_solver='auto', tol=0.0, whiten=False)),\n",
      " ('standardscaler-2', StandardScaler(copy=True, with_mean=True, with_std=True))]\n",
      "As you can see, the first StandardScaler step was named standardscaler-1 and the\n",
      "second standardscaler-2. However, in such settings it might be better to use the\n",
      "Pipeline construction with explicit names, to give more semantic names to each\n",
      "step.\n",
      "Accessing Step Attributes\n",
      "Often you will want to inspect attributes of one of the steps of the pipeline—say, the\n",
      "coefficients of a linear model or the components extracted by PCA. The easiest way to\n",
      "access the steps in a pipeline is via the named_steps attribute, which is a dictionary\n",
      "from the step names to the estimators:\n",
      "314 | Chapter 6: Algorithm Chains and Pipelines\n",
      "In[20]:\n",
      "# fit the pipeline defined before to the cancer dataset\n",
      "pipe.fit(cancer.data)\n",
      "# extract the first two principal components from the \"pca\" step\n",
      "components = pipe.named_steps[\"pca\"].components_\n",
      "print(\"components.shape: {}\".format(components.shape))\n",
      "Out[20]:\n",
      "components.shape: (2, 30)\n",
      "Accessing Attributes in a Grid-Searched Pipeline\n",
      "As we discussed earlier in this chapter, one of the main reasons to use pipelines is for\n",
      "doing grid searches. A common task is to access some of the steps of a pipeline inside\n",
      "a grid search. Let’s grid search a LogisticRegression classifier on the cancer dataset,\n",
      "using Pipeline and StandardScaler to scale the data before passing it to the Logisti\n",
      "cRegression classifier. First we create a pipeline using the make_pipeline function:\n",
      "In[21]:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
      "Next, we create a parameter grid. As explained in Chapter 2 , the regularization\n",
      "parameter to tune for LogisticRegression is the parameter C. We use a logarithmic\n",
      "grid for this parameter, searching between 0.01 and 100. Because we used the\n",
      "make_pipeline function, the name of the LogisticRegression step in the pipeline is\n",
      "the lowercased class name, logisticregression. To tune the parameter C, we there‐\n",
      "fore have to specify a parameter grid for logisticregression__C:\n",
      "In[22]:\n",
      "param_grid = {'logisticregression__C': [0.01, 0.1, 1, 10, 100]}\n",
      "As usual, we split the cancer dataset into training and test sets, and fit a grid search:\n",
      "In[23]:\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    cancer.data, cancer.target, random_state=4)\n",
      "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
      "grid.fit(X_train, y_train)\n",
      "So how do we access the coefficients of the best LogisticRegression model that was\n",
      "found by GridSearchCV? From Chapter 5  we know that the best model found by\n",
      "GridSearchCV, trained on all the training data, is stored in grid.best_estimator_:\n",
      "The General Pipeline Interface | 315\n",
      "In[24]:\n",
      "print(\"Best estimator:\\n{}\".format(grid.best_estimator_))\n",
      "Out[24]:\n",
      "Best estimator:\n",
      "Pipeline(steps=[\n",
      "    ('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "    ('logisticregression', LogisticRegression(C=0.1, class_weight=None,\n",
      "    dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "    multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "    solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "This best_estimator_ in our case is a pipeline with two steps, standardscaler and\n",
      "logisticregression. To access the logisticregression step, we can use the\n",
      "named_steps attribute of the pipeline, as explained earlier:\n",
      "In[25]:\n",
      "print(\"Logistic regression step:\\n{}\".format(\n",
      "      grid.best_estimator_.named_steps[\"logisticregression\"]))\n",
      "Out[25]:\n",
      "Logistic regression step:\n",
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                  intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "                  penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "                  verbose=0, warm_start=False)\n",
      "Now that we have the trained LogisticRegression instance, we can access the coeffi‐\n",
      "cients (weights) associated with each input feature:\n",
      "In[26]:\n",
      "print(\"Logistic regression coefficients:\\n{}\".format(\n",
      "      grid.best_estimator_.named_steps[\"logisticregression\"].coef_))\n",
      "Out[26]:\n",
      "Logistic regression coefficients:\n",
      "[[-0.389 -0.375 -0.376 -0.396 -0.115  0.017 -0.355 -0.39  -0.058  0.209\n",
      "  -0.495 -0.004 -0.371 -0.383 -0.045  0.198  0.004 -0.049  0.21   0.224\n",
      "  -0.547 -0.525 -0.499 -0.515 -0.393 -0.123 -0.388 -0.417 -0.325 -0.139]]\n",
      "This might be a somewhat lengthy expression, but often it comes in handy in under‐\n",
      "standing your models.\n",
      "316 | Chapter 6: Algorithm Chains and Pipelines\n",
      "Grid-Searching Preprocessing Steps and Model\n",
      "Parameters\n",
      "Using pipelines, we can encapsulate all the processing steps in our machine learning\n",
      "workflow in a single scikit-learn estimator. Another benefit of doing this is that we\n",
      "can now adjust the parameters of the preprocessing using the outcome of a supervised\n",
      "task like regression or classification. In previous chapters, we used polynomial fea‐\n",
      "tures on the boston dataset before applying the ridge regressor. Let’s model that using\n",
      "a pipeline instead. The pipeline contains three steps—scaling the data, computing\n",
      "polynomial features, and ridge regression:\n",
      "In[27]:\n",
      "from sklearn.datasets import load_boston\n",
      "boston = load_boston()\n",
      "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n",
      "                                                    random_state=0)\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "pipe = make_pipeline(\n",
      "    StandardScaler(),\n",
      "    PolynomialFeatures(),\n",
      "    Ridge())\n",
      "How do we know which degrees of polynomials to choose, or whether to choose any\n",
      "polynomials or interactions at all? Ideally we want to select the degree parameter\n",
      "based on the outcome of the classification. Using our pipeline, we can search over the\n",
      "degree parameter together with the parameter alpha of Ridge. To do this, we define a\n",
      "param_grid that contains both, appropriately prefixed by the step names:\n",
      "In[28]:\n",
      "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n",
      "              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
      "Now we can run our grid search again:\n",
      "In[29]:\n",
      "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
      "grid.fit(X_train, y_train)\n",
      "We can visualize the outcome of the cross-validation using a heat map (Figure 6-4), as\n",
      "we did in Chapter 5:\n",
      "In[30]:\n",
      "plt.matshow(grid.cv_results_['mean_test_score'].reshape(3, -1),\n",
      "            vmin=0, cmap=\"viridis\")\n",
      "plt.xlabel(\"ridge__alpha\")\n",
      "plt.ylabel(\"polynomialfeatures__degree\")\n",
      "Grid-Searching Preprocessing Steps and Model Parameters | 317\n",
      "plt.xticks(range(len(param_grid['ridge__alpha'])), param_grid['ridge__alpha'])\n",
      "plt.yticks(range(len(param_grid['polynomialfeatures__degree'])),\n",
      "           param_grid['polynomialfeatures__degree'])\n",
      "plt.colorbar()\n",
      "Figure 6-4. Heat map of mean cross-validation score as a function of the degree of the\n",
      "polynomial features and alpha parameter of Ridge\n",
      "Looking at the results produced by the cross-validation, we can see that using polyno‐\n",
      "mials of degree two helps, but that degree-three polynomials are much worse than\n",
      "either degree one or two. This is reflected in the best parameters that were found:\n",
      "In[31]:\n",
      "print(\"Best parameters: {}\".format(grid.best_params_))\n",
      "Out[31]:\n",
      "Best parameters: {'polynomialfeatures__degree': 2, 'ridge__alpha': 10}\n",
      "Which lead to the following score:\n",
      "In[32]:\n",
      "print(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
      "Out[32]:\n",
      "Test-set score: 0.77\n",
      "Let’s run a grid search without polynomial features for comparison:\n",
      "In[33]:\n",
      "param_grid = {'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
      "pipe = make_pipeline(StandardScaler(), Ridge())\n",
      "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
      "grid.fit(X_train, y_train)\n",
      "print(\"Score without poly features: {:.2f}\".format(grid.score(X_test, y_test)))\n",
      "318 | Chapter 6: Algorithm Chains and Pipelines\n",
      "Out[33]:\n",
      "Score without poly features: 0.63\n",
      "As we would expect looking at the grid search results visualized in Figure 6-4, using\n",
      "no polynomial features leads to decidedly worse results.\n",
      "Searching over preprocessing parameters together with model parameters is a very\n",
      "powerful strategy. However, keep in mind that GridSearchCV tries all possible combi‐\n",
      "nations of the specified parameters. Therefore, adding more parameters to your grid\n",
      "exponentially increases the number of models that need to be built.\n",
      "Grid-Searching Which Model To Use\n",
      "Y ou can even go further in combining GridSearchCV and Pipeline: it is also possible\n",
      "to search over the actual steps being performed in the pipeline (say whether to use\n",
      "StandardScaler or MinMaxScaler). This leads to an even bigger search space and\n",
      "should be considered carefully. Trying all possible solutions is usually not a viable\n",
      "machine learning strategy. However, here is an example comparing a RandomForest\n",
      "Classifier and an SVC on the iris dataset. We know that the SVC might need the\n",
      "data to be scaled, so we also search over whether to use StandardScaler or no pre‐\n",
      "processing. For the RandomForestClassifier, we know that no preprocessing is nec‐\n",
      "essary. We start by defining the pipeline. Here, we explicitly name the steps. We want\n",
      "two steps, one for the preprocessing and then a classifier. We can instantiate this\n",
      "using SVC and StandardScaler:\n",
      "In[34]:\n",
      "pipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', SVC())])\n",
      "Now we can define the parameter_grid to search over. We want the classifier to\n",
      "be either RandomForestClassifier or SVC. Because they have different parameters to\n",
      "tune, and need different preprocessing, we can make use of the list of search grids we\n",
      "discussed in “Search over spaces that are not grids” on page 271. To assign an estima‐\n",
      "tor to a step, we use the name of the step as the parameter name. When we wanted to\n",
      "skip a step in the pipeline (for example, because we don’t need preprocessing for the\n",
      "RandomForest), we can set that step to None:\n",
      "In[35]:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "param_grid = [\n",
      "    {'classifier': [SVC()], 'preprocessing': [StandardScaler(), None],\n",
      "     'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
      "     'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
      "    {'classifier': [RandomForestClassifier(n_estimators=100)],\n",
      "     'preprocessing': [None], 'classifier__max_features': [1, 2, 3]}]\n",
      "Grid-Searching Which Model To Use | 319\n",
      "Now we can instantiate and run the grid search as usual, here on the cancer dataset:\n",
      "In[36]:\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    cancer.data, cancer.target, random_state=0)\n",
      "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
      "grid.fit(X_train, y_train)\n",
      "print(\"Best params:\\n{}\\n\".format(grid.best_params_))\n",
      "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
      "print(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
      "Out[36]:\n",
      "Best params:\n",
      "{'classifier':\n",
      " SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "     decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "     max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "     tol=0.001, verbose=False),\n",
      " 'preprocessing':\n",
      " StandardScaler(copy=True, with_mean=True, with_std=True),\n",
      " 'classifier__C': 10, 'classifier__gamma': 0.01}\n",
      "Best cross-validation score: 0.99\n",
      "Test-set score: 0.98\n",
      "The outcome of the grid search is that SVC with StandardScaler preprocessing, C=10,\n",
      "and gamma=0.01 gave the best result.\n",
      "Summary and Outlook\n",
      "In this chapter we introduced the Pipeline class, a general-purpose tool to chain\n",
      "together multiple processing steps in a machine learning workflow. Real-world appli‐\n",
      "cations of machine learning rarely involve an isolated use of a model, and instead are\n",
      "a sequence of processing steps. Using pipelines allows us to encapsulate multiple steps\n",
      "into a single Python object that adheres to the familiar scikit-learn interface of fit,\n",
      "predict, and transform. In particular when doing model evaluation using cross-\n",
      "validation and parameter selection using grid search, using the Pipeline class to cap‐\n",
      "ture all the processing steps is essential for proper evaluation. The Pipeline class also\n",
      "allows writing more succinct code, and reduces the likelihood of mistakes that can\n",
      "happen when building processing chains without the pipeline class (like forgetting\n",
      "to apply all transformers on the test set, or not applying them in the right order).\n",
      "Choosing the right combination of feature extraction, preprocessing, and models is\n",
      "somewhat of an art, and often requires some trial and error. However, using pipe‐\n",
      "lines, this “trying out” of many different processing steps is quite simple. When\n",
      "320 | Chapter 6: Algorithm Chains and Pipelines\n",
      "experimenting, be careful not to overcomplicate your processes, and make sure to\n",
      "evaluate whether every component you are including in your model is necessary.\n",
      "With this chapter, we have completed our survey of general-purpose tools and algo‐\n",
      "rithms provided by scikit-learn. Y ou now possess all the required skills and know\n",
      "the necessary mechanisms to apply machine learning in practice. In the next chapter,\n",
      "we will dive in more detail into one particular type of data that is commonly seen in\n",
      "practice, and that requires some special expertise to handle correctly: text data.\n",
      "Summary and Outlook | 321\n",
      "\n",
      "CHAPTER 7\n",
      "Working with Text Data\n",
      "In Chapter 4, we talked about two kinds of features that can represent properties of\n",
      "the data: continuous features that describe a quantity, and categorical features that are\n",
      "items from a fixed list. There is a third kind of feature that can be found in many\n",
      "applications, which is text. For example, if we want to classify an email message as\n",
      "either a legitimate email or spam, the content of the email will certainly contain\n",
      "important information for this classification task. Or maybe we want to learn about\n",
      "the opinion of a politician on the topic of immigration. Here, that individual’s\n",
      "speeches or tweets might provide useful information. In customer service, we often\n",
      "want to find out if a message is a complaint or an inquiry. We can use the subject line\n",
      "and content of a message to automatically determine the customer’s intent, which\n",
      "allows us to send the message to the appropriate department, or even send a fully\n",
      "automatic reply.\n",
      "Text data is usually represented as strings, made up of characters. In any of the exam‐\n",
      "ples just given, the length of the text data will vary. This feature is clearly very differ‐\n",
      "ent from the numeric features that we’ve discussed so far, and we will need to process\n",
      "the data before we can apply our machine learning algorithms to it.\n",
      "Types of Data Represented as Strings\n",
      "Before we dive into the processing steps that go into representing text data for\n",
      "machine learning, we want to briefly discuss different kinds of text data that you\n",
      "might encounter. Text is usually just a string in your dataset, but not all string features\n",
      "should be treated as text. A string feature can sometimes represent categorical vari‐\n",
      "ables, as we discussed in Chapter 5. There is no way to know how to treat a string\n",
      "feature before looking at the data.\n",
      "323\n",
      "There are four kinds of string data you might see:\n",
      "• Categorical data\n",
      "• Free strings that can be semantically mapped to categories\n",
      "• Structured string data\n",
      "• Text data\n",
      "Categorical data is data that comes from a fixed list. Say you collect data via a survey\n",
      "where you ask people their favorite color, with a drop-down menu that allows them\n",
      "to select from “red, ” “green, ” “blue, ” “yellow, ” “black, ” “white, ” “purple, ” and “pink. ”\n",
      "This will result in a dataset with exactly eight different possible values, which clearly\n",
      "encode a categorical variable. Y ou can check whether this is the case for your data by\n",
      "eyeballing it (if you see very many different strings it is unlikely that this is a categori‐\n",
      "cal variable) and confirm it by computing the unique values over the dataset, and\n",
      "possibly a histogram over how often each appears. Y ou also might want to check\n",
      "whether each variable actually corresponds to a category that makes sense for your\n",
      "application. Maybe halfway through the existence of your survey, someone found that\n",
      "“black” was misspelled as “blak” and subsequently fixed the survey. As a result, your\n",
      "dataset contains both “blak” and “black, ” which correspond to the same semantic\n",
      "meaning and should be consolidated.\n",
      "Now imagine instead of providing a drop-down menu, you provide a text field for the\n",
      "users to provide their own favorite colors. Many people might respond with a color\n",
      "name like “black” or “blue. ” Others might make typographical errors, use different\n",
      "spellings like “gray” and “grey, ” or use more evocative and specific names like “mid‐\n",
      "night blue. ” Y ou will also have some very strange entries. Some good examples come\n",
      "from the xkcd Color Survey , where people had to name colors and came up with\n",
      "names like “velociraptor cloaka” and “my dentist’s office orange. I still remember his\n",
      "dandruff slowly wafting into my gaping yaw, ” which are hard to map to colors auto‐\n",
      "matically (or at all). The responses you can obtain from a text field belong to the sec‐\n",
      "ond category in the list, free strings that can be semantically mapped to categories . It\n",
      "will probably be best to encode this data as a categorical variable, where you can\n",
      "select the categories either by using the most common entries, or by defining cate‐\n",
      "gories that will capture responses in a way that makes sense for your application. Y ou\n",
      "might then have some categories for standard colors, maybe a category “multicol‐\n",
      "ored” for people that gave answers like “green and red stripes, ” and an “other” cate‐\n",
      "gory for things that cannot be encoded otherwise. This kind of preprocessing of\n",
      "strings can take a lot of manual effort and is not easily automated. If you are in a posi‐\n",
      "tion where you can influence data collection, we highly recommend avoiding man‐\n",
      "ually entered values for concepts that are better captured using categorical variables.\n",
      "Often, manually entered values do not correspond to fixed categories, but still have\n",
      "some underlying structure, like addresses, names of places or people, dates, telephone\n",
      "324 | Chapter 7: Working with Text Data\n",
      "1 Arguably, the content of websites linked to in tweets contains more information than the text of the tweets\n",
      "themselves.\n",
      "2 Most of what we will talk about in the rest of the chapter also applies to other languages that use the Roman\n",
      "alphabet, and partially to other languages with word boundary delimiters. Chinese, for example, does not\n",
      "delimit word boundaries, and has other challenges that make applying the techniques in this chapter difficult.\n",
      "3 The dataset is available at http://ai.stanford.edu/~amaas/data/sentiment/.\n",
      "numbers, or other identifiers. These kinds of strings are often very hard to parse, and\n",
      "their treatment is highly dependent on context and domain. A systematic treatment\n",
      "of these cases is beyond the scope of this book.\n",
      "The final category of string data is freeform text data that consists of phrases or sen‐\n",
      "tences. Examples include tweets, chat logs, and hotel reviews, as well as the collected\n",
      "works of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection\n",
      "of 50,000 ebooks. All of these collections contain information mostly as sentences\n",
      "composed of words. 1 For simplicity’s sake, let’s assume all our documents are in one\n",
      "language, English.2 In the context of text analysis, the dataset is often called the cor‐\n",
      "pus, and each data point, represented as a single text, is called a document. These\n",
      "terms come from the information retrieval (IR) and natural language processing (NLP)\n",
      "community, which both deal mostly in text data.\n",
      "Example Application: Sentiment Analysis of Movie\n",
      "Reviews\n",
      "As a running example in this chapter, we will use a dataset of movie reviews from the\n",
      "IMDb (Internet Movie Database) website collected by Stanford researcher Andrew\n",
      "Maas.3 This dataset contains the text of the reviews, together with a label that indi‐\n",
      "cates whether a review is “positive” or “negative. ” The IMDb website itself contains\n",
      "ratings from 1 to 10. To simplify the modeling, this annotation is summarized as a\n",
      "two-class classification dataset where reviews with a score of 6 or higher are labeled as\n",
      "positive, and the rest as negative. We will leave the question of whether this is a good\n",
      "representation of the data open, and simply use the data as provided by Andrew\n",
      "Maas.\n",
      "After unpacking the data, the dataset is provided as text files in two separate folders,\n",
      "one for the training data and one for the test data. Each of these in turn has two sub‐\n",
      "folders, one called pos and one called neg:\n",
      "Example Application: Sentiment Analysis of Movie Reviews | 325\n",
      "In[2]:\n",
      "!tree -L 2 data/aclImdb\n",
      "Out[2]:\n",
      "data/aclImdb\n",
      "├── test\n",
      "│   ├── neg\n",
      "│   └── pos\n",
      "└── train\n",
      "    ├── neg\n",
      "    └── pos\n",
      "6 directories, 0 files\n",
      "The pos folder contains all the positive reviews, each as a separate text file, and simi‐\n",
      "larly for the neg folder. There is a helper function in scikit-learn to load files stored\n",
      "in such a folder structure, where each subfolder corresponds to a label, called\n",
      "load_files. We apply the load_files function first to the training data:\n",
      "In[3]:\n",
      "from sklearn.datasets import load_files\n",
      "reviews_train = load_files(\"data/aclImdb/train/\")\n",
      "# load_files returns a bunch, containing training texts and training labels\n",
      "text_train, y_train = reviews_train.data, reviews_train.target\n",
      "print(\"type of text_train: {}\".format(type(text_train)))\n",
      "print(\"length of text_train: {}\".format(len(text_train)))\n",
      "print(\"text_train[1]:\\n{}\".format(text_train[1]))\n",
      "Out[3]:\n",
      "type of text_train:  <class 'list'>\n",
      "length of text_train:  25000\n",
      "text_train[1]:\n",
      "b'Words can\\'t describe how bad this movie is. I can\\'t explain it by writing\n",
      "  only. You have too see it for yourself to get at grip of how horrible a movie\n",
      "  really can be. Not that I recommend you to do that. There are so many\n",
      "  clich\\xc3\\xa9s, mistakes (and all other negative things you can imagine) here\n",
      "  that will just make you cry. To start with the technical first, there are a\n",
      "  LOT of mistakes regarding the airplane. I won\\'t list them here, but just\n",
      "  mention the coloring of the plane. They didn\\'t even manage to show an\n",
      "  airliner in the colors of a fictional airline, but instead used a 747\n",
      "  painted in the original Boeing livery. Very bad. The plot is stupid and has\n",
      "  been done many times before, only much, much better. There are so many\n",
      "  ridiculous moments here that i lost count of it really early. Also, I was on\n",
      "  the bad guys\\' side all the time in the movie, because the good guys were so\n",
      "  stupid. \"Executive Decision\" should without a doubt be you\\'re choice over\n",
      "  this one, even the \"Turbulence\"-movies are better. In fact, every other\n",
      "  movie in the world is better than this one.'\n",
      "Y ou can see that text_train is a list of length 25,000, where each entry is a string\n",
      "containing a review. We printed the review with index 1. Y ou can also see that the\n",
      "review contains some HTML line breaks ( <br />). While these are unlikely to have a\n",
      "326 | Chapter 7: Working with Text Data\n",
      "large impact on our machine learning models, it is better to clean the data and\n",
      "remove this formatting before we proceed:\n",
      "In[4]:\n",
      "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
      "The type of the entries of text_train will depend on your Python version. In Python\n",
      "3, they will be of type bytes which represents a binary encoding of the string data. In\n",
      "Python 2, text_train contains strings. We won’t go into the details of the different\n",
      "string types in Python here, but we recommend that you read the Python 2  and/or\n",
      "Python 3 documentation regarding strings and Unicode.\n",
      "The dataset was collected such that the positive class and the negative class balanced,\n",
      "so that there are as many positive as negative strings:\n",
      "In[5]:\n",
      "print(\"Samples per class (training): {}\".format(np.bincount(y_train)))\n",
      "Out[5]:\n",
      "Samples per class (training): [12500 12500]\n",
      "We load the test dataset in the same manner:\n",
      "In[6]:\n",
      "reviews_test = load_files(\"data/aclImdb/test/\")\n",
      "text_test, y_test = reviews_test.data, reviews_test.target\n",
      "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
      "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
      "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\n",
      "Out[6]:\n",
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n",
      "The task we want to solve is as follows: given a review, we want to assign the label\n",
      "“positive” or “negative” based on the text content of the review. This is a standard\n",
      "binary classification task. However, the text data is not in a format that a machine\n",
      "learning model can handle. We need to convert the string representation of the text\n",
      "into a numeric representation that we can apply our machine learning algorithms to.\n",
      "Representing Text Data as a Bag of Words\n",
      "One of the most simple but effective and commonly used ways to represent text for\n",
      "machine learning is using the bag-of-words representation. When using this represen‐\n",
      "tation, we discard most of the structure of the input text, like chapters, paragraphs,\n",
      "sentences, and formatting, and only count how often each word appears in each text in\n",
      "Representing Text Data as a Bag of Words | 327\n",
      "the corpus. Discarding the structure and counting only word occurrences leads to the\n",
      "mental image of representing text as a “bag. ”\n",
      "Computing the bag-of-words representation for a corpus of documents consists of\n",
      "the following three steps:\n",
      "1. Tokenization. Split each document into the words that appear in it (called tokens),\n",
      "for example by splitting them on whitespace and punctuation.\n",
      "2. Vocabulary building. Collect a vocabulary of all words that appear in any of the\n",
      "documents, and number them (say, in alphabetical order).\n",
      "3. Encoding. For each document, count how often each of the words in the vocabu‐\n",
      "lary appear in this document.\n",
      "There are some subtleties involved in step 1 and step 2, which we will discuss in more\n",
      "detail later in this chapter. For now, let’s look at how we can apply the bag-of-words\n",
      "processing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\n",
      "is how you get ants.\" . The output is one vector of word counts for each docu‐\n",
      "ment. For each word in the vocabulary, we have a count of how often it appears in\n",
      "each document. That means our numeric representation has one feature for each\n",
      "unique word in the whole dataset. Note how the order of the words in the original\n",
      "string is completely irrelevant to the bag-of-words feature representation.\n",
      "Figure 7-1. Bag-of-words processing\n",
      "328 | Chapter 7: Working with Text Data\n",
      "Applying Bag-of-Words to a Toy Dataset\n",
      "The bag-of-words representation is implemented in CountVectorizer, which is a\n",
      "transformer. Let’s first apply it to a toy dataset, consisting of two samples, to see it\n",
      "working:\n",
      "In[7]:\n",
      "bards_words =[\"The fool doth think he is wise,\",\n",
      "              \"but the wise man knows himself to be a fool\"]\n",
      "We import and instantiate the CountVectorizer and fit it to our toy data as follows:\n",
      "In[8]:\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vect = CountVectorizer()\n",
      "vect.fit(bards_words)\n",
      "Fitting the CountVectorizer consists of the tokenization of the training data and\n",
      "building of the vocabulary, which we can access as the vocabulary_ attribute:\n",
      "In[9]:\n",
      "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
      "print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\n",
      "Out[9]:\n",
      "Vocabulary size: 13\n",
      "Vocabulary content:\n",
      " {'the': 9, 'himself': 5, 'wise': 12, 'he': 4, 'doth': 2, 'to': 11, 'knows': 7,\n",
      "  'man': 8, 'fool': 3, 'is': 6, 'be': 0, 'think': 10, 'but': 1}\n",
      "The vocabulary consists of 13 words, from \"be\" to \"wise\".\n",
      "To create the bag-of-words representation for the training data, we call the transform\n",
      "method:\n",
      "In[10]:\n",
      "bag_of_words = vect.transform(bards_words)\n",
      "print(\"bag_of_words: {}\".format(repr(bag_of_words)))\n",
      "Out[10]:\n",
      "bag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "    with 16 stored elements in Compressed Sparse Row format>\n",
      "The bag-of-words representation is stored in a SciPy sparse matrix that only stores\n",
      "the entries that are nonzero (see Chapter 1). The matrix is of shape 2×13, with one\n",
      "row for each of the two data points and one feature for each of the words in the\n",
      "vocabulary. A sparse matrix is used as most documents only contain a small subset of\n",
      "the words in the vocabulary, meaning most entries in the feature array are 0. Think\n",
      "Representing Text Data as a Bag of Words | 329\n",
      "4 This is possible because we are using a small toy dataset that contains only 13 words. For any real dataset, this\n",
      "would result in a MemoryError.\n",
      "about how many different words might appear in a movie review compared to all the\n",
      "words in the English language (which is what the vocabulary models). Storing all\n",
      "those zeros would be prohibitive, and a waste of memory. To look at the actual con‐\n",
      "tent of the sparse matrix, we can convert it to a “dense” NumPy array (that also stores\n",
      "all the 0 entries) using the toarray method:4\n",
      "In[11]:\n",
      "print(\"Dense representation of bag_of_words:\\n{}\".format(\n",
      "    bag_of_words.toarray()))\n",
      "Out[11]:\n",
      "Dense representation of bag_of_words:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n",
      "We can see that the word counts for each word are either 0 or 1; neither of the two\n",
      "strings in bards_words contains a word twice. Let’s take a look at how to read these\n",
      "feature vectors. The first string ( \"The fool doth think he is wise,\" ) is repre‐\n",
      "sented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\n",
      "times. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\n",
      "tains the third word, \"doth\", once, and so on. Looking at both rows, we can see that\n",
      "the fourth word, \"fool\", the tenth word, \"the\", and the thirteenth word, \"wise\",\n",
      "appear in both strings.\n",
      "Bag-of-Words for Movie Reviews\n",
      "Now that we’ve gone through the bag-of-words process in detail, let’s apply it to our\n",
      "task of sentiment analysis for movie reviews. Earlier, we loaded our training and test\n",
      "data from the IMDb reviews into lists of strings ( text_train and text_test), which\n",
      "we will now process:\n",
      "In[12]:\n",
      "vect = CountVectorizer().fit(text_train)\n",
      "X_train = vect.transform(text_train)\n",
      "print(\"X_train:\\n{}\".format(repr(X_train)))\n",
      "Out[12]:\n",
      "X_train:\n",
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "    with 3431196 stored elements in Compressed Sparse Row format>\n",
      "330 | Chapter 7: Working with Text Data\n",
      "5 A quick analysis of the data confirms that this is indeed the case. Try confirming it yourself.\n",
      "The shape of X_train, the bag-of-words representation of the training data, is\n",
      "25,000×74,849, indicating that the vocabulary contains 74,849 entries. Again, the data\n",
      "is stored as a SciPy sparse matrix. Let’s look at the vocabulary in a bit more detail.\n",
      "Another way to access the vocabulary is using the get_feature_name method of the\n",
      "vectorizer, which returns a convenient list where each entry corresponds to one fea‐\n",
      "ture:\n",
      "In[13]:\n",
      "feature_names = vect.get_feature_names()\n",
      "print(\"Number of features: {}\".format(len(feature_names)))\n",
      "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
      "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
      "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\n",
      "Out[13]:\n",
      "Number of features: 74849\n",
      "First 20 features:\n",
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830',\n",
      " '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s',\n",
      " '01', '01pm', '02']\n",
      "Features 20010 to 20030:\n",
      "['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback',\n",
      " 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl',\n",
      " 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']\n",
      "Every 2000th feature:\n",
      "['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bête', 'chicanery',\n",
      " 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer',\n",
      " 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful',\n",
      " 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher',\n",
      " 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse',\n",
      " 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']\n",
      "As you can see, possibly a bit surprisingly, the first 10 entries in the vocabulary are all\n",
      "numbers. All these numbers appear somewhere in the reviews, and are therefore\n",
      "extracted as words. Most of these numbers don’t have any immediate semantic mean‐\n",
      "ing—apart from \"007\", which in the particular context of movies is likely to refer to\n",
      "the James Bond character. 5 Weeding out the meaningful from the nonmeaningful\n",
      "“words” is sometimes tricky. Looking further along in the vocabulary, we find a col‐\n",
      "lection of English words starting with “dra” . Y ou might notice that for \"draught\",\n",
      "\"drawback\", and \"drawer\" both the singular and plural forms are contained in the\n",
      "vocabulary as distinct words. These words have very closely related semantic mean‐\n",
      "ings, and counting them as different words, corresponding to different features,\n",
      "might not be ideal.\n",
      "Representing Text Data as a Bag of Words | 331\n",
      "6 The attentive reader might notice that we violate our lesson from Chapter 6 on cross-validation with prepro‐\n",
      "cessing here. Using the default settings of CountVectorizer, it actually does not collect any statistics, so our\n",
      "results are valid. Using Pipeline from the start would be a better choice for applications, but we defer it for\n",
      "ease of exposure.\n",
      "Before we try to improve our feature extraction, let’s obtain a quantitative measure of\n",
      "performance by actually building a classifier. We have the training labels stored in\n",
      "y_train and the bag-of-words representation of the training data in X_train, so we\n",
      "can train a classifier on this data. For high-dimensional, sparse data like this, linear\n",
      "models like LogisticRegression often work best.\n",
      "Let’s start by evaluating LogisticRegresssion using cross-validation:6\n",
      "In[14]:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)\n",
      "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))\n",
      "Out[14]:\n",
      "Mean cross-validation accuracy: 0.88\n",
      "We obtain a mean cross-validation score of 88%, which indicates reasonable perfor‐\n",
      "mance for a balanced binary classification task. We know that LogisticRegression\n",
      "has a regularization parameter, C, which we can tune via cross-validation:\n",
      "In[15]:\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
      "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
      "grid.fit(X_train, y_train)\n",
      "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
      "print(\"Best parameters: \", grid.best_params_)\n",
      "Out[15]:\n",
      "Best cross-validation score: 0.89\n",
      "Best parameters:  {'C': 0.1}\n",
      "We obtain a cross-validation score of 89% using C=0.1. We can now assess the gener‐\n",
      "alization performance of this parameter setting on the test set:\n",
      "In[16]:\n",
      "X_test = vect.transform(text_test)\n",
      "print(\"{:.2f}\".format(grid.score(X_test, y_test)))\n",
      "Out[16]:\n",
      "0.88\n",
      "332 | Chapter 7: Working with Text Data\n",
      "Now, let’s see if we can improve the extraction of words. The CountVectorizer\n",
      "extracts tokens using a regular expression. By default, the regular expression that is\n",
      "used is \"\\b\\w\\w+\\b\". If you are not familiar with regular expressions, this means it\n",
      "finds all sequences of characters that consist of at least two letters or numbers ( \\w)\n",
      "and that are separated by word boundaries ( \\b). It does not find single-letter words,\n",
      "and it splits up contractions like “doesn’t” or “bit.ly” , but it matches “h8ter” as a single\n",
      "word. The CountVectorizer then converts all words to lowercase characters, so that\n",
      "“soon” , “Soon” , and “sOon” all correspond to the same token (and therefore feature).\n",
      "This simple mechanism works quite well in practice, but as we saw earlier, we get\n",
      "many uninformative features (like the numbers). One way to cut back on these is to\n",
      "only use tokens that appear in at least two documents (or at least five documents, and\n",
      "so on). A token that appears only in a single document is unlikely to appear in the test\n",
      "set and is therefore not helpful. We can set the minimum number of documents a\n",
      "token needs to appear in with the min_df parameter:\n",
      "In[17]:\n",
      "vect = CountVectorizer(min_df=5).fit(text_train)\n",
      "X_train = vect.transform(text_train)\n",
      "print(\"X_train with min_df: {}\".format(repr(X_train)))\n",
      "Out[17]:\n",
      "X_train with min_df: <25000x27271 sparse matrix of type '<class 'numpy.int64'>'\n",
      "    with 3354014 stored elements in Compressed Sparse Row format>\n",
      "By requiring at least five appearances of each token, we can bring down the number\n",
      "of features to 27,271, as seen in the preceding output—only about a third of the origi‐\n",
      "nal features. Let’s look at some tokens again:\n",
      "In[18]:\n",
      "feature_names = vect.get_feature_names()\n",
      "print(\"First 50 features:\\n{}\".format(feature_names[:50]))\n",
      "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
      "print(\"Every 700th feature:\\n{}\".format(feature_names[::700]))\n",
      "Out[18]:\n",
      "First 50 features:\n",
      "['00', '000', '007', '00s', '01', '02', '03', '04', '05', '06', '07', '08',\n",
      " '09', '10', '100', '1000', '100th', '101', '102', '103', '104', '105', '107',\n",
      " '108', '10s', '10th', '11', '110', '112', '116', '117', '11th', '12', '120',\n",
      " '12th', '13', '135', '13th', '14', '140', '14th', '15', '150', '15th', '16',\n",
      " '160', '1600', '16mm', '16s', '16th']\n",
      "Features 20010 to 20030:\n",
      "['repentance', 'repercussions', 'repertoire', 'repetition', 'repetitions',\n",
      " 'repetitious', 'repetitive', 'rephrase', 'replace', 'replaced', 'replacement',\n",
      " 'replaces', 'replacing', 'replay', 'replayable', 'replayed', 'replaying',\n",
      " 'replays', 'replete', 'replica']\n",
      "Representing Text Data as a Bag of Words | 333\n",
      "Every 700th feature:\n",
      "['00', 'affections', 'appropriately', 'barbra', 'blurbs', 'butchered',\n",
      " 'cheese', 'commitment', 'courts', 'deconstructed', 'disgraceful', 'dvds',\n",
      " 'eschews', 'fell', 'freezer', 'goriest', 'hauser', 'hungary', 'insinuate',\n",
      " 'juggle', 'leering', 'maelstrom', 'messiah', 'music', 'occasional', 'parking',\n",
      " 'pleasantville', 'pronunciation', 'recipient', 'reviews', 'sas', 'shea',\n",
      " 'sneers', 'steiger', 'swastika', 'thrusting', 'tvs', 'vampyre', 'westerns']\n",
      "There are clearly many fewer numbers, and some of the more obscure words or mis‐\n",
      "spellings seem to have vanished. Let’s see how well our model performs by doing a\n",
      "grid search again:\n",
      "In[19]:\n",
      "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
      "grid.fit(X_train, y_train)\n",
      "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
      "Out[19]:\n",
      "Best cross-validation score: 0.89\n",
      "The best validation accuracy of the grid search is still 89%, unchanged from before.\n",
      "We didn’t improve our model, but having fewer features to deal with speeds up pro‐\n",
      "cessing and throwing away useless features might make the model more interpretable.\n",
      "If the transform method of CountVectorizer is called on a docu‐\n",
      "ment that contains words that were not contained in the training\n",
      "data, these words will be ignored as they are not part of the dictio‐\n",
      "nary. This is not really an issue for classification, as it’s not possible\n",
      "to learn anything about words that are not in the training data. For\n",
      "some applications, like spam detection, it might be helpful to add a\n",
      "feature that encodes how many so-called “out of vocabulary” words\n",
      "there are in a particular document, though. For this to work, you\n",
      "need to set min_df; otherwise, this feature will never be active dur‐\n",
      "ing training.\n",
      "Stopwords\n",
      "Another way that we can get rid of uninformative words is by discarding words that\n",
      "are too frequent to be informative. There are two main approaches: using a language-\n",
      "specific list of stopwords, or discarding words that appear too frequently. scikit-\n",
      "learn has a built-in list of English stopwords in the feature_extraction.text\n",
      "module:\n",
      "334 | Chapter 7: Working with Text Data\n",
      "In[20]:\n",
      "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
      "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
      "print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))\n",
      "Out[20]:\n",
      "Number of stop words: 318\n",
      "Every 10th stopword:\n",
      "['above', 'elsewhere', 'into', 'well', 'rather', 'fifteen', 'had', 'enough',\n",
      " 'herein', 'should', 'third', 'although', 'more', 'this', 'none', 'seemed',\n",
      " 'nobody', 'seems', 'he', 'also', 'fill', 'anyone', 'anything', 'me', 'the',\n",
      " 'yet', 'go', 'seeming', 'front', 'beforehand', 'forty', 'i']\n",
      "Clearly, removing the stopwords in the list can only decrease the number of features\n",
      "by the length of the list—here, 318—but it might lead to an improvement in perfor‐\n",
      "mance. Let’s give it a try:\n",
      "In[21]:\n",
      "# Specifying stop_words=\"english\" uses the built-in list.\n",
      "# We could also augment it and pass our own.\n",
      "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
      "X_train = vect.transform(text_train)\n",
      "print(\"X_train with stop words:\\n{}\".format(repr(X_train)))\n",
      "Out[21]:\n",
      "X_train with stop words:\n",
      "<25000x26966 sparse matrix of type '<class 'numpy.int64'>'\n",
      "    with 2149958 stored elements in Compressed Sparse Row format>\n",
      "There are now 305 (27,271–26,966) fewer features in the dataset, which means that\n",
      "most, but not all, of the stopwords appeared. Let’s run the grid search again:\n",
      "In[22]:\n",
      "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
      "grid.fit(X_train, y_train)\n",
      "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
      "Out[22]:\n",
      "Best cross-validation score: 0.88\n",
      "The grid search performance decreased slightly using the stopwords—not enough to\n",
      "worry about, but given that excluding 305 features out of over 27,000 is unlikely to\n",
      "change performance or interpretability a lot, it doesn’t seem worth using this list.\n",
      "Fixed lists are mostly helpful for small datasets, which might not contain enough\n",
      "information for the model to determine which words are stopwords from the data\n",
      "itself. As an exercise, you can try out the other approach, discarding frequently\n",
      "Stopwords | 335\n",
      "7 We provide this formula here mostly for completeness; you don’t need to remember it to use the tf–idf\n",
      "encoding.\n",
      "appearing words, by setting the max_df option of CountVectorizer and see how it\n",
      "influences the number of features and the performance.\n",
      "Rescaling the Data with tf–idf\n",
      "Instead of dropping features that are deemed unimportant, another approach is to\n",
      "rescale features by how informative we expect them to be. One of the most common\n",
      "ways to do this is using the term frequency–inverse document frequency  (tf–idf)\n",
      "method. The intuition of this method is to give high weight to any term that appears\n",
      "often in a particular document, but not in many documents in the corpus. If a word\n",
      "appears often in a particular document, but not in very many documents, it is likely\n",
      "to be very descriptive of the content of that document. scikit-learn implements the\n",
      "tf–idf method in two classes: TfidfTransformer, which takes in the sparse matrix\n",
      "output produced by CountVectorizer and transforms it, and TfidfVectorizer,\n",
      "which takes in the text data and does both the bag-of-words feature extraction and\n",
      "the tf–idf transformation. There are several variants of the tf–idf rescaling scheme,\n",
      "which you can read about on Wikipedia . The tf–idf score for word w in document d\n",
      "as implemented in both the TfidfTransformer and TfidfVectorizer classes is given\n",
      "by:7\n",
      "tfidf w, d = tf log N + 1\n",
      "Nw + 1 + 1\n",
      "where N is the number of documents in the training set, Nw is the number of docu‐\n",
      "ments in the training set that the word w appears in, and tf (the term frequency) is the\n",
      "number of times that the word w appears in the query document d (the document\n",
      "you want to transform or encode). Both classes also apply L2 normalization after\n",
      "computing the tf–idf representation; in other words, they rescale the representation\n",
      "of each document to have Euclidean norm 1. Rescaling in this way means that the\n",
      "length of a document (the number of words) does not change the vectorized repre‐\n",
      "sentation.\n",
      "Because tf–idf actually makes use of the statistical properties of the training data, we\n",
      "will use a pipeline, as described in Chapter 6, to ensure the results of our grid search\n",
      "are valid. This leads to the following code:\n",
      "336 | Chapter 7: Working with Text Data\n",
      "In[23]:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.pipeline import make_pipeline\n",
      "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\n",
      "                     LogisticRegression())\n",
      "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
      "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
      "grid.fit(text_train, y_train)\n",
      "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
      "Out[23]:\n",
      "Best cross-validation score: 0.89\n",
      "As you can see, there is some improvement when using tf–idf instead of just word\n",
      "counts. We can also inspect which words tf–idf found most important. Keep in mind\n",
      "that the tf–idf scaling is meant to find words that distinguish documents, but it is a\n",
      "purely unsupervised technique. So, “important” here does not necessarily relate to the\n",
      "“positive review” and “negative review” labels we are interested in. First, we extract\n",
      "the TfidfVectorizer from the pipeline:\n",
      "In[24]:\n",
      "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
      "# transform the training dataset\n",
      "X_train = vectorizer.transform(text_train)\n",
      "# find maximum value for each of the features over the dataset\n",
      "max_value = X_train.max(axis=0).toarray().ravel()\n",
      "sorted_by_tfidf = max_value.argsort()\n",
      "# get feature names\n",
      "feature_names = np.array(vectorizer.get_feature_names())\n",
      "print(\"Features with lowest tfidf:\\n{}\".format(\n",
      "    feature_names[sorted_by_tfidf[:20]]))\n",
      "print(\"Features with highest tfidf: \\n{}\".format(\n",
      "    feature_names[sorted_by_tfidf[-20:]]))\n",
      "Out[24]:\n",
      "Features with lowest tfidf:\n",
      "['poignant' 'disagree' 'instantly' 'importantly' 'lacked' 'occurred'\n",
      " 'currently' 'altogether' 'nearby' 'undoubtedly' 'directs' 'fond' 'stinker'\n",
      " 'avoided' 'emphasis' 'commented' 'disappoint' 'realizing' 'downhill'\n",
      " 'inane']\n",
      "Features with highest tfidf:\n",
      "['coop' 'homer' 'dillinger' 'hackenstein' 'gadget' 'taker' 'macarthur'\n",
      " 'vargas' 'jesse' 'basket' 'dominick' 'the' 'victor' 'bridget' 'victoria'\n",
      " 'khouri' 'zizek' 'rob' 'timon' 'titanic']\n",
      "Rescaling the Data with tf–idf | 337\n",
      "Features with low tf–idf are those that either are very commonly used across docu‐\n",
      "ments or are only used sparingly, and only in very long documents. Interestingly,\n",
      "many of the high-tf–idf features actually identify certain shows or movies. These\n",
      "terms only appear in reviews for this particular show or franchise, but tend to appear\n",
      "very often in these particular reviews. This is very clear, for example, for \"pokemon\",\n",
      "\"smallville\", and \"doodlebops\", but \"scanners\" here actually also refers to a\n",
      "movie title. These words are unlikely to help us in our sentiment classification task\n",
      "(unless maybe some franchises are universally reviewed positively or negatively) but\n",
      "certainly contain a lot of specific information about the reviews.\n",
      "We can also find the words that have low inverse document frequency—that is, those\n",
      "that appear frequently and are therefore deemed less important. The inverse docu‐\n",
      "ment frequency values found on the training set are stored in the idf_ attribute:\n",
      "In[25]:\n",
      "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
      "print(\"Features with lowest idf:\\n{}\".format(\n",
      "    feature_names[sorted_by_idf[:100]]))\n",
      "Out[25]:\n",
      "Features with lowest idf:\n",
      "['the' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that' 'but' 'for' 'with'\n",
      " 'was' 'as' 'on' 'movie' 'not' 'have' 'one' 'be' 'film' 'are' 'you' 'all'\n",
      " 'at' 'an' 'by' 'so' 'from' 'like' 'who' 'they' 'there' 'if' 'his' 'out'\n",
      " 'just' 'about' 'he' 'or' 'has' 'what' 'some' 'good' 'can' 'more' 'when'\n",
      " 'time' 'up' 'very' 'even' 'only' 'no' 'would' 'my' 'see' 'really' 'story'\n",
      " 'which' 'well' 'had' 'me' 'than' 'much' 'their' 'get' 'were' 'other'\n",
      " 'been' 'do' 'most' 'don' 'her' 'also' 'into' 'first' 'made' 'how' 'great'\n",
      " 'because' 'will' 'people' 'make' 'way' 'could' 'we' 'bad' 'after' 'any'\n",
      " 'too' 'then' 'them' 'she' 'watch' 'think' 'acting' 'movies' 'seen' 'its'\n",
      " 'him']\n",
      "As expected, these are mostly English stopwords like \"the\" and \"no\". But some are\n",
      "clearly domain-specific to the movie reviews, like \"movie\", \"film\", \"time\", \"story\",\n",
      "and so on. Interestingly, \"good\", \"great\", and \"bad\" are also among the most fre‐\n",
      "quent and therefore “least relevant” words according to the tf–idf measure, even\n",
      "though we might expect these to be very important for our sentiment analysis task.\n",
      "Investigating Model Coefficients\n",
      "Finally, let’s look in a bit more detail into what our logistic regression model actually\n",
      "learned from the data. Because there are so many features—27,271 after removing the\n",
      "infrequent ones—we clearly cannot look at all of the coefficients at the same time.\n",
      "However, we can look at the largest coefficients, and see which words these corre‐\n",
      "spond to. We will use the last model that we trained, based on the tf–idf features.\n",
      "The following bar chart ( Figure 7-2) shows the 25 largest and 25 smallest coefficients\n",
      "of the logistic regression model, with the bars showing the size of each coefficient:\n",
      "338 | Chapter 7: Working with Text Data\n",
      "In[26]:\n",
      "mglearn.tools.visualize_coefficients(\n",
      "    grid.best_estimator_.named_steps[\"logisticregression\"].coef_,\n",
      "    feature_names, n_top_features=40)\n",
      "Figure 7-2. Largest and smallest coefficients of logistic regression trained on tf-idf fea‐\n",
      "tures\n",
      "The negative coefficients on the left belong to words that according to the model are\n",
      "indicative of negative reviews, while the positive coefficients on the right belong to\n",
      "words that according to the model indicate positive reviews. Most of the terms are\n",
      "quite intuitive, like \"worst\", \"waste\", \"disappointment\", and \"laughable\" indicat‐\n",
      "ing bad movie reviews, while \"excellent\", \"wonderful\", \"enjoyable\", and\n",
      "\"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\n",
      "\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\n",
      "today. ”\n",
      "Bag-of-Words with More Than One Word (n-Grams)\n",
      "One of the main disadvantages of using a bag-of-words representation is that word\n",
      "order is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n",
      "“it’s good, not bad at all” have exactly the same representation, even though the mean‐\n",
      "ings are inverted. Putting “not” in front of a word is only one example (if an extreme\n",
      "one) of how context matters. Fortunately, there is a way of capturing context when\n",
      "using a bag-of-words representation, by not only considering the counts of single\n",
      "tokens, but also the counts of pairs or triplets of tokens that appear next to each other.\n",
      "Pairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\n",
      "more generally sequences of tokens are known as n-grams. We can change the range\n",
      "of tokens that are considered as features by changing the ngram_range parameter of\n",
      "CountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\n",
      "Bag-of-Words with More Than One Word (n-Grams) | 339\n",
      "sisting of the minimum length and the maximum length of the sequences of tokens\n",
      "that are considered. Here is an example on the toy data we used earlier:\n",
      "In[27]:\n",
      "print(\"bards_words:\\n{}\".format(bards_words))\n",
      "Out[27]:\n",
      "bards_words:\n",
      "['The fool doth think he is wise,',\n",
      " 'but the wise man knows himself to be a fool']\n",
      "The default is to create one feature per sequence of tokens that is at least one token\n",
      "long and at most one token long, or in other words exactly one token long (single\n",
      "tokens are also called unigrams):\n",
      "In[28]:\n",
      "cv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\n",
      "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
      "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\n",
      "Out[28]:\n",
      "Vocabulary size: 13\n",
      "Vocabulary:\n",
      "['be', 'but', 'doth', 'fool', 'he', 'himself', 'is', 'knows', 'man', 'the',\n",
      " 'think', 'to', 'wise']\n",
      "To look only at bigrams—that is, only at sequences of two tokens following each\n",
      "other—we can set ngram_range to (2, 2):\n",
      "In[29]:\n",
      "cv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
      "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
      "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\n",
      "Out[29]:\n",
      "Vocabulary size: 14\n",
      "Vocabulary:\n",
      "['be fool', 'but the', 'doth think', 'fool doth', 'he is', 'himself to',\n",
      " 'is wise', 'knows himself', 'man knows', 'the fool', 'the wise',\n",
      " 'think he', 'to be', 'wise man']\n",
      "Using longer sequences of tokens usually results in many more features, and in more\n",
      "specific features. There is no common bigram between the two phrases in\n",
      "bard_words:\n",
      "340 | Chapter 7: Working with Text Data\n",
      "In[30]:\n",
      "print(\"Transformed data (dense):\\n{}\".format(cv.transform(bards_words).toarray()))\n",
      "Out[30]:\n",
      "Transformed data (dense):\n",
      "[[0 0 1 1 1 0 1 0 0 1 0 1 0 0]\n",
      " [1 1 0 0 0 1 0 1 1 0 1 0 1 1]]\n",
      "For most applications, the minimum number of tokens should be one, as single\n",
      "words often capture a lot of meaning. Adding bigrams helps in most cases. Adding\n",
      "longer sequences—up to 5-grams—might help too, but this will lead to an explosion\n",
      "of the number of features and might lead to overfitting, as there will be many very\n",
      "specific features. In principle, the number of bigrams could be the number of\n",
      "unigrams squared and the number of trigrams could be the number of unigrams to\n",
      "the power of three, leading to very large feature spaces. In practice, the number of\n",
      "higher n-grams that actually appear in the data is much smaller, because of the struc‐\n",
      "ture of the (English) language, though it is still large.\n",
      "Here is what using unigrams, bigrams, and trigrams on bards_words looks like:\n",
      "In[31]:\n",
      "cv = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)\n",
      "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
      "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\n",
      "Out[31]:\n",
      "Vocabulary size: 39\n",
      "Vocabulary:\n",
      "['be', 'be fool', 'but', 'but the', 'but the wise', 'doth', 'doth think',\n",
      " 'doth think he', 'fool', 'fool doth', 'fool doth think', 'he', 'he is',\n",
      " 'he is wise', 'himself', 'himself to', 'himself to be', 'is', 'is wise',\n",
      " 'knows', 'knows himself', 'knows himself to', 'man', 'man knows',\n",
      " 'man knows himself', 'the', 'the fool', 'the fool doth', 'the wise',\n",
      " 'the wise man', 'think', 'think he', 'think he is', 'to', 'to be',\n",
      " 'to be fool', 'wise', 'wise man', 'wise man knows']\n",
      "Let’s try out the TfidfVectorizer on the IMDb movie review data and find the best\n",
      "setting of n-gram range using a grid search:\n",
      "In[32]:\n",
      "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
      "# running the grid search takes a long time because of the\n",
      "# relatively large grid and the inclusion of trigrams\n",
      "param_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
      "              \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\n",
      "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
      "grid.fit(text_train, y_train)\n",
      "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
      "print(\"Best parameters:\\n{}\".format(grid.best_params_))\n",
      "Bag-of-Words with More Than One Word (n-Grams) | 341\n",
      "Out[32]:\n",
      "Best cross-validation score: 0.91\n",
      "Best parameters:\n",
      "{'tfidfvectorizer__ngram_range': (1, 3), 'logisticregression__C': 100}\n",
      "As you can see from the results, we improved performance by a bit more than a per‐\n",
      "cent by adding bigram and trigram features. We can visualize the cross-validation\n",
      "accuracy as a function of the ngram_range and C parameter as a heat map, as we did\n",
      "in Chapter 5 (see Figure 7-3):\n",
      "In[33]:\n",
      "# extract scores from grid_search\n",
      "scores = grid.cv_results_['mean_test_score'].reshape(-1, 3).T\n",
      "# visualize heat map\n",
      "heatmap = mglearn.tools.heatmap(\n",
      "    scores, xlabel=\"C\", ylabel=\"ngram_range\", cmap=\"viridis\", fmt=\"%.3f\",\n",
      "    xticklabels=param_grid['logisticregression__C'],\n",
      "    yticklabels=param_grid['tfidfvectorizer__ngram_range'])\n",
      "plt.colorbar(heatmap)\n",
      "Figure 7-3. Heat map visualization of mean cross-validation accuracy as a function of\n",
      "the parameters ngram_range and C\n",
      "From the heat map we can see that using bigrams increases performance quite a bit,\n",
      "while adding trigrams only provides a very small benefit in terms of accuracy. To\n",
      "understand better how the model improved, we can visualize the important coeffi‐\n",
      "342 | Chapter 7: Working with Text Data\n",
      "cient for the best model, which includes unigrams, bigrams, and trigrams (see\n",
      "Figure 7-4):\n",
      "In[34]:\n",
      "# extract feature names and coefficients\n",
      "vect = grid.best_estimator_.named_steps['tfidfvectorizer']\n",
      "feature_names = np.array(vect.get_feature_names())\n",
      "coef = grid.best_estimator_.named_steps['logisticregression'].coef_\n",
      "mglearn.tools.visualize_coefficients(coef, feature_names, n_top_features=40)\n",
      "Figure 7-4. Most important features when using unigrams, bigrams, and trigrams with\n",
      "tf-idf rescaling\n",
      "There are particularly interesting features containing the word “worth” that were not\n",
      "present in the unigram model: \"not worth\" is indicative of a negative review, while\n",
      "\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\n",
      "prime example of context influencing the meaning of the word “worth. ”\n",
      "Next, we’ll visualize only trigrams, to provide further insight into why these features\n",
      "are helpful. Many of the useful bigrams and trigrams consist of common words that\n",
      "would not be informative on their own, as in the phrases \"none of the\", \"the only\n",
      "good\", \"on and on\" , \"this is one\" , \"of the most\" , and so on. However, the\n",
      "impact of these features is quite limited compared to the importance of the unigram\n",
      "features, as you can see in Figure 7-5:\n",
      "In[35]:\n",
      "# find 3-gram features\n",
      "mask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\n",
      "# visualize only 3-gram features\n",
      "mglearn.tools.visualize_coefficients(coef.ravel()[mask],\n",
      "                                     feature_names[mask], n_top_features=40)\n",
      "Bag-of-Words with More Than One Word (n-Grams) | 343\n",
      "Figure 7-5. Visualization of only the important trigram features of the model\n",
      "Advanced Tokenization, Stemming, and Lemmatization\n",
      "As mentioned previously, the feature extraction in the CountVectorizer and Tfidf\n",
      "Vectorizer is relatively simple, and much more elaborate methods are possible. One\n",
      "particular step that is often improved in more sophisticated text-processing applica‐\n",
      "tions is the first step in the bag-of-words model: tokenization. This step defines what\n",
      "constitutes a word for the purpose of feature extraction.\n",
      "We saw earlier that the vocabulary often contains singular and plural versions of\n",
      "some words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n",
      "\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\n",
      "of \"drawback\" and \"drawbacks\" are so close that distinguishing them will only\n",
      "increase overfitting, and not allow the model to fully exploit the training data. Simi‐\n",
      "larly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replace\n",
      "ment\", \"replaces\", and \"replacing\", which are different verb forms and a noun\n",
      "relating to the verb “to replace. ” Similarly to having singular and plural forms of a\n",
      "noun, treating different verb forms and related words as distinct tokens is disadvanta‐\n",
      "geous for building a model that generalizes well.\n",
      "This problem can be overcome by representing each word using its word stem, which\n",
      "involves identifying (or conflating) all the words that have the same word stem. If this\n",
      "is done by using a rule-based heuristic, like dropping common suffixes, it is usually\n",
      "referred to as stemming. If instead a dictionary of known word forms is used (an\n",
      "explicit and human-verified system), and the role of the word in the sentence is taken\n",
      "into account, the process is referred to as lemmatization and the standardized form of\n",
      "the word is referred to as the lemma. Both processing methods, lemmatization and\n",
      "stemming, are forms of normalization that try to extract some normal form of a\n",
      "word. Another interesting case of normalization is spelling correction, which can be\n",
      "helpful in practice but is outside of the scope of this book.\n",
      "344 | Chapter 7: Working with Text Data\n",
      "8 For details of the interface, consult the nltk and spacy documentation. We are more interested in the general\n",
      "principles here.\n",
      "To get a better understanding of normalization, let’s compare a method for stemming\n",
      "—the Porter stemmer, a widely used collection of heuristics (here imported from the\n",
      "nltk package)—to lemmatization as implemented in the spacy package:8\n",
      "In[36]:\n",
      "import spacy\n",
      "import nltk\n",
      "# load spacy's English-language models\n",
      "en_nlp = spacy.load('en')\n",
      "# instantiate nltk's Porter stemmer\n",
      "stemmer = nltk.stem.PorterStemmer()\n",
      "# define function to compare lemmatization in spacy with stemming in nltk\n",
      "def compare_normalization(doc):\n",
      "    # tokenize document in spacy\n",
      "    doc_spacy = en_nlp(doc)\n",
      "    # print lemmas found by spacy\n",
      "    print(\"Lemmatization:\")\n",
      "    print([token.lemma_ for token in doc_spacy])\n",
      "    # print tokens found by Porter stemmer\n",
      "    print(\"Stemming:\")\n",
      "    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])\n",
      "We will compare lemmatization and the Porter stemmer on a sentence designed to\n",
      "show some of the differences:\n",
      "In[37]:\n",
      "compare_normalization(u\"Our meeting today was worse than yesterday, \"\n",
      "                       \"I'm scared of meeting the clients tomorrow.\")\n",
      "Out[37]:\n",
      "Lemmatization:\n",
      "['our', 'meeting', 'today', 'be', 'bad', 'than', 'yesterday', ',', 'i', 'be',\n",
      " 'scared', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\n",
      "Stemming:\n",
      "['our', 'meet', 'today', 'wa', 'wors', 'than', 'yesterday', ',', 'i', \"'m\",\n",
      " 'scare', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\n",
      "Stemming is always restricted to trimming the word to a stem, so \"was\" becomes\n",
      "\"wa\", while lemmatization can retrieve the correct base verb form, \"be\". Similarly,\n",
      "lemmatization can normalize \"worse\" to \"bad\", while stemming produces \"wors\".\n",
      "Another major difference is that stemming reduces both occurrences of \"meeting\" to\n",
      "\"meet\". Using lemmatization, the first occurrence of \"meeting\" is recognized as a\n",
      "Advanced Tokenization, Stemming, and Lemmatization | 345\n",
      "noun and left as is, while the second occurrence is recognized as a verb and reduced\n",
      "to \"meet\". In general, lemmatization is a much more involved process than stem‐\n",
      "ming, but it usually produces better results than stemming when used for normaliz‐\n",
      "ing tokens for machine learning.\n",
      "While scikit-learn implements neither form of normalization, CountVectorizer\n",
      "allows specifying your own tokenizer to convert each document into a list of tokens\n",
      "using the tokenizer parameter. We can use the lemmatization from spacy to create a\n",
      "callable that will take a string and produce a list of lemmas:\n",
      "In[38]:\n",
      "# Technicality: we want to use the regexp-based tokenizer\n",
      "# that is used by CountVectorizer and only use the lemmatization\n",
      "# from spacy. To this end, we replace en_nlp.tokenizer (the spacy tokenizer)\n",
      "# with the regexp-based tokenization.\n",
      "import re\n",
      "# regexp used in CountVectorizer\n",
      "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
      "# load spacy language model and save old tokenizer\n",
      "en_nlp = spacy.load('en')\n",
      "old_tokenizer = en_nlp.tokenizer\n",
      "# replace the tokenizer with the preceding regexp\n",
      "en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\n",
      "    regexp.findall(string))\n",
      "# create a custom tokenizer using the spacy document processing pipeline\n",
      "# (now using our own tokenizer)\n",
      "def custom_tokenizer(document):\n",
      "    doc_spacy = en_nlp(document, entity=False, parse=False)\n",
      "    return [token.lemma_ for token in doc_spacy]\n",
      "# define a count vectorizer with the custom tokenizer\n",
      "lemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)\n",
      "Let’s transform the data and inspect the vocabulary size:\n",
      "In[39]:\n",
      "# transform text_train using CountVectorizer with lemmatization\n",
      "X_train_lemma = lemma_vect.fit_transform(text_train)\n",
      "print(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))\n",
      "# standard CountVectorizer for reference\n",
      "vect = CountVectorizer(min_df=5).fit(text_train)\n",
      "X_train = vect.transform(text_train)\n",
      "print(\"X_train.shape: {}\".format(X_train.shape))\n",
      "346 | Chapter 7: Working with Text Data\n",
      "Out[39]:\n",
      "X_train_lemma.shape:  (25000, 21596)\n",
      "X_train.shape:  (25000, 27271)\n",
      "As you can see from the output, lemmatization reduced the number of features from\n",
      "27,271 (with the standard CountVectorizer processing) to 21,596. Lemmatization\n",
      "can be seen as a kind of regularization, as it conflates certain features. Therefore, we\n",
      "expect lemmatization to improve performance most when the dataset is small. To\n",
      "illustrate how lemmatization can help, we will use StratifiedShuffleSplit for\n",
      "cross-validation, using only 1% of the data as training data and the rest as test data:\n",
      "In[40]:\n",
      "# build a grid search using only 1% of the data as the training set\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
      "cv = StratifiedShuffleSplit(n_iter=5, test_size=0.99,\n",
      "                            train_size=0.01, random_state=0)\n",
      "grid = GridSearchCV(LogisticRegression(), param_grid, cv=cv)\n",
      "# perform grid search with standard CountVectorizer\n",
      "grid.fit(X_train, y_train)\n",
      "print(\"Best cross-validation score \"\n",
      "      \"(standard CountVectorizer): {:.3f}\".format(grid.best_score_))\n",
      "# perform grid search with lemmatization\n",
      "grid.fit(X_train_lemma, y_train)\n",
      "print(\"Best cross-validation score \"\n",
      "      \"(lemmatization): {:.3f}\".format(grid.best_score_))\n",
      "Out[40]:\n",
      "Best cross-validation score (standard CountVectorizer): 0.721\n",
      "Best cross-validation score (lemmatization): 0.731\n",
      "In this case, lemmatization provided a modest improvement in performance. As with\n",
      "many of the different feature extraction techniques, the result varies depending on\n",
      "the dataset. Lemmatization and stemming can sometimes help in building better (or\n",
      "at least more compact) models, so we suggest you give these techniques a try when\n",
      "trying to squeeze out the last bit of performance on a particular task.\n",
      "Topic Modeling and Document Clustering\n",
      "One particular technique that is often applied to text data is topic modeling, which is\n",
      "an umbrella term describing the task of assigning each document to one or multiple\n",
      "topics, usually without supervision. A good example for this is news data, which\n",
      "might be categorized into topics like “politics, ” “sports, ” “finance, ” and so on. If each\n",
      "document is assigned a single topic, this is the task of clustering the documents, as\n",
      "discussed in Chapter 3 . If each document can have more than one topic, the task\n",
      "Topic Modeling and Document Clustering | 347\n",
      "9 There is another machine learning model that is also often abbreviated LDA: Linear Discriminant Analysis, a\n",
      "linear classification model. This leads to quite some confusion. In this book, LDA refers to Latent Dirichlet\n",
      "Allocation.\n",
      "relates to the decomposition methods from Chapter 3. Each of the components we\n",
      "learn then corresponds to one topic, and the coefficients of the components in the\n",
      "representation of a document tell us how strongly related that document is to a par‐\n",
      "ticular topic. Often, when people talk about topic modeling, they refer to one particu‐\n",
      "lar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\n",
      "Latent Dirichlet Allocation\n",
      "Intuitively, the LDA model tries to find groups of words (the topics) that appear\n",
      "together frequently. LDA also requires that each document can be understood as a\n",
      "“mixture” of a subset of the topics. It is important to understand that for the machine\n",
      "learning model a “topic” might not be what we would normally call a topic in every‐\n",
      "day speech, but that it resembles more the components extracted by PCA or NMF\n",
      "(which we discussed in Chapter 3), which might or might not have a semantic mean‐\n",
      "ing. Even if there is a semantic meaning for an LDA “topic” , it might not be some‐\n",
      "thing we’ d usually call a topic. Going back to the example of news articles, we might\n",
      "have a collection of articles about sports, politics, and finance, written by two specific\n",
      "authors. In a politics article, we might expect to see words like “governor, ” “vote, ”\n",
      "“party, ” etc., while in a sports article we might expect words like “team, ” “score, ” and\n",
      "“season. ” Words in each of these groups will likely appear together, while it’s less likely\n",
      "that, for example, “team” and “governor” will appear together. However, these are not\n",
      "the only groups of words we might expect to appear together. The two reporters\n",
      "might prefer different phrases or different choices of words. Maybe one of them likes\n",
      "to use the word “demarcate” and one likes the word “polarize. ” Other “topics” would\n",
      "then be “words often used by reporter A ” and “words often used by reporter B, ”\n",
      "though these are not topics in the usual sense of the word.\n",
      "Let’s apply LDA to our movie review dataset to see how it works in practice. For\n",
      "unsupervised text document models, it is often good to remove very common words,\n",
      "as they might otherwise dominate the analysis. We’ll remove words that appear in at\n",
      "least 20 percent of the documents, and we’ll limit the bag-of-words model to the\n",
      "10,000 words that are most common after removing the top 20 percent:\n",
      "In[41]:\n",
      "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
      "X = vect.fit_transform(text_train)\n",
      "348 | Chapter 7: Working with Text Data\n",
      "10 In fact, NMF and LDA solve quite related problems, and we could also use NMF to extract topics.\n",
      "We will learn a topic model with 10 topics, which is few enough that we can look at all\n",
      "of them. Similarly to the components in NMF , topics don’t have an inherent ordering,\n",
      "and changing the number of topics will change all of the topics. 10 We’ll use the\n",
      "\"batch\" learning method, which is somewhat slower than the default ( \"online\") but\n",
      "usually provides better results, and increase \"max_iter\", which can also lead to better\n",
      "models:\n",
      "In[42]:\n",
      "from sklearn.decomposition import LatentDirichletAllocation\n",
      "lda = LatentDirichletAllocation(n_topics=10, learning_method=\"batch\",\n",
      "                                max_iter=25, random_state=0)\n",
      "# We build the model and transform the data in one step\n",
      "# Computing transform takes some time,\n",
      "# and we can save time by doing both at once\n",
      "document_topics = lda.fit_transform(X)\n",
      "Like the decomposition methods we saw in Chapter 3, LatentDirichletAllocation\n",
      "has a components_ attribute that stores how important each word is for each topic.\n",
      "The size of components_ is (n_topics, n_words):\n",
      "In[43]:\n",
      "lda.components_.shape\n",
      "Out[43]:\n",
      "(10, 10000)\n",
      "To understand better what the different topics mean, we will look at the most impor‐\n",
      "tant words for each of the topics. The print_topics function provides a nice format‐\n",
      "ting for these features:\n",
      "In[44]:\n",
      "# For each topic (a row in the components_), sort the features (ascending)\n",
      "# Invert rows with [:, ::-1] to make sorting descending\n",
      "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
      "# Get the feature names from the vectorizer\n",
      "feature_names = np.array(vect.get_feature_names())\n",
      "In[45]:\n",
      "# Print out the 10 topics:\n",
      "mglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\n",
      "                           sorting=sorting, topics_per_chunk=5, n_words=10)\n",
      "Topic Modeling and Document Clustering | 349\n",
      "Out[45]:\n",
      "topic 0       topic 1       topic 2       topic 3       topic 4\n",
      "--------      --------      --------      --------      --------\n",
      "between       war           funny         show          didn\n",
      "young         world         worst         series        saw\n",
      "family        us            comedy        episode       am\n",
      "real          our           thing         tv            thought\n",
      "performance   american      guy           episodes      years\n",
      "beautiful     documentary   re            shows         book\n",
      "work          history       stupid        season        watched\n",
      "each          new           actually      new           now\n",
      "both          own           nothing       television    dvd\n",
      "director      point         want          years         got\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9\n",
      "--------      --------      --------      --------      --------\n",
      "horror        kids          cast          performance   house\n",
      "action        action        role          role          woman\n",
      "effects       animation     john          john          gets\n",
      "budget        game          version       actor         killer\n",
      "nothing       fun           novel         oscar         girl\n",
      "original      disney        both          cast          wife\n",
      "director      children      director      plays         horror\n",
      "minutes       10            played        jack          young\n",
      "pretty        kid           performance   joe           goes\n",
      "doesn         old           mr            performances  around\n",
      "Judging from the important words, topic 1 seems to be about historical and war mov‐\n",
      "ies, topic 2 might be about bad comedies, topic 3 might be about TV series. Topic 4\n",
      "seems to capture some very common words, while topic 6 appears to be about child‐\n",
      "ren’s movies and topic 8 seems to capture award-related reviews. Using only 10 topics,\n",
      "each of the topics needs to be very broad, so that they can together cover all the dif‐\n",
      "ferent kinds of reviews in our dataset.\n",
      "Next, we will learn another model, this time with 100 topics. Using more topics\n",
      "makes the analysis much harder, but makes it more likely that topics can specialize to\n",
      "interesting subsets of the data:\n",
      "In[46]:\n",
      "lda100 = LatentDirichletAllocation(n_topics=100, learning_method=\"batch\",\n",
      "                                   max_iter=25, random_state=0)\n",
      "document_topics100 = lda100.fit_transform(X)\n",
      "Looking at all 100 topics would be a bit overwhelming, so we selected some interest‐\n",
      "ing and representative topics:\n",
      "350 | Chapter 7: Working with Text Data\n",
      "In[47]:\n",
      "topics = np.array([7, 16, 24, 25, 28, 36, 37, 45, 51, 53, 54, 63, 89, 97])\n",
      "sorting = np.argsort(lda100.components_, axis=1)[:, ::-1]\n",
      "feature_names = np.array(vect.get_feature_names())\n",
      "mglearn.tools.print_topics(topics=topics, feature_names=feature_names,\n",
      "                           sorting=sorting, topics_per_chunk=7, n_words=20)\n",
      "Out[48]:\n",
      "topic 7       topic 16      topic 24      topic 25      topic 28\n",
      "--------      --------      --------      --------      --------\n",
      "thriller      worst         german        car           beautiful\n",
      "suspense      awful         hitler        gets          young\n",
      "horror        boring        nazi          guy           old\n",
      "atmosphere    horrible      midnight      around        romantic\n",
      "mystery       stupid        joe           down          between\n",
      "house         thing         germany       kill          romance\n",
      "director      terrible      years         goes          wonderful\n",
      "quite         script        history       killed        heart\n",
      "bit           nothing       new           going         feel\n",
      "de            worse         modesty       house         year\n",
      "performances  waste         cowboy        away          each\n",
      "dark          pretty        jewish        head          french\n",
      "twist         minutes       past          take          sweet\n",
      "hitchcock     didn          kirk          another       boy\n",
      "tension       actors        young         getting       loved\n",
      "interesting   actually      spanish       doesn         girl\n",
      "mysterious    re            enterprise    now           relationship\n",
      "murder        supposed      von           night         saw\n",
      "ending        mean          nazis         right         both\n",
      "creepy        want          spock         woman         simple\n",
      "topic 36      topic 37      topic 41      topic 45      topic 51\n",
      "--------      --------      --------      --------      --------\n",
      "performance   excellent     war           music         earth\n",
      "role          highly        american      song          space\n",
      "actor         amazing       world         songs         planet\n",
      "cast          wonderful     soldiers      rock          superman\n",
      "play          truly         military      band          alien\n",
      "actors        superb        army          soundtrack    world\n",
      "performances  actors        tarzan        singing       evil\n",
      "played        brilliant     soldier       voice         humans\n",
      "supporting    recommend     america       singer        aliens\n",
      "director      quite         country       sing          human\n",
      "oscar         performance   americans     musical       creatures\n",
      "roles         performances  during        roll          miike\n",
      "actress       perfect       men           fan           monsters\n",
      "excellent     drama         us            metal         apes\n",
      "screen        without       government    concert       clark\n",
      "plays         beautiful     jungle        playing       burton\n",
      "award         human         vietnam       hear          tim\n",
      "work          moving        ii            fans          outer\n",
      "playing       world         political     prince        men\n",
      "gives         recommended   against       especially    moon\n",
      "Topic Modeling and Document Clustering | 351\n",
      "topic 53      topic 54      topic 63      topic 89      topic 97\n",
      "--------      --------      --------      --------      --------\n",
      "scott         money         funny         dead          didn\n",
      "gary          budget        comedy        zombie        thought\n",
      "streisand     actors        laugh         gore          wasn\n",
      "star          low           jokes         zombies       ending\n",
      "hart          worst         humor         blood         minutes\n",
      "lundgren      waste         hilarious     horror        got\n",
      "dolph         10            laughs        flesh         felt\n",
      "career        give          fun           minutes       part\n",
      "sabrina       want          re            body          going\n",
      "role          nothing       funniest      living        seemed\n",
      "temple        terrible      laughing      eating        bit\n",
      "phantom       crap          joke          flick         found\n",
      "judy          must          few           budget        though\n",
      "melissa       reviews       moments       head          nothing\n",
      "zorro         imdb          guy           gory          lot\n",
      "gets          director      unfunny       evil          saw\n",
      "barbra        thing         times         shot          long\n",
      "cast          believe       laughed       low           interesting\n",
      "short         am            comedies      fulci         few\n",
      "serial        actually      isn           re            half\n",
      "The topics we extracted this time seem to be more specific, though many are hard to\n",
      "interpret. Topic 7 seems to be about horror movies and thrillers; topics 16 and 54\n",
      "seem to capture bad reviews, while topic 63 mostly seems to be capturing positive\n",
      "reviews of comedies. If we want to make further inferences using the topics that were\n",
      "discovered, we should confirm the intuition we gained from looking at the highest-\n",
      "ranking words for each topic by looking at the documents that are assigned to these\n",
      "topics. For example, topic 45 seems to be about music. Let’s check which kinds of\n",
      "reviews are assigned to this topic:\n",
      "In[49]:\n",
      "# sort by weight of \"music\" topic 45\n",
      "music = np.argsort(document_topics100[:, 45])[::-1]\n",
      "# print the five documents where the topic is most important\n",
      "for i in music[:10]:\n",
      "    # pshow first two sentences\n",
      "    print(b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\n\")\n",
      "Out[49]:\n",
      "b'I love this movie and never get tired of watching. The music in it is great.\\n'\n",
      "b\"I enjoyed Still Crazy more than any film I have seen in years. A successful\n",
      "  band from the 70's decide to give it another try.\\n\"\n",
      "b'Hollywood Hotel was the last movie musical that Busby Berkeley directed for\n",
      "  Warner Bros. His directing style had changed or evolved to the point that\n",
      "  this film does not contain his signature overhead shots or huge production\n",
      "  numbers with thousands of extras.\\n'\n",
      "b\"What happens to washed up rock-n-roll stars in the late 1990's?\n",
      "  They launch a comeback / reunion tour. At least, that's what the members of\n",
      "  Strange Fruit, a (fictional) 70's stadium rock group do.\\n\"\n",
      "352 | Chapter 7: Working with Text Data\n",
      "b'As a big-time Prince fan of the last three to four years, I really can\\'t\n",
      "  believe I\\'ve only just got round to watching \"Purple Rain\". The brand new\n",
      "  2-disc anniversary Special Edition led me to buy it.\\n'\n",
      "b\"This film is worth seeing alone for Jared Harris' outstanding portrayal\n",
      "  of John Lennon. It doesn't matter that Harris doesn't exactly resemble\n",
      "  Lennon; his mannerisms, expressions, posture, accent and attitude are\n",
      "  pure Lennon.\\n\"\n",
      "b\"The funky, yet strictly second-tier British glam-rock band Strange Fruit\n",
      "  breaks up at the end of the wild'n'wacky excess-ridden 70's. The individual\n",
      "  band members go their separate ways and uncomfortably settle into lackluster\n",
      "  middle age in the dull and uneventful 90's: morose keyboardist Stephen Rea\n",
      "  winds up penniless and down on his luck, vain, neurotic, pretentious lead\n",
      "  singer Bill Nighy tries (and fails) to pursue a floundering solo career,\n",
      "  paranoid drummer Timothy Spall resides in obscurity on a remote farm so he\n",
      "  can avoid paying a hefty back taxes debt, and surly bass player Jimmy Nail\n",
      "  installs roofs for a living.\\n\"\n",
      "b\"I just finished reading a book on Anita Loos' work and the photo in TCM\n",
      "  Magazine of MacDonald in her angel costume looked great (impressive wings),\n",
      "  so I thought I'd watch this movie. I'd never heard of the film before, so I\n",
      "  had no preconceived notions about it whatsoever.\\n\"\n",
      "b'I love this movie!!! Purple Rain came out the year I was born and it has had\n",
      "  my heart since I can remember. Prince is so tight in this movie.\\n'\n",
      "b\"This movie is sort of a Carrie meets Heavy Metal. It's about a highschool\n",
      "  guy who gets picked on alot and he totally gets revenge with the help of a\n",
      "  Heavy Metal ghost.\\n\"\n",
      "As we can see, this topic covers a wide variety of music-centered reviews, from musi‐\n",
      "cals, to biographical movies, to some hard-to-specify genre in the last review. Another\n",
      "interesting way to inspect the topics is to see how much weight each topic gets over‐\n",
      "all, by summing the document_topics over all reviews. We name each topic by the\n",
      "two most common words. Figure 7-6 shows the topic weights learned:\n",
      "In[50]:\n",
      "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
      "topic_names = [\"{:>2} \".format(i) + \" \".join(words)\n",
      "               for i, words in enumerate(feature_names[sorting[:, :2]])]\n",
      "# two column bar chart:\n",
      "for col in [0, 1]:\n",
      "    start = col * 50\n",
      "    end = (col + 1) * 50\n",
      "    ax[col].barh(np.arange(50), np.sum(document_topics100, axis=0)[start:end])\n",
      "    ax[col].set_yticks(np.arange(50))\n",
      "    ax[col].set_yticklabels(topic_names[start:end], ha=\"left\", va=\"top\")\n",
      "    ax[col].invert_yaxis()\n",
      "    ax[col].set_xlim(0, 2000)\n",
      "    yax = ax[col].get_yaxis()\n",
      "    yax.set_tick_params(pad=130)\n",
      "plt.tight_layout()\n",
      "Topic Modeling and Document Clustering | 353\n",
      "Figure 7-6. Topic weights learned by LDA\n",
      "The most important topics are 97, which seems to consist mostly of stopwords, possi‐\n",
      "bly with a slight negative direction; topic 16, which is clearly about bad reviews; fol‐\n",
      "lowed by some genre-specific topics and 36 and 37, both of which seem to contain\n",
      "laudatory words.\n",
      "It seems like LDA mostly discovered two kind of topics, genre-specific and rating-\n",
      "specific, in addition to several more unspecific topics. This is an interesting discovery,\n",
      "as most reviews are made up of some movie-specific comments and some comments\n",
      "that justify or emphasize the rating.\n",
      "Topic models like LDA are interesting methods to understand large text corpora in\n",
      "the absence of labels—or, as here, even if labels are available. The LDA algorithm is\n",
      "randomized, though, and changing the random_state parameter can lead to quite\n",
      "354 | Chapter 7: Working with Text Data\n",
      "different outcomes. While identifying topics can be helpful, any conclusions you\n",
      "draw from an unsupervised model should be taken with a grain of salt, and we rec‐\n",
      "ommend verifying your intuition by looking at the documents in a specific topic. The\n",
      "topics produced by the LDA.transform method can also sometimes be used as a com‐\n",
      "pact representation for supervised learning. This is particularly helpful when few\n",
      "training examples are available.\n",
      "Summary and Outlook\n",
      "In this chapter we talked about the basics of processing text, also known as natural\n",
      "language processing (NLP), with an example application classifying movie reviews.\n",
      "The tools discussed here should serve as a great starting point when trying to process\n",
      "text data. In particular for text classification tasks such as spam and fraud detection\n",
      "or sentiment analysis, bag-of-words representations provide a simple and powerful\n",
      "solution. As is often the case in machine learning, the representation of the data is key\n",
      "in NLP applications, and inspecting the tokens and n-grams that are extracted can\n",
      "give powerful insights into the modeling process. In text-processing applications, it is\n",
      "often possible to introspect models in a meaningful way, as we saw in this chapter, for\n",
      "both supervised and unsupervised tasks. Y ou should take full advantage of this ability\n",
      "when using NLP-based methods in practice.\n",
      "Natural language and text processing is a large research field, and discussing the\n",
      "details of advanced methods is far beyond the scope of this book. If you want to learn\n",
      "more, we recommend the O’Reilly book Natural Language Processing with Python by\n",
      "Steven Bird, Ewan Klein, and Edward Loper, which provides an overview of NLP\n",
      "together with an introduction to the nltk Python package for NLP . Another great and\n",
      "more conceptual book is the standard reference Introduction to Information Retrieval\n",
      "by Christopher Manning, Prabhakar Raghavan, and Hinrich Schütze, which describes\n",
      "fundamental algorithms in information retrieval, NLP , and machine learning. Both\n",
      "books have online versions that can be accessed free of charge. As we discussed ear‐\n",
      "lier, the classes CountVectorizer and TfidfVectorizer only implement relatively\n",
      "simple text-processing methods. For more advanced text-processing methods, we\n",
      "recommend the Python packages spacy (a relatively new but very efficient and well-\n",
      "designed package), nltk (a very well-established and complete but somewhat dated\n",
      "library), and gensim (an NLP package with an emphasis on topic modeling).\n",
      "There have been several very exciting new developments in text processing in recent\n",
      "years, which are outside of the scope of this book and relate to neural networks. The\n",
      "first is the use of continuous vector representations, also known as word vectors or\n",
      "distributed word representations, as implemented in the word2vec library. The origi‐\n",
      "nal paper “Distributed Representations of Words and Phrases and Their Composi‐\n",
      "tionality” by Thomas Mikolov et al. is a great introduction to the subject. Both spacy\n",
      "Summary and Outlook | 355\n",
      "and gensim provide functionality for the techniques discussed in this paper and its\n",
      "follow-ups.\n",
      "Another direction in NLP that has picked up momentum in recent years is the use of\n",
      "recurrent neural networks (RNNs) for text processing. RNNs are a particularly power‐\n",
      "ful type of neural network that can produce output that is again text, in contrast to\n",
      "classification models that can only assign class labels. The ability to produce text as\n",
      "output makes RNNs well suited for automatic translation and summarization. An\n",
      "introduction to the topic can be found in the relatively technical paper “Sequence to\n",
      "Sequence Learning with Neural Networks” by Ilya Suskever, Oriol Vinyals, and Quoc\n",
      "Le. A more practical tutorial using the tensorflow framework can be found on the\n",
      "TensorFlow website.\n",
      "356 | Chapter 7: Working with Text Data\n",
      "CHAPTER 8\n",
      "Wrapping Up\n",
      "Y ou now know how to apply the important machine learning algorithms for super‐\n",
      "vised and unsupervised learning, which allow you to solve a wide variety of machine\n",
      "learning problems. Before we leave you to explore all the possibilities that machine\n",
      "learning offers, we want to give you some final words of advice, point you toward\n",
      "some additional resources, and give you suggestions on how you can further improve\n",
      "your machine learning and data science skills.\n",
      "Approaching a Machine Learning Problem\n",
      "With all the great methods that we introduced in this book now at your fingertips, it\n",
      "may be tempting to jump in and start solving your data-related problem by just run‐\n",
      "ning your favorite algorithm. However, this is not usually a good way to begin your\n",
      "analysis. The machine learning algorithm is usually only a small part of a larger data\n",
      "analysis and decision-making process. To make effective use of machine learning, we\n",
      "need to take a step back and consider the problem at large. First, you should think\n",
      "about what kind of question you want to answer. Do you want to do exploratory anal‐\n",
      "ysis and just see if you find something interesting in the data? Or do you already have\n",
      "a particular goal in mind? Often you will start with a goal, like detecting fraudulent\n",
      "user transactions, making movie recommendations, or finding unknown planets. If\n",
      "you have such a goal, before building a system to achieve it, you should first think\n",
      "about how to define and measure success, and what the impact of a successful solu‐\n",
      "tion would be to your overall business or research goals. Let’s say your goal is fraud\n",
      "detection.\n",
      "357\n",
      "Then the following questions open up:\n",
      "• How do I measure if my fraud prediction is actually working?\n",
      "• Do I have the right data to evaluate an algorithm?\n",
      "• If I am successful, what will be the business impact of my solution?\n",
      "As we discussed in Chapter 5, it is best if you can measure the performance of your\n",
      "algorithm directly using a business metric, like increased profit or decreased losses.\n",
      "This is often hard to do, though. A question that can be easier to answer is “What if I\n",
      "built the perfect model?” If perfectly detecting any fraud will save your company $100\n",
      "a month, these possible savings will probably not be enough to warrant the effort of\n",
      "you even starting to develop an algorithm. On the other hand, if the model might\n",
      "save your company tens of thousands of dollars every month, the problem might be\n",
      "worth exploring.\n",
      "Say you’ve defined the problem to solve, you know a solution might have a significant\n",
      "impact for your project, and you’ve ensured that you have the right information to\n",
      "evaluate success. The next steps are usually acquiring the data and building a working\n",
      "prototype. In this book we have talked about many models you can employ, and how\n",
      "to properly evaluate and tune these models. While trying out models, though, keep in\n",
      "mind that this is only a small part of a larger data science workflow, and model build‐\n",
      "ing is often part of a feedback circle of collecting new data, cleaning data, building\n",
      "models, and analyzing the models. Analyzing the mistakes a model makes can often\n",
      "be informative about what is missing in the data, what additional data could be col‐\n",
      "lected, or how the task could be reformulated to make machine learning more effec‐\n",
      "tive. Collecting more or different data or changing the task formulation slightly might\n",
      "provide a much higher payoff than running endless grid searches to tune parameters.\n",
      "Humans in the Loop\n",
      "Y ou should also consider if and how you should have humans in the loop. Some pro‐\n",
      "cesses (like pedestrian detection in a self-driving car) need to make immediate deci‐\n",
      "sions. Others might not need immediate responses, and so it can be possible to have\n",
      "humans confirm uncertain decisions. Medical applications, for example, might need\n",
      "very high levels of precision that possibly cannot be achieved by a machine learning\n",
      "algorithm alone. But if an algorithm can make 90 percent, 50 percent, or maybe even\n",
      "just 10 percent of decisions automatically, that might already increase response time\n",
      "or reduce cost. Many applications are dominated by “simple cases, ” for which an algo‐\n",
      "rithm can make a decision, with relatively few “complicated cases, ” which can be\n",
      "rerouted to a human.\n",
      "358 | Chapter 8: Wrapping Up\n",
      "From Prototype to Production\n",
      "The tools we’ve discussed in this book are great for many machine learning applica‐\n",
      "tions, and allow very quick analysis and prototyping. Python and scikit-learn are\n",
      "also used in production systems in many organizations—even very large ones like\n",
      "international banks and global social media companies. However, many companies\n",
      "have complex infrastructure, and it is not always easy to include Python in these sys‐\n",
      "tems. That is not necessarily a problem. In many companies, the data analytics teams\n",
      "work with languages like Python and R that allow the quick testing of ideas, while\n",
      "production teams work with languages like Go, Scala, C++, and Java to build robust,\n",
      "scalable systems. Data analysis has different requirements from building live services,\n",
      "and so using different languages for these tasks makes sense. A relatively common\n",
      "solution is to reimplement the solution that was found by the analytics team inside\n",
      "the larger framework, using a high-performance language. This can be easier than\n",
      "embedding a whole library or programming language and converting from and to the\n",
      "different data formats.\n",
      "Regardless of whether you can use scikit-learn in a production system or not, it is\n",
      "important to keep in mind that production systems have different requirements from\n",
      "one-off analysis scripts. If an algorithm is deployed into a larger system, software\n",
      "engineering aspects like reliability, predictability, runtime, and memory requirements\n",
      "gain relevance. Simplicity is key in providing machine learning systems that perform\n",
      "well in these areas. Critically inspect each part of your data processing and prediction\n",
      "pipeline and ask yourself how much complexity each step creates, how robust each\n",
      "component is to changes in the data or compute infrastructure, and if the benefit of\n",
      "each component warrants the complexity. If you are building involved machine learn‐\n",
      "ing systems, we highly recommend reading the paper “Machine Learning: The High\n",
      "Interest Credit Card of Technical Debt” , published by researchers in Google’s\n",
      "machine learning team. The paper highlights the trade-off in creating and maintain‐\n",
      "ing machine learning software in production at a large scale. While the issue of tech‐\n",
      "nical debt is particularly pressing in large-scale and long-term projects, the lessons\n",
      "learned can help us build better software even for short-lived and smaller systems.\n",
      "Testing Production Systems\n",
      "In this book, we covered how to evaluate algorithmic predictions based on a test set\n",
      "that we collected beforehand. This is known as offline evaluation. If your machine\n",
      "learning system is user-facing, this is only the first step in evaluating an algorithm,\n",
      "though. The next step is usually online testing or live testing, where the consequences\n",
      "of employing the algorithm in the overall system are evaluated. Changing the recom‐\n",
      "mendations or search results users are shown by a website can drastically change\n",
      "their behavior and lead to unexpected consequences. To protect against these sur‐\n",
      "prises, most user-facing services employ A/B testing, a form of blind user study. In\n",
      "From Prototype to Production | 359\n",
      "A/B testing, without their knowledge a selected portion of users will be provided with\n",
      "a website or service using algorithm A, while the rest of the users will be provided\n",
      "with algorithm B. For both groups, relevant success metrics will be recorded for a set\n",
      "period of time. Then, the metrics of algorithm A and algorithm B will be compared,\n",
      "and a selection between the two approaches will be made according to these metrics.\n",
      "Using A/B testing enables us to evaluate the algorithms “in the wild, ” which might\n",
      "help us to discover unexpected consequences when users are interacting with our\n",
      "model. Often A is a new model, while B is the established system. There are more\n",
      "elaborate mechanisms for online testing that go beyond A/B testing, such as bandit\n",
      "algorithms. A great introduction to this subject can be found in the book Bandit Algo‐\n",
      "rithms for Website Optimization by John Myles White (O’Reilly). \n",
      "Building Your Own Estimator\n",
      "This book has covered a variety of tools and algorithms implemented in scikit-\n",
      "learn that can be used on a wide range of tasks. However, often there will be some\n",
      "particular processing you need to do for your data that is not implemented in\n",
      "scikit-learn. It may be enough to just preprocess your data before passing it to your\n",
      "scikit-learn model or pipeline. However, if your preprocessing is data dependent,\n",
      "and you want to apply a grid search or cross-validation, things become trickier.\n",
      "In Chapter 6 we discussed the importance of putting all data-dependent processing\n",
      "inside the cross-validation loop. So how can you use your own processing together\n",
      "with the scikit-learn tools? There is a simple solution: build your own estimator!\n",
      "Implementing an estimator that is compatible with the scikit-learn interface, so\n",
      "that it can be used with Pipeline, GridSearchCV, and cross_val_score, is quite easy.\n",
      "Y ou can find detailed instructions in the scikit-learn documentation, but here is\n",
      "the gist. The simplest way to implement a transformer class is by inheriting from\n",
      "BaseEstimator and TransformerMixin, and then implementing the __init__, fit,\n",
      "and predict functions like this:\n",
      "360 | Chapter 8: Wrapping Up\n",
      "In[1]:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "class MyTransformer(BaseEstimator, TransformerMixin):\n",
      "    def __init__(self, first_parameter=1, second_parameter=2):\n",
      "        # All parameters must be specified in the __init__ function\n",
      "        self.first_parameter = 1\n",
      "        self.second_parameter = 2\n",
      "    def fit(self, X, y=None):\n",
      "        # fit should only take X and y as parameters\n",
      "        # Even if your model is unsupervised, you need to accept a y argument!\n",
      "        # Model fitting code goes here\n",
      "        print(\"fitting the model right here\")\n",
      "        # fit returns self\n",
      "        return self\n",
      "    def transform(self, X):\n",
      "        # transform takes as parameter only X\n",
      "        # Apply some transformation to X\n",
      "        X_transformed = X + 1\n",
      "        return X_transformed\n",
      "Implementing a classifier or regressor works similarly, only instead of Transformer\n",
      "Mixin you need to inherit from ClassifierMixin or RegressorMixin. Also, instead\n",
      "of implementing transform, you would implement predict.\n",
      "As you can see from the example given here, implementing your own estimator\n",
      "requires very little code, and most scikit-learn users build up a collection of cus‐\n",
      "tom models over time.\n",
      "Where to Go from Here\n",
      "This book provides an introduction to machine learning and will make you an effec‐\n",
      "tive practitioner. However, if you want to further your machine learning skills, here\n",
      "are some suggestions of books and more specialized resources to investigate to dive\n",
      "deeper.\n",
      "Theory\n",
      "In this book, we tried to provide an intuition of how the most common machine\n",
      "learning algorithms work, without requiring a strong foundation in mathematics or\n",
      "computer science. However, many of the models we discussed use principles from\n",
      "probability theory, linear algebra, and optimization. While it is not necessary to\n",
      "understand all the details of how these algorithms are implemented, we think that\n",
      "Where to Go from Here | 361\n",
      "1 Andreas might not be entirely objective in this matter.\n",
      "knowing some of the theory behind the algorithms will make you a better data scien‐\n",
      "tist. There have been many good books written about the theory of machine learning,\n",
      "and if we were able to excite you about the possibilities that machine learning opens\n",
      "up, we suggest you pick up at least one of them and dig deeper. We already men‐\n",
      "tioned Hastie, Tibshirani, and Friedman’s book The Elements of Statistical Learning in\n",
      "the Preface, but it is worth repeating this recommendation here. Another quite acces‐\n",
      "sible book, with accompanying Python code, is Machine Learning: An Algorithmic\n",
      "Perspective by Stephen Marsland (Chapman and Hall/CRC). Two other highly recom‐\n",
      "mended classics are Pattern Recognition and Machine Learning by Christopher Bishop\n",
      "(Springer), a book that emphasizes a probabilistic framework, and Machine Learning:\n",
      "A Probabilistic Perspective  by Kevin Murphy (MIT Press), a comprehensive (read:\n",
      "1,000+ pages) dissertation on machine learning methods featuring in-depth discus‐\n",
      "sions of state-of-the-art approaches, far beyond what we could cover in this book.\n",
      "Other Machine Learning Frameworks and Packages\n",
      "While scikit-learn is our favorite package for machine learning 1 and Python is our\n",
      "favorite language for machine learning, there are many other options out there.\n",
      "Depending on your needs, Python and scikit-learn might not be the best fit for\n",
      "your particular situation. Often using Python is great for trying out and evaluating\n",
      "models, but larger web services and applications are more commonly written in Java\n",
      "or C++, and integrating into these systems might be necessary for your model to be\n",
      "deployed. Another reason you might want to look beyond scikit-learn is if you are\n",
      "more interested in statistical modeling and inference than prediction. In this case,\n",
      "you should consider the statsmodel package for Python, which implements several\n",
      "linear models with a more statistically minded interface. If you are not married to\n",
      "Python, you might also consider using R, another lingua franca of data scientists. R is\n",
      "a language designed specifically for statistical analysis and is famous for its excellent\n",
      "visualization capabilities and the availability of many (often highly specialized) statis‐\n",
      "tical modeling packages.\n",
      "Another popular machine learning package is vowpal wabbit (often called vw to\n",
      "avoid possible tongue twisting), a highly optimized machine learning package written\n",
      "in C++ with a command-line interface. vw is particularly useful for large datasets and\n",
      "for streaming data. For running machine learning algorithms distributed on a cluster,\n",
      "one of the most popular solutions at the time of writing is mllib, a Scala library built\n",
      "on top of the spark distributed computing environment.\n",
      "362 | Chapter 8: Wrapping Up\n",
      "Ranking, Recommender Systems, and Other Kinds of Learning\n",
      "Because this is an introductory book, we focused on the most common machine\n",
      "learning tasks: classification and regression in supervised learning, and clustering and\n",
      "signal decomposition in unsupervised learning. There are many more kinds of\n",
      "machine learning out there, with many important applications. There are two partic‐\n",
      "ularly important topics that we did not cover in this book. The first is ranking, in\n",
      "which we want to retrieve answers to a particular query, ordered by their relevance.\n",
      "Y ou’ve probably already used a ranking system today; this is how search engines\n",
      "operate. Y ou input a search query and obtain a sorted list of answers, ranked by how\n",
      "relevant they are. A great introduction to ranking is provided in Manning, Raghavan,\n",
      "and Schütze’s book Introduction to Information Retrieval. The second topic is recom‐\n",
      "mender systems , which provide suggestions to users based on their preferences.\n",
      "Y ou’ve probably encountered recommender systems under headings like “People Y ou\n",
      "May Know, ” “Customers Who Bought This Item Also Bought, ” or “Top Picks for\n",
      "Y ou. ” There is plenty of literature on the topic, and if you want to dive right in you\n",
      "might be interested in the now classic “Netflix prize challenge”, in which the Netflix\n",
      "video streaming site released a large dataset of movie preferences and offered a prize\n",
      "of $1 million to the team that could provide the best recommendations. Another\n",
      "common application is prediction of time series (like stock prices), which also has a\n",
      "whole body of literature devoted to it. There are many more machine learning tasks\n",
      "out there—much more than we can list here—and we encourage you to seek out\n",
      "information from books, research papers, and online communities to find the para‐\n",
      "digms that best apply to your situation.\n",
      "Probabilistic Modeling, Inference, and Probabilistic Programming\n",
      "Most machine learning packages provide predefined machine learning models that\n",
      "apply one particular algorithm. However, many real-world problems have a particular\n",
      "structure that, when properly incorporated into the model, can yield much better-\n",
      "performing predictions. Often, the structure of a particular problem can be expressed\n",
      "using the language of probability theory. Such structure commonly arises from hav‐\n",
      "ing a mathematical model of the situation for which you want to predict. To under‐\n",
      "stand what we mean by a structured problem, consider the following example.\n",
      "Let’s say you want to build a mobile application that provides a very detailed position\n",
      "estimate in an outdoor space, to help users navigate a historical site. A mobile phone\n",
      "provides many sensors to help you get precise location measurements, like the GPS,\n",
      "accelerometer, and compass. Y ou also have an exact map of the area. This problem is\n",
      "highly structured. Y ou know where the paths and points of interest are from your\n",
      "map. Y ou also have rough positions from the GPS, and the accelerometer and com‐\n",
      "pass in the user’s device provide you with very precise relative measurements. But\n",
      "throwing these all together into a black-box machine learning system to predict posi‐\n",
      "tions might not be the best idea. This would throw away all the information you\n",
      "Where to Go from Here | 363\n",
      "2 A preprint of Deep Learning can be viewed at http://www.deeplearningbook.org/.\n",
      "already know about how the real world works. If the compass and accelerometer tell\n",
      "you a user is going north, and the GPS is telling you the user is going south, you\n",
      "probably can’t trust the GPS. If your position estimate tells you the user just walked\n",
      "through a wall, you should also be highly skeptical. It’s possible to express this situa‐\n",
      "tion using a probabilistic model, and then use machine learning or probabilistic\n",
      "inference to find out how much you should trust each measurement, and to reason\n",
      "about what the best guess for the location of a user is.\n",
      "Once you’ve expressed the situation and your model of how the different factors work\n",
      "together in the right way, there are methods to compute the predictions using these\n",
      "custom models directly. The most general of these methods are called probabilistic\n",
      "programming languages, and they provide a very elegant and compact way to express\n",
      "a learning problem. Examples of popular probabilistic programming languages are\n",
      "PyMC (which can be used in Python) and Stan (a framework that can be used from\n",
      "several languages, including Python). While these packages require some under‐\n",
      "standing of probability theory, they simplify the creation of new models significantly.\n",
      "Neural Networks\n",
      "While we touched on the subject of neural networks briefly in Chapters 2 and 7, this\n",
      "is a rapidly evolving area of machine learning, with innovations and new applications\n",
      "being announced on a weekly basis. Recent breakthroughs in machine learning and\n",
      "artificial intelligence, such as the victory of the Alpha Go program against human\n",
      "champions in the game of Go, the constantly improving performance of speech\n",
      "understanding, and the availability of near-instantaneous speech translation, have all\n",
      "been driven by these advances. While the progress in this field is so fast-paced that\n",
      "any current reference to the state of the art will soon be outdated, the recent book\n",
      "Deep Learning by Ian Goodfellow, Y oshua Bengio, and Aaron Courville (MIT Press)\n",
      "is a comprehensive introduction into the subject.2\n",
      "Scaling to Larger Datasets\n",
      "In this book, we always assumed that the data we were working with could be stored\n",
      "in a NumPy array or SciPy sparse matrix in memory (RAM). Even though modern\n",
      "servers often have hundreds of gigabytes (GB) of RAM, this is a fundamental restric‐\n",
      "tion on the size of data you can work with. Not everybody can afford to buy such a\n",
      "large machine, or even to rent one from a cloud provider. In most applications, the\n",
      "data that is used to build a machine learning system is relatively small, though, and\n",
      "few machine learning datasets consist of hundreds of gigabites of data or more. This\n",
      "makes expanding your RAM or renting a machine from a cloud provider a viable sol‐\n",
      "ution in many cases. If you need to work with terabytes of data, however, or you need\n",
      "364 | Chapter 8: Wrapping Up\n",
      "to process large amounts of data on a budget, there are two basic strategies: out-of-\n",
      "core learning and parallelization over a cluster.\n",
      "Out-of-core learning describes learning from data that cannot be stored in main\n",
      "memory, but where the learning takes place on a single computer (or even a single\n",
      "processor within a computer). The data is read from a source like the hard disk or the\n",
      "network either one sample at a time or in chunks of multiple samples, so that each\n",
      "chunk fits into RAM. This subset of the data is then processed and the model is upda‐\n",
      "ted to reflect what was learned from the data. Then, this chunk of the data is dis‐\n",
      "carded and the next bit of data is read. Out-of-core learning is implemented for some\n",
      "of the models in scikit-learn, and you can find details on it in the online user\n",
      "guide. Because out-of-core learning requires all of the data to be processed by a single\n",
      "computer, this can lead to long runtimes on very large datasets. Also, not all machine\n",
      "learning algorithms can be implemented in this way.\n",
      "The other strategy for scaling is distributing the data over multiple machines in a\n",
      "compute cluster, and letting each computer process part of the data. This can be\n",
      "much faster for some models, and the size of the data that can be processed is only\n",
      "limited by the size of the cluster. However, such computations often require relatively\n",
      "complex infrastructure. One of the most popular distributed computing platforms at\n",
      "the moment is the spark platform built on top of Hadoop. spark includes some\n",
      "machine learning functionality within the MLLib package. If your data is already on a\n",
      "Hadoop filesystem, or you are already using spark to preprocess your data, this might\n",
      "be the easiest option. If you don’t already have such infrastructure in place, establish‐\n",
      "ing and integrating a spark cluster might be too large an effort, however. The vw\n",
      "package mentioned earlier provides some distributed features and might be a better\n",
      "solution in this case.\n",
      "Honing Your Skills\n",
      "As with many things in life, only practice will allow you to become an expert in the\n",
      "topics we covered in this book. Feature extraction, preprocessing, visualization, and\n",
      "model building can vary widely between different tasks and different datasets. Maybe\n",
      "you are lucky enough to already have access to a variety of datasets and tasks. If you\n",
      "don’t already have a task in mind, a good place to start is machine learning competi‐\n",
      "tions, in which a dataset with a given task is published, and teams compete in creating\n",
      "the best possible predictions. Many companies, nonprofit organizations, and univer‐\n",
      "sities host these competitions. One of the most popular places to find them is Kaggle,\n",
      "a website that regularly holds data science competitions, some of which have substan‐\n",
      "tial prize money attached.\n",
      "The Kaggle forums are also a good source of information about the latest tools and\n",
      "tricks in machine learning, and a wide range of datasets are available on the site. Even\n",
      "more datasets with associated tasks can be found on the OpenML platform , which\n",
      "Where to Go from Here | 365\n",
      "hosts over 20,000 datasets with over 50,000 associated machine learning tasks. Work‐\n",
      "ing with these datasets can provide a great opportunity to practice your machine\n",
      "learning skills. A disadvantage of competitions is that they already provide a particu‐\n",
      "lar metric to optimize, and usually a fixed, preprocessed dataset. Keep in mind that\n",
      "defining the problem and collecting the data are also important aspects of real-world\n",
      "problems, and that representing the problem in the right way might be much more\n",
      "important than squeezing the last percent of accuracy out of a classifier.\n",
      "Conclusion\n",
      "We hope we have convinced you of the usefulness of machine learning in a wide vari‐\n",
      "ety of applications, and how easily machine learning can be implemented in practice.\n",
      "Keep digging into the data, and don’t lose sight of the larger picture.\n",
      "366 | Chapter 8: Wrapping Up\n",
      "Index\n",
      "A\n",
      "A/B testing, 359\n",
      "accuracy, 22, 282\n",
      "acknowledgments, xi\n",
      "adjusted rand index (ARI), 191\n",
      "agglomerative clustering\n",
      "evaluating and comparing, 191\n",
      "example of, 183\n",
      "hierarchical clustering, 184\n",
      "linkage choices, 182\n",
      "principle of, 182\n",
      "algorithm chains and pipelines, 305-321\n",
      "building pipelines, 308\n",
      "building pipelines with make_pipeline,\n",
      "313-316\n",
      "grid search preprocessing steps, 317\n",
      "grid-searching for model selection, 319\n",
      "importance of, 305\n",
      "overview of, 320\n",
      "parameter selection with preprocessing, 306\n",
      "pipeline interface, 312\n",
      "using pipelines in grid searches, 309-311\n",
      "algorithm parameter, 118\n",
      "algorithms (see also models; problem solving)\n",
      "evaluating, 28\n",
      "minimal code to apply to algorithm, 24\n",
      "sample datasets, 30-34\n",
      "scaling\n",
      "MinMaxScaler, 102, 135-139, 190, 230,\n",
      "308, 319\n",
      "Normalizer, 134\n",
      "RobustScaler, 133\n",
      "StandardScaler, 114, 133, 138, 144, 150,\n",
      "190-195, 314-320\n",
      "supervised, classification\n",
      "decision trees, 70-83\n",
      "gradient boosting, 88-91, 119, 124\n",
      "k-nearest neighbors, 35-44\n",
      "kernelized support vector machines,\n",
      "92-104\n",
      "linear SVMs, 56\n",
      "logistic regression, 56\n",
      "naive Bayes, 68-70\n",
      "neural networks, 104-119\n",
      "random forests, 84-88\n",
      "supervised, regression\n",
      "decision trees, 70-83\n",
      "gradient boosting, 88-91\n",
      "k-nearest neighbors, 40\n",
      "Lasso, 53-55\n",
      "linear regression (OLS), 47, 220-229\n",
      "neural networks, 104-119\n",
      "random forests, 84-88\n",
      "Ridge, 49-55, 67, 112, 231, 234, 310,\n",
      "317-319\n",
      "unsupervised, clustering\n",
      "agglomerative clustering, 182-187,\n",
      "191-195, 203-207\n",
      "DBSCAN, 187-190\n",
      "k-means, 168-181\n",
      "unsupervised, manifold learning\n",
      "t-SNE, 163-168\n",
      "unsupervised, signal decomposition\n",
      "non-negative matrix factorization,\n",
      "156-163\n",
      "principal component analysis, 140-155\n",
      "alpha parameter in linear models, 50\n",
      "Anaconda, 6\n",
      "367\n",
      "analysis of variance (ANOV A), 236\n",
      "area under the curve (AUC), 294-296\n",
      "attributions, x\n",
      "average precision, 292\n",
      "B\n",
      "bag-of-words representation\n",
      "applying to movie reviews, 330-334\n",
      "applying to toy dataset, 329\n",
      "more than one word (n-grams), 339-344\n",
      "steps in computing, 327\n",
      "BernoulliNB, 68\n",
      "bigrams, 339\n",
      "binary classification, 25, 56, 276-296\n",
      "binning, 144, 220-224\n",
      "bootstrap samples, 84\n",
      "Boston Housing dataset, 34\n",
      "boundary points, 188\n",
      "Bunch objects, 33\n",
      "business metric, 275, 358\n",
      "C\n",
      "C parameter in SVC, 99\n",
      "calibration, 288\n",
      "cancer dataset, 32\n",
      "categorical features\n",
      "categorical data, defined, 324\n",
      "defined, 211\n",
      "encoded as numbers, 218\n",
      "example of, 212\n",
      "representation in training and test sets, 217\n",
      "representing using one-hot-encoding, 213\n",
      "categorical variables (see categorical features)\n",
      "chaining (see algorithm chains and pipelines)\n",
      "class labels, 25\n",
      "classification problems\n",
      "binary vs. multiclass, 25\n",
      "examples of, 26\n",
      "goals for, 25\n",
      "iris classification example, 14\n",
      "k-nearest neighbors, 35\n",
      "linear models, 56\n",
      "naive Bayes classifiers, 68\n",
      "vs. regression problems, 26\n",
      "classifiers\n",
      "DecisionTreeClassifier, 75, 278\n",
      "DecisionTreeRegressor, 75, 80\n",
      "KNeighborsClassifier, 21-24, 37-43\n",
      "KNeighborsRegressor, 42-47\n",
      "LinearSVC, 56-59, 65, 67, 68\n",
      "LogisticRegression, 56-62, 67, 209, 253, 279,\n",
      "315, 332-347\n",
      "MLPClassifier, 107-119\n",
      "naive Bayes, 68-70\n",
      "SVC, 56, 100, 134, 139, 260, 269-272, 273,\n",
      "305-309, 313-320\n",
      "uncertainty estimates from, 119-127\n",
      "cluster centers, 168\n",
      "clustering algorithms\n",
      "agglomerative clustering, 182-187\n",
      "applications for, 131\n",
      "comparing on faces dataset, 195-207\n",
      "DBSCAN, 187-190\n",
      "evaluating with ground truth, 191-193\n",
      "evaluating without ground truth, 193-195\n",
      "goals of, 168\n",
      "k-means clustering, 168-181\n",
      "summary of, 207\n",
      "code examples\n",
      "downloading, x\n",
      "permission for use, x\n",
      "coef_ attribute, 47, 50\n",
      "comments and questions, xi\n",
      "competitions, 365\n",
      "conflation, 344\n",
      "confusion matrices, 279-286\n",
      "context, 343\n",
      "continuous features, 211, 218\n",
      "core samples/core points, 187\n",
      "corpus, 325\n",
      "cos function, 232\n",
      "CountVectorizer, 334\n",
      "cross-validation\n",
      "analyzing results of, 267-271\n",
      "benefits of, 254\n",
      "cross-validation splitters, 256\n",
      "grid search and, 263-275\n",
      "in scikit-learn, 253\n",
      "leave-one-out cross-validation, 257\n",
      "nested, 272\n",
      "parallelizing with grid search, 274\n",
      "principle of, 252\n",
      "purpose of, 254\n",
      "shuffle-split cross-validation, 258\n",
      "stratified k-fold, 254-256\n",
      "with groups, 259\n",
      "cross_val_score function, 254, 307\n",
      "368 | Index\n",
      "D\n",
      "data points, defined, 4\n",
      "data representation, 211-250 (see also feature\n",
      "extraction/feature engineering; text data)\n",
      "automatic feature selection, 236-241\n",
      "binning and, 220-224\n",
      "categorical features, 212-220\n",
      "effect on model performance, 211\n",
      "integer features, 218\n",
      "model complexity vs. dataset size, 29\n",
      "overview of, 250\n",
      "table analogy, 4\n",
      "in training vs. test sets, 217\n",
      "understanding your data, 4\n",
      "univariate nonlinear transformations,\n",
      "232-236\n",
      "data transformations, 134\n",
      "(see also preprocessing)\n",
      "data-driven research, 1\n",
      "DBSCAN\n",
      "evaluating and comparing, 191-207\n",
      "parameters, 189\n",
      "principle of, 187\n",
      "returned cluster assignments, 190\n",
      "strengths and weaknesses, 187\n",
      "decision boundaries, 37, 56\n",
      "decision function, 120\n",
      "decision trees\n",
      "analyzing, 76\n",
      "building, 71\n",
      "controlling complexity of, 74\n",
      "data representation and, 220-224\n",
      "feature importance in, 77\n",
      "if/else structure of, 70\n",
      "parameters, 82\n",
      "vs. random forests, 83\n",
      "strengths and weaknesses, 83\n",
      "decision_function, 286\n",
      "deep learning (see neural networks)\n",
      "dendrograms, 184\n",
      "dense regions, 187\n",
      "dimensionality reduction, 141, 156\n",
      "discrete features, 211\n",
      "discretization, 220-224\n",
      "distributed computing, 362\n",
      "document clustering, 347\n",
      "documents, defined, 325\n",
      "dual_coef_ attribute, 98\n",
      "E\n",
      "eigenfaces, 147\n",
      "embarrassingly parallel, 274\n",
      "encoding, 328\n",
      "ensembles\n",
      "defined, 83\n",
      "gradient boosted regression trees, 88-92\n",
      "random forests, 83-88\n",
      "Enthought Canopy, 6\n",
      "estimators, 21, 360\n",
      "estimator_ attribute of RFECV, 85\n",
      "evaluation metrics and scoring\n",
      "for binary classification, 276-296\n",
      "for multiclass classification, 296-299\n",
      "metric selection, 275\n",
      "model selection and, 300\n",
      "regression metrics, 299\n",
      "testing production systems, 359\n",
      "exp function, 232\n",
      "expert knowledge, 242-250\n",
      "F\n",
      "f(x)=y formula, 18\n",
      "facial recognition, 147, 157\n",
      "factor analysis (FA), 163\n",
      "false positive rate (FPR), 292\n",
      "false positive/false negative errors, 277\n",
      "feature extraction/feature engineering, 211-250\n",
      "(see also data representation; text data)\n",
      "augmenting data with, 211\n",
      "automatic feature selection, 236-241\n",
      "categorical features, 212-220\n",
      "continuous vs. discrete features, 211\n",
      "defined, 4, 34, 211\n",
      "interaction features, 224-232\n",
      "with non-negative matrix factorization, 156\n",
      "overview of, 250\n",
      "polynomial features, 224-232\n",
      "with principal component analysis, 147\n",
      "univariate nonlinear transformations,\n",
      "232-236\n",
      "using expert knowledge, 242-250\n",
      "feature importance, 77\n",
      "features, defined, 4\n",
      "feature_names attribute, 33\n",
      "feed-forward neural networks, 104\n",
      "fit method, 21, 68, 119, 135\n",
      "fit_transform method, 138\n",
      "floating-point numbers, 26\n",
      "Index | 369\n",
      "folds, 252\n",
      "forge dataset, 30\n",
      "frameworks, 362\n",
      "free string data, 324\n",
      "freeform text data, 325\n",
      "G\n",
      "gamma parameter, 100\n",
      "Gaussian kernels of SVC, 97, 100\n",
      "GaussianNB, 68\n",
      "generalization\n",
      "building models for, 26\n",
      "defined, 17\n",
      "examples of, 27\n",
      "get_dummies function, 218\n",
      "get_support method of feature selection, 237\n",
      "gradient boosted regression trees\n",
      "for feature selection, 220-224\n",
      "learning_rate parameter, 89\n",
      "parameters, 91\n",
      "vs. random forests, 88\n",
      "strengths and weaknesses, 91\n",
      "training set accuracy, 90\n",
      "graphviz module, 76\n",
      "grid search\n",
      "accessing pipeline attributes, 315\n",
      "alternate strategies for, 272\n",
      "avoiding overfitting, 261\n",
      "model selection with, 319\n",
      "nested cross-validation, 272\n",
      "parallelizing with cross-validation, 274\n",
      "pipeline preprocessing, 317\n",
      "searching non-grid spaces, 271\n",
      "simple example of, 261\n",
      "tuning parameters with, 260\n",
      "using pipelines in, 309-311\n",
      "with cross-validation, 263-275\n",
      "GridSearchCV\n",
      "best_estimator_ attribute, 267\n",
      "best_params_ attribute, 266\n",
      "best_score_ attribute, 266\n",
      "H\n",
      "handcoded rules, disadvantages of, 1\n",
      "heat maps, 146\n",
      "hidden layers, 106\n",
      "hidden units, 105\n",
      "hierarchical clustering, 184\n",
      "high recall, 293\n",
      "high-dimensional datasets, 32\n",
      "histograms, 144\n",
      "hit rate, 283\n",
      "hold-out sets, 17\n",
      "human involvement/oversight, 358\n",
      "I\n",
      "imbalanced datasets, 277\n",
      "independent component analysis (ICA), 163\n",
      "inference, 363\n",
      "information leakage, 310\n",
      "information retrieval (IR), 325\n",
      "integer features, 218\n",
      "\"intelligent\" applications, 1\n",
      "interactions, 34, 224-232\n",
      "intercept_ attribute, 47\n",
      "iris classification application\n",
      "data inspection, 19\n",
      "dataset for, 14\n",
      "goals for, 13\n",
      "k-nearest neighbors, 20\n",
      "making predictions, 22\n",
      "model evaluation, 22\n",
      "multiclass problem, 26\n",
      "overview of, 23\n",
      "training and testing data, 17\n",
      "iterative feature selection, 240\n",
      "J\n",
      "Jupyter Notebook, 7\n",
      "K\n",
      "k-fold cross-validation, 252\n",
      "k-means clustering\n",
      "applying with scikit-learn, 170\n",
      "vs. classification, 171\n",
      "cluster centers, 169\n",
      "complex datasets, 179\n",
      "evaluating and comparing, 191\n",
      "example of, 168\n",
      "failures of, 173\n",
      "strengths and weaknesses, 181\n",
      "vector quantization with, 176\n",
      "k-nearest neighbors (k-NN)\n",
      "analyzing KNeighborsClassifier, 37\n",
      "analyzing KNeighborsRegressor, 43\n",
      "building, 20\n",
      "classification, 35-37\n",
      "370 | Index\n",
      "vs. linear models, 46\n",
      "parameters, 44\n",
      "predictions with, 35\n",
      "regression, 40\n",
      "strengths and weaknesses, 44\n",
      "Kaggle, 365\n",
      "kernelized support vector machines (SVMs)\n",
      "kernel trick, 97\n",
      "linear models and nonlinear features, 92\n",
      "vs. linear support vector machines, 92\n",
      "mathematics of, 92\n",
      "parameters, 104\n",
      "predictions with, 98\n",
      "preprocessing data for, 102\n",
      "strengths and weaknesses, 104\n",
      "tuning SVM parameters, 99\n",
      "understanding, 98\n",
      "knn object, 21\n",
      "L\n",
      "L1 regularization, 53\n",
      "L2 regularization, 49, 60, 67\n",
      "Lasso model, 53\n",
      "Latent Dirichlet Allocation (LDA), 348-355\n",
      "leafs, 71\n",
      "leakage, 310\n",
      "learn from the past approach, 243\n",
      "learning_rate parameter, 89\n",
      "leave-one-out cross-validation, 257\n",
      "lemmatization, 344-347\n",
      "linear functions, 56\n",
      "linear models\n",
      "classification, 56\n",
      "data representation and, 220-224\n",
      "vs. k-nearest neighbors, 46\n",
      "Lasso, 53\n",
      "linear SVMs, 56\n",
      "logistic regression, 56\n",
      "multiclass classification, 63\n",
      "ordinary least squares, 47\n",
      "parameters, 67\n",
      "predictions with, 45\n",
      "regression, 45\n",
      "ridge regression, 49\n",
      "strengths and weaknesses, 67\n",
      "linear regression, 47, 224-232\n",
      "linear support vector machines (SVMs), 56\n",
      "linkage arrays, 185\n",
      "live testing, 359\n",
      "log function, 232\n",
      "loss functions, 56\n",
      "low-dimensional datasets, 32\n",
      "M\n",
      "machine learning\n",
      "algorithm chains and pipelines, 305-321\n",
      "applications for, 1-5\n",
      "approach to problem solving, 357-366\n",
      "benefits of Python for, 5\n",
      "building your own systems, vii\n",
      "data representation, 211-250\n",
      "examples of, 1, 13-23\n",
      "mathematics of, vii\n",
      "model evaluation and improvement,\n",
      "251-303\n",
      "preprocessing and scaling, 132-140\n",
      "prerequisites to learning, vii\n",
      "resources, ix, 361-366\n",
      "scikit-learn and, 5-13\n",
      "supervised learning, 25-129\n",
      "understanding your data, 4\n",
      "unsupervised learning, 131-209\n",
      "working with text data, 323-356\n",
      "make_pipeline function\n",
      "accessing step attributes, 314\n",
      "displaying steps attribute, 314\n",
      "grid-searched pipelines and, 315\n",
      "syntax for, 313\n",
      "manifold learning algorithms\n",
      "applications for, 164\n",
      "example of, 164\n",
      "results of, 168\n",
      "visualizations with, 163\n",
      "mathematical functions for feature transforma‐\n",
      "tions, 232\n",
      "matplotlib, 9\n",
      "max_features parameter, 84\n",
      "meta-estimators for trees and forests, 266\n",
      "method chaining, 68\n",
      "metrics (see evaluation metrics and scoring)\n",
      "mglearn, 11\n",
      "mllib, 362\n",
      "model-based feature selection, 238\n",
      "models (see also algorithms)\n",
      "calibrated, 288\n",
      "capable of generalization, 26\n",
      "coefficients with text data, 338-347\n",
      "complexity vs. dataset size, 29\n",
      "Index | 371\n",
      "cross-validation of, 252-260\n",
      "effect of data representation choices on, 211\n",
      "evaluation and improvement, 251-252\n",
      "evaluation metrics and scoring, 275-302\n",
      "iris classification application, 13-23\n",
      "overfitting vs. underfitting, 28\n",
      "pipeline preprocessing and, 317\n",
      "selecting, 300\n",
      "selecting with grid search, 319\n",
      "theory behind, 361\n",
      "tuning parameters with grid search, 260-275\n",
      "movie reviews, 325\n",
      "multiclass classification\n",
      "vs. binary classification, 25\n",
      "evaluation metrics and scoring for, 296-299\n",
      "linear models for, 63\n",
      "uncertainty estimates, 124\n",
      "multilayer perceptrons (MLPs), 104\n",
      "MultinomialNB, 68\n",
      "N\n",
      "n-grams, 339\n",
      "naive Bayes classifiers\n",
      "kinds in scikit-learn, 68\n",
      "parameters, 70\n",
      "strengths and weaknesses, 70\n",
      "natural language processing (NLP), 325, 355\n",
      "negative class, 26\n",
      "nested cross-validation, 272\n",
      "Netflix prize challenge, 363\n",
      "neural networks (deep learning)\n",
      "accuracy of, 114\n",
      "estimating complexity in, 118\n",
      "predictions with, 104\n",
      "randomization in, 113\n",
      "recent breakthroughs in, 364\n",
      "strengths and weaknesses, 117\n",
      "tuning, 108\n",
      "non-negative matrix factorization (NMF)\n",
      "applications for, 156\n",
      "applying to face images, 157\n",
      "applying to synthetic data, 156\n",
      "normalization, 344\n",
      "normalized mutual information (NMI), 191\n",
      "NumPy (Numeric Python) library, 7\n",
      "O\n",
      "offline evaluation, 359\n",
      "one-hot-encoding, 213-217\n",
      "one-out-of-N encoding, 213-217\n",
      "one-vs.-rest approach, 63\n",
      "online resources, ix\n",
      "online testing, 359\n",
      "OpenML platform, 365\n",
      "operating points, 289\n",
      "ordinary least squares (OLS), 47\n",
      "out-of-core learning, 364\n",
      "outlier detection, 197\n",
      "overfitting, 28, 261\n",
      "P\n",
      "pair plots, 19\n",
      "pandas\n",
      "benefits of, 10\n",
      "checking string-encoded data, 214\n",
      "column indexing in, 216\n",
      "converting data to one-hot-encoding, 214\n",
      "get_dummies function, 218\n",
      "parallelization over a cluster, 364\n",
      "permissions, x\n",
      "pipelines (see algorithm chains and pipelines)\n",
      "polynomial features, 224-232\n",
      "polynomial kernels, 97\n",
      "polynomial regression, 228\n",
      "positive class, 26\n",
      "POSIX time, 244\n",
      "pre- and post-pruning, 74\n",
      "precision, 282, 358\n",
      "precision-recall curves, 289-292\n",
      "predict for the future approach, 243\n",
      "predict method, 22, 37, 68, 267\n",
      "predict_proba function, 122, 286\n",
      "preprocessing, 132-140\n",
      "data transformation application, 134\n",
      "effect on supervised learning, 138\n",
      "kinds of, 133\n",
      "parameter selection with, 306\n",
      "pipelines and, 317\n",
      "purpose of, 132\n",
      "scaling training and test data, 136\n",
      "principal component analysis (PCA)\n",
      "drawbacks of, 146\n",
      "example of, 140\n",
      "feature extraction with, 147\n",
      "unsupervised nature of, 145\n",
      "visualizations with, 142\n",
      "whitening option, 150\n",
      "probabilistic modeling, 363\n",
      "372 | Index\n",
      "probabilistic programming, 363\n",
      "problem solving\n",
      "building your own estimators, 360\n",
      "business metrics and, 358\n",
      "initial approach to, 357\n",
      "resources, 361-366\n",
      "simple vs. complicated cases, 358\n",
      "steps of, 358\n",
      "testing your system, 359\n",
      "tool choice, 359\n",
      "production systems\n",
      "testing, 359\n",
      "tool choice, 359\n",
      "pruning for decision trees, 74\n",
      "pseudorandom number generators, 18\n",
      "pure leafs, 73\n",
      "PyMC language, 364\n",
      "Python\n",
      "benefits of, 5\n",
      "prepackaged distributions, 6\n",
      "Python 2 vs. Python 3, 12\n",
      "Python(x,y), 6\n",
      "statsmodel package, 362\n",
      "R\n",
      "R language, 362\n",
      "radial basis function (RBF) kernel, 97\n",
      "random forests\n",
      "analyzing, 85\n",
      "building, 84\n",
      "data representation and, 220-224\n",
      "vs. decision trees, 83\n",
      "vs. gradient boosted regression trees, 88\n",
      "parameters, 88\n",
      "predictions with, 84\n",
      "randomization in, 83\n",
      "strengths and weaknesses, 87\n",
      "random_state parameter, 18\n",
      "ranking, 363\n",
      "real numbers, 26\n",
      "recall, 282\n",
      "receiver operating characteristics (ROC)\n",
      "curves, 292-296\n",
      "recommender systems, 363\n",
      "rectified linear unit (relu), 106\n",
      "rectifying nonlinearity, 106\n",
      "recurrent neural networks (RNNs), 356\n",
      "recursive feature elimination (RFE), 240\n",
      "regression\n",
      "f_regression, 236, 310\n",
      "LinearRegression, 47-56, 81, 247\n",
      "regression problems\n",
      "Boston Housing dataset, 34\n",
      "vs. classification problems, 26\n",
      "evaluation metrics and scoring, 299\n",
      "examples of, 26\n",
      "goals for, 26\n",
      "k-nearest neighbors, 40\n",
      "Lasso, 53\n",
      "linear models, 45\n",
      "ridge regression, 49\n",
      "wave dataset illustration, 31\n",
      "regularization\n",
      "L1 regularization, 53\n",
      "L2 regularization, 49, 60\n",
      "rescaling\n",
      "example of, 132-140\n",
      "kernel SVMs, 102\n",
      "resources, ix\n",
      "ridge regression, 49\n",
      "robustness-based clustering, 194\n",
      "roots, 72\n",
      "S\n",
      "Safari Books Online, x\n",
      "samples, defined, 4\n",
      "scaling, 132-140\n",
      "data transformation application, 134\n",
      "effect on supervised learning, 138\n",
      "into larger datasets, 364\n",
      "kinds of, 133\n",
      "purpose of, 132\n",
      "training and test data, 136\n",
      "scatter plots, 19\n",
      "scikit-learn\n",
      "alternate frameworks, 362\n",
      "benefits of, 5\n",
      "Bunch objects, 33\n",
      "cancer dataset, 32\n",
      "core code for, 24\n",
      "data and labels in, 18\n",
      "documentation, 6\n",
      "feature_names attribute, 33\n",
      "fit method, 21, 68, 119, 135\n",
      "fit_transform method, 138\n",
      "installing, 6\n",
      "knn object, 21\n",
      "libraries and tools, 7-11\n",
      "Index | 373\n",
      "predict method, 22, 37, 68\n",
      "Python 2 vs. Python 3, 12\n",
      "random_state parameter, 18\n",
      "scaling mechanisms in, 139\n",
      "score method, 23, 37, 43\n",
      "transform method, 135\n",
      "user guide, 6\n",
      "versions used, 12\n",
      "scikit-learn classes and functions\n",
      "accuracy_score, 193\n",
      "adjusted_rand_score, 191\n",
      "AgglomerativeClustering, 182, 191, 203-207\n",
      "average_precision_score, 292\n",
      "BaseEstimator, 360\n",
      "classification_report, 284-288, 298\n",
      "confusion_matrix, 279-299\n",
      "CountVectorizer, 329-355\n",
      "cross_val_score, 253, 256, 300, 307, 360\n",
      "DBSCAN, 187-190\n",
      "DecisionTreeClassifier, 75, 278\n",
      "DecisionTreeRegressor, 75, 80\n",
      "DummyClassifier, 278\n",
      "ElasticNet class, 55\n",
      "ENGLISH_STOP_WORDS, 334\n",
      "Estimator, 21\n",
      "export_graphviz, 76\n",
      "f1_score, 284, 291\n",
      "fetch_lfw_people, 147\n",
      "f_regression, 236, 310\n",
      "GradientBoostingClassifier, 88-91, 119, 124\n",
      "GridSearchCV, 263-275, 300-301, 305-309,\n",
      "315-320, 360\n",
      "GroupKFold, 259\n",
      "KFold, 256, 260\n",
      "KMeans, 174-181\n",
      "KNeighborsClassifier, 21-24, 37-43\n",
      "KNeighborsRegressor, 42-47\n",
      "Lasso, 53-55\n",
      "LatentDirichletAllocation, 348\n",
      "LeaveOneOut, 257\n",
      "LinearRegression, 47-56, 81, 247\n",
      "LinearSVC, 56-59, 65, 67, 68\n",
      "load_boston, 34, 230, 317\n",
      "load_breast_cancer, 32, 38, 59, 75, 134, 144,\n",
      "236, 305\n",
      "load_digits, 164, 278\n",
      "load_files, 326\n",
      "load_iris, 14, 124, 253\n",
      "LogisticRegression, 56-62, 67, 209, 253, 279,\n",
      "315, 332-347\n",
      "make_blobs, 92, 119, 136, 173-183, 188, 286\n",
      "make_circles, 119\n",
      "make_moons, 85, 108, 175, 190-195\n",
      "make_pipeline, 313-319\n",
      "MinMaxScaler, 102, 133, 135-139, 190, 230,\n",
      "308, 309, 319\n",
      "MLPClassifier, 107-119\n",
      "NMF, 140, 159-163, 179-182, 348\n",
      "Normalizer, 134\n",
      "OneHotEncoder, 218, 247\n",
      "ParameterGrid, 274\n",
      "PCA, 140-166, 179, 195-206, 313-314, 348\n",
      "Pipeline, 305-319, 320\n",
      "PolynomialFeatures, 227-230, 248, 317\n",
      "precision_recall_curve, 289-292\n",
      "RandomForestClassifier, 84-86, 238, 290,\n",
      "319\n",
      "RandomForestRegressor, 84, 231, 240\n",
      "RFE, 240-241\n",
      "Ridge, 49, 67, 112, 231, 234, 310, 317-319\n",
      "RobustScaler, 133\n",
      "roc_auc_score, 294-301\n",
      "roc_curve, 293-296\n",
      "SCORERS, 301\n",
      "SelectFromModel, 238\n",
      "SelectPercentile, 236, 310\n",
      "ShuffleSplit, 258, 258\n",
      "silhouette_score, 193\n",
      "StandardScaler, 114, 133, 138, 144, 150,\n",
      "190-195, 314-320\n",
      "StratifiedKFold, 260, 274\n",
      "StratifiedShuffleSplit, 258, 347\n",
      "SVC, 56, 100, 134, 139, 260-267, 269-272,\n",
      "305-309, 313-320\n",
      "SVR, 92, 229\n",
      "TfidfVectorizer, 336-356\n",
      "train_test_split, 17-19, 251, 286, 289\n",
      "TransformerMixin, 360\n",
      "TSNE, 166\n",
      "SciPy, 8\n",
      "score method, 23, 37, 43, 267, 308\n",
      "sensitivity, 283\n",
      "sentiment analysis example, 325\n",
      "shapes, defined, 16\n",
      "shuffle-split cross-validation, 258\n",
      "sin function, 232\n",
      "soft voting strategy, 84\n",
      "374 | Index\n",
      "spark computing environment, 362\n",
      "sparse coding (dictionary learning), 163\n",
      "sparse datasets, 44\n",
      "splits, 252\n",
      "Stan language, 364\n",
      "statsmodel package, 362\n",
      "stemming, 344-347\n",
      "stopwords, 334\n",
      "stratified k-fold cross-validation, 254-256\n",
      "string-encoded categorical data, 214\n",
      "supervised learning, 25-129 (see also classifica‐\n",
      "tion problems; regression problems)\n",
      "algorithms for\n",
      "decision trees, 70-83\n",
      "ensembles of decision trees, 83-92\n",
      "k-nearest neighbors, 35-44\n",
      "kernelized support vector machines,\n",
      "92-104\n",
      "linear models, 45-68\n",
      "naive Bayes classifiers, 68\n",
      "neural networks (deep learning),\n",
      "104-119\n",
      "overview of, 2\n",
      "data representation, 4\n",
      "examples of, 3\n",
      "generalization, 26\n",
      "goals for, 25\n",
      "model complexity vs. dataset size, 29\n",
      "overfitting vs. underfitting, 28\n",
      "overview of, 127\n",
      "sample datasets, 30-34\n",
      "uncertainty estimates, 119-127\n",
      "support vectors, 98\n",
      "synthetic datasets, 30\n",
      "T\n",
      "t-SNE algorithm (see manifold learning algo‐\n",
      "rithms)\n",
      "tangens hyperbolicus (tanh), 106\n",
      "term frequency–inverse document frequency\n",
      "(tf–idf), 336-347\n",
      "terminal nodes, 71\n",
      "test data/test sets\n",
      "Boston Housing dataset, 34\n",
      "defined, 17\n",
      "forge dataset, 30\n",
      "wave dataset, 31\n",
      "Wisconsin Breast Cancer dataset, 32\n",
      "text data, 323-356\n",
      "bag-of-words representation, 327-334\n",
      "examples of, 323\n",
      "model coefficients, 338\n",
      "overview of, 355\n",
      "rescaling data with tf-idf, 336-338\n",
      "sentiment analysis example, 325\n",
      "stopwords, 334\n",
      "topic modeling and document clustering,\n",
      "347-355\n",
      "types of, 323-325\n",
      "time series predictions, 363\n",
      "tokenization, 328, 344-347\n",
      "top nodes, 72\n",
      "topic modeling, with LDA, 347-355\n",
      "training data, 17\n",
      "train_test_split function, 254\n",
      "transform method, 135, 312, 334\n",
      "transformations\n",
      "selecting, 235\n",
      "univariate nonlinear, 232-236\n",
      "unsupervised, 131\n",
      "tree module, 76\n",
      "trigrams, 339\n",
      "true positive rate (TPR), 283, 292\n",
      "true positives/true negatives, 281\n",
      "typographical conventions, ix\n",
      "U\n",
      "uncertainty estimates\n",
      "applications for, 119\n",
      "decision function, 120\n",
      "in binary classification evaluation, 286-288\n",
      "multiclass classification, 124\n",
      "predicting probabilities, 122\n",
      "underfitting, 28\n",
      "unigrams, 340\n",
      "univariate nonlinear transformations, 232-236\n",
      "univariate statistics, 236\n",
      "unsupervised learning, 131-209\n",
      "algorithms for\n",
      "agglomerative clustering, 182-187\n",
      "clustering, 168-207\n",
      "DBSCAN, 187-190\n",
      "k-means clustering, 168-181\n",
      "manifold learning with t-SNE, 163-168\n",
      "non-negative matrix factorization,\n",
      "156-163\n",
      "overview of, 3\n",
      "principal component analysis, 140-155\n",
      "Index | 375\n",
      "challenges of, 132\n",
      "data representation, 4\n",
      "examples of, 3\n",
      "overview of, 208\n",
      "scaling and preprocessing for, 132-140\n",
      "types of, 131\n",
      "unsupervised transformations, 131\n",
      "V\n",
      "value_counts function, 214\n",
      "vector quantization, 176\n",
      "vocabulary building, 328\n",
      "voting, 36\n",
      "vowpal wabbit, 362\n",
      "W\n",
      "wave dataset, 31\n",
      "weak learners, 88\n",
      "weights, 47, 106\n",
      "whitening option, 150\n",
      "Wisconsin Breast Cancer dataset, 32\n",
      "word stems, 344\n",
      "X\n",
      "xgboost package, 91\n",
      "xkcd Color Survey, 324\n",
      "376 | Index\n",
      "About the Authors\n",
      "Andreas Müller received his PhD in machine learning from the University of Bonn.\n",
      "After working as a machine learning researcher on computer vision applications at\n",
      "Amazon for a year, he joined the Center for Data Science at New Y ork University. For\n",
      "the last four years, he has been a maintainer of and one of the core contributors to\n",
      "scikit-learn, a machine learning toolkit widely used in industry and academia, and\n",
      "has authored and contributed to several other widely used machine learning pack‐\n",
      "ages. His mission is to create open tools to lower the barrier of entry for machine\n",
      "learning applications, promote reproducible science, and democratize the access to\n",
      "high-quality machine learning algorithms.\n",
      "Sarah Guido is a data scientist who has spent a lot of time working in start-ups. She\n",
      "loves Python, machine learning, large quantities of data, and the tech world. An\n",
      "accomplished conference speaker, Sarah attended the University of Michigan for grad\n",
      "school and currently resides in New Y ork City.\n",
      "Colophon\n",
      "The animal on the cover of Introduction to Machine Learning with Python  is a hell‐\n",
      "bender salamander (Cryptobranchus alleganiensis), an amphibian native to the eastern\n",
      "United States (ranging from New Y ork to Georgia). It has many colorful nicknames,\n",
      "including “ Allegheny alligator, ” “snot otter, ” and “mud-devil. ” The origin of the name\n",
      "“hellbender” is unclear: one theory is that early settlers found the salamander’s\n",
      "appearance unsettling and supposed it to be a demonic creature trying to return to\n",
      "hell.\n",
      "The hellbender salamander is a member of the giant salamander family, and can grow\n",
      "as large as 29 inches long. This is the third-largest aquatic salamander species in the\n",
      "world. Their bodies are rather flat, with thick folds of skin along their sides. While\n",
      "they do have a single gill on each side of the neck, hellbenders largely rely on their\n",
      "skin folds to breathe: gas flows in and out through capillaries near the surface of the\n",
      "skin.\n",
      "Because of this, their ideal habitat is in clear, fast-moving, shallow streams, which\n",
      "provide plenty of oxygen. The hellbender shelters under rocks and hunts primarily by\n",
      "sense of smell, though it is also able to detect vibrations in the water. Its diet is made\n",
      "up of crayfish, small fish, and occasionally the eggs of its own species. The hellbender\n",
      "is also a key member of its ecosystem as prey: predators include various fish, snakes,\n",
      "and turtles.\n",
      "Hellbender salamander populations have decreased significantly in the last few deca‐\n",
      "des. Water quality is the largest issue, as their respiratory system makes them very\n",
      "sensitive to polluted or murky water. An increase in agriculture and other human\n",
      "activity near their habitat means greater amounts of sediment and chemicals in the\n",
      "water. In an effort to save this endangered species, biologists have begun to raise the\n",
      "amphibians in captivity and release them when they reach a less vulnerable age.\n",
      "Many of the animals on O’Reilly covers are endangered; all of them are important to\n",
      "the world. To learn more about how you can help, go to animals.oreilly.com.\n",
      "The cover image is from Wood’s Animate Creation. The cover fonts are URW Type‐\n",
      "writer and Guardian Sans. The text font is Adobe Minion Pro; the heading font is\n",
      "Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all loaded documents and print their metadata.\n",
    "\n",
    "for documents in docs:\n",
    "    print(documents.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b197e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902\n",
      "About the Author\n",
      "Aurélien Géron is a Machine Learning consultant. A former Googler, he led the Y ou‐\n",
      "Tube video classification team from 2013 to 2016. He was also a founder and CTO of\n",
      "Wifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\n",
      "of Polyconseil in 2001, the firm that now manages the electric car sharing service\n",
      "Autolib’ .\n",
      "Before this he worked as an engineer in a variety of domains: finance (JP Morgan and\n",
      "Société Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\n",
      "published a few technical books (on C++, WiFi, and internet architectures), and was\n",
      "a Computer Science lecturer in a French engineering school.\n",
      "A few fun facts: he taught his three children to count in binary with their fingers (up\n",
      "to 1023), he studied microbiology and evolutionary genetics before going into soft‐\n",
      "ware engineering, and his parachute didn’t open on the second jump.\n",
      "Colophon\n",
      "The animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\n",
      "sorFlow is the fire salamander ( Salamandra salamandra), an amphibian found across\n",
      "most of Europe. Its black, glossy skin features large yellow spots on the head and\n",
      "back, signaling the presence of alkaloid toxins. This is a possible source of this\n",
      "amphibian’s common name: contact with these toxins (which they can also spray\n",
      "short distances) causes convulsions and hyperventilation. Either the painful poisons\n",
      "or the moistness of the salamander’s skin (or both) led to a misguided belief that these\n",
      "creatures not only could survive being placed in fire but could extinguish it as well.\n",
      "Fire salamanders live in shaded forests, hiding in moist crevices and under logs near\n",
      "the pools or other freshwater bodies that facilitate their breeding. Though they spend\n",
      "most of their life on land, they give birth to their young in water. They subsist mostly\n",
      "on a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\n",
      "in length, and in captivity, may live as long as 50 years.\n",
      "The fire salamander’s numbers have been reduced by destruction of their forest habi‐\n",
      "tat and capture for the pet trade, but the greatest threat is the susceptibility of their\n",
      "moisture-permeable skin to pollutants and microbes. Since 2014, they have become\n",
      "extinct in parts of the Netherlands and Belgium due to an introduced fungus.\n",
      "Many of the animals on O’Reilly covers are endangered; all of them are important to\n",
      "the world. To learn more about how you can help, go to animals.oreilly.com.\n",
      "The cover image is from Wood’s Illustrated Natural History. The cover fonts are URW\n",
      "Typewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\n",
      "is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'moddate': '2019-05-07T15:51:31+00:00', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'trapped': '/False', 'source': 'books\\\\Hands-On Machine Learning with Scikit-Learn, Keras, and -- Aurélien Géron -- ( WeLib.org ).pdf', 'total_pages': 510, 'page': 509, 'page_label': '484'}\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of loaded documents/pages.\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "# Print the content of the 510th document (index 509).\n",
    "print(docs[509].page_content)\n",
    "\n",
    "\n",
    "# Print the metadata of the 510th document (index 509).\n",
    "print(docs[509].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe0f79",
   "metadata": {},
   "source": [
    "# **WebBase Loader**\n",
    "\n",
    "webbase loader is a document loader in langchain used to load and extract text content from web pages(urls)\n",
    "\n",
    "it uses beautifulSoup under the hood to parse HTML and extract visible text.\n",
    "\n",
    "When to use:\n",
    "\n",
    "for blogs, news articles or public websites where the content is primarily text based and static.\n",
    "\n",
    "LImitations:\n",
    "Doesn't handle javascript heavy pages well (use Selenium URLLoader for that).\n",
    "loads only static content (what's in the html, not what loads after the page reders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e1528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\practicing_langchain_models\\venv\\lib\\site-packages (from beautifulsoup4) (4.14.0)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "\n",
      "   ---------------------------------------- 0/2 [soupsieve]\n",
      "   ---------------------------------------- 0/2 [soupsieve]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   ---------------------------------------- 2/2 [beautifulsoup4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.13.4 soupsieve-2.7\n"
     ]
    }
   ],
   "source": [
    "# Install BeautifulSoup (bs4) for web page parsing support in WebBaseLoader\n",
    "! pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1af3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import WebBaseLoader for loading web pages as documents,\n",
    "# and other necessary modules for LLM, output parsing, and prompt templates.\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (such as API keys) from a .env file.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27e09c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize the HuggingFaceEndpoint and wrap it in a ChatHuggingFace model for text generation.\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    task='text-generation'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for answering a question based on provided text.\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='Answer the following question \\n {question} from the following text - \\n {text} ',\n",
    "    input_variables=['question' , 'text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07819e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a string output parser to extract plain text from the model's response.\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf9125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the URL to load and create a WebBaseLoader instance for it.\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "\n",
    "loader = WebBaseLoader(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the web page content as LangChain document objects.\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730488ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a chain that applies the prompt, model, and parser in sequence.\n",
    "\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef30ee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of data science is expected to be shaped by several trends and advancements. Some of the key developments that are likely to impact the field of data science include:\n",
      "\n",
      "1. **Artificial Intelligence and Machine Learning**: The increasing use of AI and ML in data science will lead to more sophisticated and accurate analysis of large datasets.\n",
      "2. **Cloud Computing**: The growth of cloud computing will enable easier access to computational power and storage, making it easier to process and analyze large datasets.\n",
      "3. **Big Data and IoT**: The increasing amount of data being generated by IoT devices and other sources will require more efficient and effective methods for processing and analyzing large datasets.\n",
      "4. **Data Governance and Ethics**: As data science becomes more prevalent, there will be a greater need for data governance and ethics to ensure that data is collected and used responsibly.\n",
      "5. **Interdisciplinary Collaboration**: Data science will continue to be an interdisciplinary field, requiring collaboration between experts from various fields, including computer science, statistics, mathematics, and domain-specific knowledge.\n",
      "6. **Advances in Data Visualization**: Improvements in data visualization will enable data scientists to communicate complex insights more effectively to stakeholders.\n",
      "7. **Increased Focus on Explainability**: As AI and ML models become more complex, there will be a greater need for explainability and interpretability to ensure that insights are understood and trusted.\n",
      "8. **Growing Need for Data Literacy**: As data science becomes more ubiquitous, there will be a growing need for data literacy among stakeholders, including business leaders, policymakers, and the general public.\n",
      "9. **Advances in Domain-Specific Knowledge**: The integration of domain-specific knowledge into data science will lead to more accurate and relevant insights.\n",
      "10. **Rise of New Data Sources**: New data sources, such as social media, wearables, and IoT devices, will provide new opportunities for data science.\n",
      "\n",
      "In terms of specific skills and tools, some of the key areas that will be in demand in the future of data science include:\n",
      "\n",
      "1. **Programming languages**: Python, R, and SQL will continue to be in demand, while new languages, such as Julia and Julia-based languages, will also gain traction.\n",
      "2. **Machine learning**: Knowledge of ML algorithms and techniques will be essential for data scientists.\n",
      "3. **Data visualization**: The ability to effectively communicate insights through data visualization will be crucial.\n",
      "4. **Cloud computing**: Experience with cloud-based platforms, such as AWS and GCP, will be increasingly important.\n",
      "5. **Data governance**: Knowledge of data governance and ethics will be essential for responsible data science practices.\n",
      "6. **Domain-specific knowledge**: Integration of domain-specific knowledge will be critical for accurate and relevant insights.\n",
      "7. **Communication skills**: The ability to communicate complex insights to stakeholders will be essential.\n",
      "8. **Data storytelling**: The ability to tell compelling stories with data will be increasingly important.\n",
      "9. **Data wrangling**: The ability to clean, preprocess, and transform data will be crucial.\n",
      "10. **Data engineering**: The ability to design and implement data pipelines, architectures, and systems will be essential.\n",
      "\n",
      "Overall, the future of data science will be shaped by the increasing need for more sophisticated and accurate analysis of large datasets, the growth of cloud computing, and the integration of domain-specific knowledge.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain with a question and the loaded web page content, then print the answer.\n",
    "\n",
    "print(chain.invoke({'question':'what is the future of data science ', 'text':docs[0].page_content}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327fa6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Print the number of loaded documents and the content of the first document.\n",
    "# print(len(docs))\n",
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd4ae2",
   "metadata": {},
   "source": [
    "# **CSV Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CSVLoader for loading CSV files as LangChain document objects.\n",
    "\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "# Create a CSVLoader instance for the file 'Social_Network_Ads.csv'.\n",
    "\n",
    "loader = CSVLoader(file_path='Social_Network_Ads.csv')\n",
    "\n",
    "# Load the CSV file as documents.\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd970d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "page_content='User ID: 15728773\n",
      "Gender: Male\n",
      "Age: 27\n",
      "EstimatedSalary: 58000\n",
      "Purchased: 0' metadata={'source': 'Social_Network_Ads.csv', 'row': 5}\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of loaded documents (rows).\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "# Print the content of the sixth document (row) in the CSV file.\n",
    "\n",
    "print(docs[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
